---
title: "Problem Set 2"
author:
  - Your Name Here
format: 
  html:
    embed-resources: true
    toc: true
    toc-depth: 2
    toc-title: Contents
---

```{r}
#| echo: false
#| message: false

library(tidyverse)
library(cowplot)
library(boot)
theme_set(theme_cowplot())

# Required files for this problem set:
# small_data.csv
# salmon_pop.csv

```

# Randomization test

In problem set 1, you examined a simulated a null distribution for the `small_data.csv` dataset. Here, we will generate an empirical null distribution and set up a simple randomization test of the means below using a `for` loop. In Quant Methods 1, we did many similar for loops. We also showed how to search through the object returned by `summary()` to find the relevant test statistic. Instead, try to use the simpler tibble returned by `broom::tidy()` rather than digging through the summary object. Once you have your empirical null, calculate an empirical *P*-value for your observed difference in means (refer back to PS1 if you don't remember the details of this dataset).

```{r}
#| echo: true

#FIXME
set.seed(38913)

MM <- read_csv("../Data/small_data.csv",
               show_col_types = FALSE)

ggplot(MM, aes(x = Group, y = y)) +
  geom_point()

fm <- lm(y ~ Group, data = MM)

library(broom)

iters <- 1000
t_stats <- numeric(iters)
t_stats[1] <- as.numeric(tidy(fm)[2, 4])

for (ii in 2:iters) {
  MM_rand <- MM |> mutate(Group = sample(Group))
  fm_summary <- lm(y ~ Group, data = MM_rand) |> 
    summary() |> 
    tidy()
  t_stats[ii] <- as.numeric(fm_summary[2, 4])
}

ggplot(data = tibble(t_stats),
       aes(t_stats)) +
  geom_histogram(bins = 50, fill = "gray75") +
  scale_y_continuous(expand = expansion(mult = c(0, 0.1))) +
  geom_vline(xintercept = t_stats[1], color = "dodgerblue4", linewidth = 1)

mean(t_stats[1] >= t_stats)
```

Now, do the same randomization test, but this time, write a function to do each randomization and use `map_dbl()` to iterate through 1000 randomizations.  

```{r}
#| echo: true

# FIXME
set.seed(4574577)
d_obs <- MM |> 
    group_by(Group) |> 
    summarize(y_mean = mean(y)) |> 
    pivot_wider(names_from = Group, values_from = y_mean) |> 
    mutate(d = Group_2 - Group_1) |> 
    pull(d)

Group_diff <- function(.x, MM) {
  MM_resample <- MM |> 
    mutate(Group = sample(Group))
  d <- MM_resample |> 
    group_by(Group) |> 
    summarize(y_bar = mean(y)) |> 
    pivot_wider(names_from = Group, values_from = y_bar) |> 
    mutate(d = Group_2 - Group_1) |> 
    pull(d)
  return(d)
}

# With map
niter <- 1000
d_resample <- map_dbl(.x = seq_len(niter),
                      .f = Group_diff,
                      MM = MM)
d_resample[1] <- d_obs

ggplot(data = tibble(d_resample),
       aes(d_resample)) +
  geom_histogram(bins = 50, fill = "gray75") +
  scale_y_continuous(expand = expansion(mult = c(0, 0.1))) +
  geom_vline(xintercept = d_obs, color = "dodgerblue4", linewidth = 1)

mean(d_resample <= d_obs)
```

We ran 1000 randomizations. However, for very small datasets like this one, a concern is having only a limited number of possible combinations. We don't want to be randomizing and getting the same combinations over and over again. Use `unique()` to figure out how many unique differences you got in your randomization test. 

```{r}
#| echo: true

# FIXME
length(unique(d_resample))
```

One way to figure out how many combinations are possible is with the binomial coefficient. For example, we can use the `choose` function to ask how many sets of two are possible with a list of three things, which we've done below. 

```{r}
choose(3, 2)
```

1. 1 and 2
2. 1 and 3
3. 2 and 3

Use `choose` to see how many ways there are to assign your 5 labels to a set of 10 data points, as you are doing in your randomization test of the small data. This should be close to the number of unique differences you saw in your test. If you calculate 1 divided by the number of unique combinations, you get the lowest possible empirical *P*-value you can observe.

```{r}
#| echo: true

# FIXME
choose(10, 5)

1 / choose(10, 5)
```

For larger datasets, you quickly get to many possible combinations. Try out the same example for a dataset with 20 data points in two groups of 10. 

```{r}
#| echo: true

# FIXME
choose(20, 10)

1 / choose(20, 10)
```


# Allele frequencies at the *Clock* gene

[O'Malley et al.(2010)](https://doi.org/10.1098/rspb.2010.0762) performed a study of a set of different alleles for the *Clock* gene in salmon populations and found evidence that this gene is involved in seasonal adaptation and the timing of reproduction across different latitudes. You might imagine that it would be useful to estimate confidence intervals for the allele frequencies they measured in different populations and years. Let's consider one population where one allele has a frequency of 0.18 for a sample of 100 salmon. Load in the `salmon_pop.csv` file that holds the allele types for this set of fish. Look at the data and calculate the observed frequency for the clock allele and save it as an object.     


```{r}
#| echo: true

# FIXME
set.seed(90843)
Clock <- tibble("ID" = paste0("S", seq(1:100)),
                "Clock_Allele" = sample(c(rep(1, 18), rep(0, 82))))

write_csv(Clock, file = "../Data/salmon_pop.csv")

####

Clock <- read_csv(file = "../Data/salmon_pop.csv",
                  show_col_types = FALSE)
pA.obs <- mean(Clock$Clock_Allele)

```

Let's try applying the jackknife method to calculate the confidence interval. Use a loop or `map()` to remove one salmon from the dataset and get a new estimate of the allele frequency and then calculate the pseudovalue. Save both your set of allele frequencies and your pseudovalues for each jackknife sample. 

```{r}
#| echo: true

# FIXME

jk.PA <- tibble("est" = rep(NA, length = nrow(Clock)),
                "ps" = rep(NA, length = nrow(Clock)))

for(ii in 1:nrow(Clock)) {
  pA.est <- mean(Clock$Clock_Allele[-ii])
  jk.PA$est[ii] <- pA.est
  jk.PA$ps[ii] <- nrow(Clock) * pA.obs - (nrow(Clock)-1) * pA.est
}

```

Plot a histogram of your estimates of allele frequency and of your pseudovalues. 

```{r}
#| echo: true

# FIXME
jk.PA |>
  ggplot(aes(est)) +
  geom_histogram(fill = "grey75")

jk.PA |>
  ggplot(aes(ps)) +
  geom_histogram(fill = "grey75")

```

What do you notice about these estimates and pseudovalues? Give a brief explanation for your observations. Do you have any concerns about using the jackknife method in this application?

> The estimates and pseudovalues only take on two values. The estimates only have two values because you are deleting a 0 or a 1 so there are only two possible outcomes. The pseudovalues are basically the calculation of the value was left out because a proportion is a mean of a set of 0's and 1's and so they are always a 0 or a 1. The distributions are certainly not close to normal so it might not work well to use the jackknife. The standard error and confidence interval calculation we would use comes from expectations from a normal distribution. 


Get the jackknife estimate of the mean and confidence interval as we did in lecture.

```{r}
#| echo: true

# FIXME
muj <- mean(jk.PA$ps)

sej <- sd(jk.PA$ps)/sqrt(nrow(Clock))

muj - qt(0.975,nrow(Clock)-1)*sej
muj + qt(0.975,nrow(Clock)-1)*sej

```

Now let's try the bootstrap. Load the `boot` library and examine the help for the `boot` function. Write a function where the first argument is the data (your allele calls or the tibble that holds them) and the second is the new indices from sampling with replacement. Test out your function by passing it data and a new set of indices before using it in the `boot` function. Then use the `boot` function with 1000 bootstrap replicates and `stype = "i"` to tell it to use indices. 

```{r}
#| echo: true

# FIXME
bootfreq <- function(data, indices) {
  data_sub <- data[indices]
  return(mean(data_sub))
}

bb <- boot(data = Clock$Clock_Allele, statistic = bootfreq, R = 1000,
           stype = "i")

```

Use `str()` to look at the output of `boot` and use the help to find the bootstrap replicates. Make a histogram of your bootstrap samples.

```{r}
# FIXME
breps <- tibble("breps" = bb$t[ , 1])
breps |>
  ggplot(aes(breps)) +
  geom_histogram(fill = "grey75", bins = 25)
  
```

What do you notice about your bootstrap replicates? Give a brief explanation for your observations. Do you have any concerns about using the bootstrap method in this application?

> The bootstrap replicates appear to be approximatley normally distributed. Given this, the bootstrap method could work well here. 


Use the `boot.ci` function to get a confidence interval specifying the percentile method and compare your results to the jackknife CI. 

```{r}
# FIXME
(bbci <-boot.ci(bb, type="perc"))

```

There are many examples in the literature using simulations to demonstrate the bootstrap or jackknife either does or does not perform well for different applications. Let's do a small simulation for the parameters in this example (frequency = 0.18, sample size = 100) to see how these methods do. 

1. Set the true frequency to 0.18 and save this as an object
2. Set your sample size to 100 and save this as an object
3. Use `rbinom` to generate allele calls for your sample size and true frequency
4. Perform the jackknife and obtain a CI
5. Perform the bootstrap and obtain a CI
6. Use your preferred iteration method (loop, map, apply, etc.) to perform these steps 10,000 times and keep your jackknife and bootstrap CIs in a tibble. You probably want to start with few iterations while you get it working and then scale up. It will take a few minutes to run at 10,000. 
7. Calculate the percentage of times each CI includes the true frequency.


```{r}
#| echo: true

# FIXME
set.seed(879371)

ff <- 0.18
Nsamp <- 100

niter <- 10000

output <- tibble("jlower"= rep(NA,niter),
                 "jupper" = rep(NA,niter),
                 "blower" = rep(NA,niter),
                 "bupper" = rep(NA,niter))

for(jj in 1:niter){
  
  AA <- tibble("Allele" = rbinom(Nsamp, 1, ff))
  
  pA.obs <- mean(AA$Allele)
  
  jk.PA <- tibble("ps" = rep(NA, length=nrow(AA)))
  
  for(ii in 1:nrow(AA))
  {
    pA.est <- mean(AA$Allele[-ii])
    jk.PA$ps[ii] <- nrow(AA)*pA.obs - (nrow(AA)-1)*pA.est
  }
  
  muj <- mean(jk.PA$ps)
  
  sej <- sd(jk.PA$ps)/sqrt(nrow(AA))
  
  output$jlower[jj] <- muj - qt(0.975,nrow(AA)-1)*sej
  output$jupper[jj] <- muj + qt(0.975,nrow(AA)-1)*sej
  
  bb <- boot(data = AA$Allele, statistic = bootfreq, R = 1000,stype = "i")
  bbci <-boot.ci(bb, type="perc")
  output$blower[jj] <- bbci$percent[1,4]
  output$bupper[jj] <- bbci$percent[1,5]
}

jk.c <- mean(output$jlower <= ff & output$jupper >= ff)
b.c <- mean(output$blower <= ff & output$bupper >= ff) 

```

Based on your simulation, do you think these methods would work well for calculating CIs for this salmon study and similar studies of allele frequencies? Why or why not?

> Maybe. Both methods seem to do ok. The bootstrap has closer to the correct 95% coverage but the jackknife still performs surprisingly well given the distributions we observed. So if a study was of a similar size and similar proportions, it seems both method would do well, with a prefernece for the bootstrap.

What values should further simulations change to test the generality of these methods? (Try some out if you wish)

> We would want to try different sample sizes and different proportions to confirm that these methods perform well across a range of samples. 


# Sticklebacks 

[Colosimo et al. 2004](https://doi.org/10.1371/journal.pbio.0020109) performed crosses of benthic and marine sticklebacks to investigate the genetic basis of armor plates (an anti-predator adaptation seen in marine sticklebacks). You have seen the data for one of these loci before in QMLS 1. Load the stickleback plates data in the abdData package using `abdData::SticklebackPlates` and plot histograms for each genotype and get the mean and median for each genotype. 

![](https://journals.plos.org/plosbiology/article/figure/image?size=large&id=10.1371/journal.pbio.0020109.g001){width=80% fig-align="center"}


```{r}
#| echo: true

# FIXME
SP <- abdData::SticklebackPlates

ggplot(SP, aes(plates)) +
  geom_histogram(bins = 30) +
  facet_grid(genotype ~ .)

SP |> 
  group_by(genotype) |> 
  summarize(Mean = mean(plates),
            Median = median(plates))

```

Colosimo et al. 2004 fit a linear model at many different variants in the genome to test for an association between the marine vs benthic genotype and the number of armor plates. We will focus on this one locus and given the distribution of plate number, we will use a randomization test. Following the procedure from lecture, perform a randomization and collect the following test statistic: the 3 pairwise differences in mean plate number for the different genotypes and the 3 pairwise differences in median plate number for the different genotypes. Perform 10000 randomizations and plot histograms facetting by the pairwise difference and type. Place your observed values on your histograms with a vertical line. 

```{r}
#| echo: true

# FIXME
niter <- 10000
output_r <- tibble("MM_Mm_mean" = rep(NA, niter),
                   "MM_mm_mean" = rep(NA, niter),
                   "Mm_mm_mean" = rep(NA, niter),
                   "MM_Mm_median" = rep(NA, niter),
                   "MM_mm_median" = rep(NA, niter),
                   "Mm_mm_median" = rep(NA, niter))
obs.st <- SP |> 
  group_by(genotype) |>
  summarize(Mean = mean(plates),
            Median = median(plates))

output_r$MM_Mm_mean[1] <-  obs.st$Mean[obs.st$genotype=="MM"] -
  obs.st$Mean[obs.st$genotype=="Mm"]
output_r$MM_mm_mean[1] <-  obs.st$Mean[obs.st$genotype=="MM"] -
  obs.st$Mean[obs.st$genotype=="mm"]
output_r$Mm_mm_mean[1] <-  obs.st$Mean[obs.st$genotype=="Mm"] -
  obs.st$Mean[obs.st$genotype=="mm"]

output_r$MM_Mm_median[1] <-  obs.st$Median[obs.st$genotype=="MM"] -
  obs.st$Median[obs.st$genotype=="Mm"]
output_r$MM_mm_median[1] <-  obs.st$Median[obs.st$genotype=="MM"] -
  obs.st$Median[obs.st$genotype=="mm"]
output_r$Mm_mm_median[1] <-  obs.st$Median[obs.st$genotype=="Mm"] -
  obs.st$Median[obs.st$genotype=="mm"]
  
for (ii in 2:niter) {
  SP.s <- SP
  SP.s$genotype <- sample(SP.s$genotype)
  rand.st <- SP.s |> 
    group_by(genotype) |>
    summarize(Mean = mean(plates),
              Median = median(plates))
  
  output_r$MM_Mm_mean[ii] <-  rand.st$Mean[rand.st$genotype=="MM"] -
    rand.st$Mean[rand.st$genotype=="Mm"]
  output_r$MM_mm_mean[ii] <-  rand.st$Mean[rand.st$genotype=="MM"] -
    rand.st$Mean[rand.st$genotype=="mm"]
  output_r$Mm_mm_mean[ii] <-  rand.st$Mean[rand.st$genotype=="Mm"] -
    rand.st$Mean[rand.st$genotype=="mm"]
  
  output_r$MM_Mm_median[ii] <-  rand.st$Median[rand.st$genotype=="MM"] -
    rand.st$Median[rand.st$genotype=="Mm"]
  output_r$MM_mm_median[ii] <-  rand.st$Median[rand.st$genotype=="MM"] -
    rand.st$Median[rand.st$genotype=="mm"]
  output_r$Mm_mm_median[ii] <-  rand.st$Median[rand.st$genotype=="Mm"] -
    rand.st$Median[rand.st$genotype=="mm"]
}

out_plot <- output_r |>
  pivot_longer(everything(), names_to = "Pair", 
                       values_to = "Difference")

obs_plot <- output_r[1,] |>
  pivot_longer(everything(), names_to = "Pair", 
                       values_to = "Difference")

out_plot |>
  ggplot(aes(Difference)) +
  geom_histogram(fill = "grey75", bins = 50) +
  geom_vline(data= obs_plot, aes(xintercept = Difference), color = "firebrick") +
  facet_wrap(.~Pair, ncol = 3)

```

For each of your pairwise differences, calculate the two-sided empirical *P*-value. 

```{r}
#| echo: true

# FIXME
emp.ps <- apply(output_r, 2, function(x) sum(abs(x) >= abs(x[1]))/length(x))

```

What do you conclude from this randomization test? Which set of test statistics do you think is most appropriate in this case? 

>

Here, we are not doing just one test. Let's consider the set of tests using the differences in the medians. For the empirical nulls you have generated here (a set of three pairwise differences in medians), you can count the number of times you get a difference as extreme or more across all three of your differences at various magnitudes. This quantity is the expected number of false positives across your three tests and will become useful when we use sampling methods to assess false positive and false discovery rates. Let's calculate this for one threshold value to see how this would work. 

1. Pick a threshold difference to try (e.g., 20). You might base your choice on the observed data or your histograms of the empirical null to decide on a difference magnitude to try.
2. For each of your 10,000 iterations, determine the number of times you get a pairwise difference (out of your set of 3) that equals or exceeds this threshold difference. These numbers should all be 0, 1, 2, or 3 (those are the only possible - either none, 1, 2, or all 3 will equal or exceed your threshold).
3. Plot a histogram to visualize this across your 10,000 iterations. 
4. Take the mean across all your iterations. You've now calculated the expected number of differences you'd expect to exceed your threshold value for a set of 3 tests here (or the expected number of false positives for this test at your threshold value). 

```{r}
#| echo: true

# FIXME
th.d <- 4
oo <- output_r[-1, 4:6]

fp <- apply(oo, 1, function(x) sum(abs(x) >= th.d))
mean(fp)

fp_t <- tibble(fp = fp)
fp_t |>
  ggplot(aes(fp)) +
  geom_histogram(fill = "grey75", bins = 30)

```
