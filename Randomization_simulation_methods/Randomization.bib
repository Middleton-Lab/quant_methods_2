% Generated by Paperpile. Check out https://paperpile.com for more information.
% BibTeX export options can be customized via Settings -> BibTeX.

@ARTICLE{Beaumont2010-vr,
  title       = "Approximate Bayesian Computation in Evolution and Ecology",
  author      = "Beaumont, Mark A",
  journal     = "Annu. Rev. Ecol. Evol. Syst.",
  volume      =  41,
  number      =  1,
  pages       = "379--406",
  year        =  2010,
  url         = "http://www.annualreviews.org/doi/abs/10.1146/annurev-ecolsys-102209-144621",
  keywords    = "ABC;Module - Randomization",
  issn        = "1543-592X",
  doi         = "10.1146/annurev-ecolsys-102209-144621",
  original_id = "855cf87d-7976-0e5e-a3fc-ef5e2f1b10c3"
}

@ARTICLE{Storey2003-mz,
  title       = "Statistical significance for genomewide studies",
  author      = "Storey, John D and Tibshirani, Robert",
  abstract    = "With the increase in genomewide experiments and the sequencing
                 of multiple genomes, the analysis of large data sets has
                 become commonplace in biology. It is often the case that
                 thousands of features in a genomewide data set are tested
                 against some null hypothesis, where a number of features are
                 expected to be significant. Here we propose an approach to
                 measuring statistical significance in these genomewide studies
                 based on the concept of the false discovery rate. This
                 approach offers a sensible balance between the number of true
                 and false positives that is automatically calibrated and
                 easily interpreted. In doing so, a measure of statistical
                 significance called the q value is associated with each tested
                 feature. The q value is similar to the well known p value,
                 except it is a measure of significance in terms of the false
                 discovery rate rather than the false positive rate. Our
                 approach avoids a flood of false positive results, while
                 offering a more liberal criterion than what has been used in
                 genome scans for linkage.",
  journal     = "Proc. Natl. Acad. Sci. U. S. A.",
  volume      =  100,
  number      =  16,
  pages       = "9440--9445",
  month       =  aug,
  year        =  2003,
  url         = "http://dx.doi.org/10.1073/pnas.1530509100",
  keywords    = "xx Archived/FDR;Module - Randomization;Quantitative Methods",
  language    = "en",
  issn        = "0027-8424",
  pmid        = "12883005",
  doi         = "10.1073/pnas.1530509100",
  pmc         = "PMC170937",
  original_id = "cb386ad5-cd57-0385-8c7c-4dbdf850cef6"
}

@ARTICLE{Benjamini1995-cw,
  title       = "Controlling the False Discovery Rate: A Practical and Powerful
                 Approach to Multiple Testing",
  author      = "Benjamini, Yoav and Hochberg, Yosef",
  abstract    = "The common approach to the multiplicity problem calls for
                 controlling the familywise error rate (FWER). This approach,
                 though, has faults, and we point out a few. A different
                 approach to problems of multiple significance testing is
                 presented. It calls for controlling the expected proportion of
                 falsely rejected hypotheses-the false discovery rate. This
                 error rate is equivalent to the FWER when all hypotheses are
                 true but is smaller otherwise. Therefore, in problems where
                 the control of the false discovery rate rather than that of
                 the FWER is desired, there is potential for a gain in power. A
                 simple sequential Bonferroni-type procedure is proved to
                 control the false discovery rate for independent test
                 statistics, and a simulation study shows that the gain in
                 power is substantial. The use of the new procedure and the
                 appropriateness of the criterion are illustrated with
                 examples.",
  journal     = "J. R. Stat. Soc. Series B Stat. Methodol.",
  volume      =  57,
  number      =  1,
  pages       = "289--300",
  year        =  1995,
  url         = "http://www.jstor.org/stable/2346101",
  keywords    = "xx Archived/FDR;Module - Randomization;Quantitative Methods",
  issn        = "1369-7412, 0035-9246",
  original_id = "f82b0881-aae3-0dd0-9d8d-150bd55279dc"
}

@ARTICLE{Curran-Everett2000-qv,
  title       = "Multiple comparisons: philosophies and illustrations",
  author      = "Curran-Everett, Douglas",
  abstract    = "Statistical procedures underpin the process of scientific
                 discovery. As researchers, one way we use these procedures is
                 to test the validity of a null hypothesis. Often, we test the
                 validity of more than one null hypothesis. If we fail to use
                 an appropriate procedure to account for this multiplicity,
                 then we are more likely to reach a wrong scientific
                 conclusion[---]we are more likely to make a mistake. In
                 physiology, experiments that involve multiple comparisons are
                 common: of the original articles published in 1997 by the
                 American Physiological Society, ~40\% cite a multiple
                 comparison procedure. In this review, I demonstrate the
                 statistical issue embedded in multiple comparisons, and I
                 summarize the philosophies of handling this issue. I also
                 illustrate the three procedures[---]Newman-Keuls, Bonferroni,
                 least significant difference[---]cited most often in my
                 literature review; each of these procedures is of limited
                 practical value. Last, I demonstrate the false discovery rate
                 procedure, a promising development in multiple comparisons.
                 The false discovery rate procedure may be the best practical
                 solution to the problems of multiple comparisons that exist
                 within physiology and other scientific disciplines.",
  journal     = "Am. J. Physiol. Regul. Integr. Comp. Physiol.",
  volume      =  279,
  number      =  1,
  pages       = "R1--8",
  month       =  jul,
  year        =  2000,
  url         = "http://ajpregu.physiology.org/content/279/1/R1",
  keywords    = "xx Archived/FDR;Module - Randomization;Quantitative Methods",
  issn        = "0363-6119",
  original_id = "42551e24-cdd3-0e20-8f12-bfcc634958d6"
}

@ARTICLE{Blum2010-ol,
  title       = "Non-linear regression models for Approximate Bayesian
                 Computation",
  author      = "Blum, Michael G B and François, Olivier",
  abstract    = "Approximate Bayesian inference on the basis of summary
                 statistics is well-suited to complex problems for which the
                 likelihood is either mathematically or computationally
                 intractable. However the methods that use rejection suffer
                 from the curse of dimensionality when the number of summary
                 statistics is increased. Here we propose a machine-learning
                 approach to the estimation of the posterior density by
                 introducing two innovations. The new method fits a nonlinear
                 conditional heteroscedastic regression of the parameter on the
                 summary statistics, and then adaptively improves estimation
                 using importance sampling. The new algorithm is compared to
                 the state-of-the-art approximate Bayesian methods, and
                 achieves considerable reduction of the computational burden in
                 two examples of inference in statistical genetics and in a
                 queueing model.",
  journal     = "Stat. Comput.",
  publisher   = "Springer US",
  volume      =  20,
  number      =  1,
  pages       = "63--73",
  month       =  jan,
  year        =  2010,
  url         = "http://link.springer.com/article/10.1007/s11222-009-9116-0",
  keywords    = "ABC;Module - Randomization;Bayes Readings;Module - Bayes",
  language    = "en",
  issn        = "0960-3174, 1573-1375",
  doi         = "10.1007/s11222-009-9116-0",
  original_id = "3154d38c-98ee-09d6-b910-2d5db2013a92"
}

@ARTICLE{Brauer2002-ks,
  title       = "Genetic algorithms and parallel processing in
                 maximum-likelihood phylogeny inference",
  author      = "Brauer, M and Holder, Mark and Dries, L and Zwickl, Derrick
                 and Lewis, Paul O and Hillis, David M",
  abstract    = "We investigated the usefulness of a parallel genetic algorithm
                 for phylogenetic inference under the maximum-likelihood (NIL)
                 optimality criterion. Parallelization was accomplished by
                 assigning each ``individual'' in the genetic algorithm
                 ``population'' to a separate processor so that the number of
                 processors used was equal to the size of the evolving
                 population (plus one additional processor for the control of
                 operations). The genetic algorithm incorporated branch-length
                 and topological mutation, recombination, selection on the ML
                 score, and (in some cases) migration and recombination among
                 subpopulations. We tested this parallel genetic algorithm with
                 large (228 taxa) data sets of both empirically observed DNA
                 sequence data (for angiosperms) as well as simulated DNA
                 sequence data. For both observed and simulated data,
                 search-time improvement was nearly linear with respect to the
                 number of processors, so the parallelization strategy appears
                 to be highly effective at improving computation time for large
                 phylogenetic problems using the genetic algorithm. We also
                 explored various ways of optimizing and tuning the parameters
                 of the genetic algorithm. Under the conditions of our
                 analyses, we did not find the best-known solution using the
                 genetic algorithm approach before terminating each run. We
                 discuss some possible limitations of the current
                 implementation of this genetic algorithm as well as of avenues
                 for its future improvement.",
  journal     = "Mol. Biol. Evol.",
  volume      =  19,
  number      =  10,
  pages       = "1717--1726",
  year        =  2002,
  keywords    = "Genetic algorithm;Module - Randomization",
  issn        = "0737-4038",
  original_id = "e8a5f52b-c7cf-0d21-be86-98ed49f2a111"
}

@ARTICLE{Zwickl2008-yy,
  title       = "{GARLI}: Genetic Algorithm for Rapid Likelihood Inference",
  author      = "Zwickl, Derrick",
  journal     = "Presentations",
  pages       = "87",
  year        =  2008,
  keywords    = "Genetic algorithm;Module - Randomization",
  issn        = "1041-9780",
  original_id = "713926f5-5cac-0978-82f5-2d0699cf0931"
}

@ARTICLE{Csillery2010-dj,
  title       = "Approximate Bayesian Computation ({ABC}) in practice",
  author      = "Csilléry, Katalin and Blum, Michael G B and Gaggiotti, Oscar E
                 and François, Olivier",
  journal     = "Trends Ecol. Evol.",
  volume      =  25,
  number      =  7,
  pages       = "410--418",
  year        =  2010,
  url         = "http://dx.doi.org/10.1016/j.tree.2010.04.001",
  keywords    = "ABC;Module - Randomization",
  issn        = "0169-5347",
  pmid        = "20488578",
  doi         = "10.1016/j.tree.2010.04.001",
  original_id = "f0698de0-8c5c-03c0-a792-d8fcdd808a2f"
}

@ARTICLE{Beaumont2002-kn,
  title       = "Approximate Bayesian Computation in Population Genetics",
  author      = "Beaumont, Mark A and Zhang, Wenyang and Balding, David J",
  abstract    = "We propose a new method for approximate Bayesian statistical
                 inference on the basis of summary statistics. The method is
                 suited to complex problems that arise in population genetics,
                 extending ideas developed in this setting by earlier authors.
                 Properties of the posterior distribution of a parameter, such
                 as its mean or density curve, are approximated without
                 explicit likelihood calculations. This is achieved by fitting
                 a local-linear regression of simulated parameter values on
                 simulated summary statistics, and then substituting the
                 observed summary statistics into the regression equation. The
                 method combines many of the advantages of Bayesian statistical
                 inference with the computational efficiency of methods based
                 on summary statistics. A key advantage of the method is that
                 the nuisance parameters are automatically integrated out in
                 the simulation step, so that the large numbers of nuisance
                 parameters that arise in population genetics problems can be
                 handled without difficulty. Simulation results indicate
                 computational and statistical efficiency that compares
                 favorably with those of alternative methods previously
                 proposed in the literature. We also compare the relative
                 efficiency of inferences obtained using methods based on
                 summary statistics with those obtained directly from the data
                 using MCMC.",
  journal     = "Genetics",
  volume      =  162,
  number      =  4,
  pages       = "2025--2035",
  month       =  dec,
  year        =  2002,
  url         = "http://www.genetics.org/content/162/4/2025",
  keywords    = "ABC;Module - Randomization",
  language    = "en",
  issn        = "0016-6731, 1943-2631",
  pmid        = "12524368",
  original_id = "76290b01-59ff-0930-8ea5-fb8d032ed3ec"
}

@ARTICLE{Scrucca2013-jf,
  title    = "{GA}: A Package for Genetic Algorithms in {R}",
  author   = "Scrucca, Luca",
  abstract = "Genetic algorithms (GAs) are stochastic search algorithms
              inspired by the basic principles of biological evolution and
              natural selection. GAs simulate the evolution of living
              organisms, where the fittest individuals dominate over the weaker
              ones, by mimicking the biological mechanisms of evolution, such
              as selection, crossover and mutation. GAs have been successfully
              applied to solve optimization problems, both for continuous
              (whether differentiable or not) and discrete functions. This
              paper describes the R package GA, a collection of general purpose
              functions that provide a flexible set of tools for applying a
              wide range of genetic algorithm methods. Several examples are
              discussed, ranging from mathematical functions in one and two
              dimensions known to be hard to optimize with standard
              derivative-based methods, to some selected statistical problems
              which require the optimization of user defined objective
              functions. (This paper contains animations that can be viewed
              using the Adobe Acrobat PDF viewer.)",
  journal  = "Journal of Statistical Software, Articles",
  volume   =  53,
  number   =  4,
  pages    = "1--37",
  year     =  2013,
  url      = "https://www.jstatsoft.org/v053/i04",
  keywords = "Genetic algorithm;Module - Randomization",
  issn     = "1548-7660",
  doi      = "10.18637/jss.v053.i04"
}

@ARTICLE{Mebane2011-vy,
  title    = "Genetic Optimization Using Derivatives: The rgenoud Package for
              {R}",
  author   = "Mebane, Jr., Walter and Sekhon, Jasjeet",
  abstract = "genoud is an R function that combines evolutionary algorithm
              methods with a derivative-based (quasi-Newton) method to solve
              difficult optimization problems. genoud may also be used for
              optimization problems for which derivatives do not exist. genoud
              solves problems that are nonlinear or perhaps even discontinuous
              in the parameters of the function to be optimized. When the
              function to be optimized (for example, a log-likelihood) is
              nonlinear in the model's parameters, the function will generally
              not be globally concave and may have irregularities such as
              saddlepoints or discontinuities. Optimization methods that rely
              on derivatives of the objective function may be unable to find
              any optimum at all. Multiple local optima may exist, so that
              there is no guarantee that a derivative-based method will
              converge to the global optimum. On the other hand, algorithms
              that do not use derivative information (such as pure genetic
              algorithms) are for many problems needlessly poor at local hill
              climbing. Most statistical problems are regular in a neighborhood
              of the solution. Therefore, for some portion of the search space,
              derivative information is useful. The function supports parallel
              processing on multiple CPUs on a single machine or a cluster of
              computers.",
  journal  = "Journal of Statistical Software",
  volume   =  42,
  number   =  11,
  pages    = "1--26",
  year     =  2011,
  url      = "https://www.jstatsoft.org/v042/i11",
  keywords = "Genetic algorithm;Module - Randomization",
  issn     = "1548-7660",
  doi      = "10.18637/jss.v042.i11"
}

@ARTICLE{Turner2012-rw,
  title    = "A tutorial on approximate Bayesian computation",
  author   = "Turner, Brandon M and Van Zandt, Trisha",
  abstract = "This tutorial explains the foundation of approximate Bayesian
              computation (ABC), an approach to Bayesian inference that does
              not require the specification of a likelihood function, and hence
              that can be used to estimate posterior distributions of
              parameters for simulation-based models. We discuss briefly the
              philosophy of Bayesian inference and then present several
              algorithms for ABC. We then apply these algorithms in a number of
              examples. For most of these examples, the posterior distributions
              are known, and so we can compare the estimated posteriors derived
              from ABC to the true posteriors and verify that the algorithms
              recover the true posteriors accurately. We also consider a
              popular simulation-based model of recognition memory (REM) for
              which the true posteriors are unknown. We conclude with a number
              of recommendations for applying ABC methods to solve real-world
              problems.",
  journal  = "J. Math. Psychol.",
  volume   =  56,
  number   =  2,
  pages    = "69--85",
  month    =  apr,
  year     =  2012,
  url      = "http://www.sciencedirect.com/science/article/pii/S0022249612000272",
  keywords = "Approximate Bayesian computation; Tutorial; Bayesian estimation;
              Population Monte Carlo;ABC;Module - Randomization;Bayes
              Readings;Module - Bayes",
  issn     = "0022-2496",
  doi      = "10.1016/j.jmp.2012.02.005"
}

@ARTICLE{Pritchard1999-uh,
  title    = "Population growth of human {Y} chromosomes: a study of {Y}
              chromosome microsatellites",
  author   = "Pritchard, J K and Seielstad, M T and Perez-Lezaun, A and
              Feldman, M W",
  abstract = "We use variation at a set of eight human Y chromosome
              microsatellite loci to investigate the demographic history of the
              Y chromosome. Instead of assuming a population of constant size,
              as in most of the previous work on the Y chromosome, we consider
              a model which permits a period of recent population growth. We
              show that for most of the populations in our sample this model
              fits the data far better than a model with no growth. We estimate
              the demographic parameters of this model for each population and
              also the time to the most recent common ancestor. Since there is
              some uncertainty about the details of the microsatellite mutation
              process, we consider several plausible mutation schemes and
              estimate the variance in mutation size simultaneously with the
              demographic parameters of interest. Our finding of a recent
              common ancestor (probably in the last 120,000 years), coupled
              with a strong signal of demographic expansion in all populations,
              suggests either a recent human expansion from a small ancestral
              population, or natural selection acting on the Y chromosome.",
  journal  = "Mol. Biol. Evol.",
  volume   =  16,
  number   =  12,
  pages    = "1791--1798",
  month    =  dec,
  year     =  1999,
  url      = "http://dx.doi.org/10.1093/oxfordjournals.molbev.a026091",
  keywords = "ABC;Module - Randomization",
  language = "en",
  issn     = "0737-4038",
  pmid     = "10605120",
  doi      = "10.1093/oxfordjournals.molbev.a026091"
}

@ARTICLE{Sekhon1998-hs,
  title     = "Genetic Optimization Using Derivatives",
  author    = "Sekhon, Jasjeet S and Mebane, Walter R",
  abstract  = "[We describe a new computer program that combines evolutionary
               algorithm methods with a derivative-based, quasi-Newton method
               to solve difficult unconstrained optimization problems. The
               program, called GENOUD (GENetic Optimization Using Derivatives),
               effectively solves problems that are nonlinear or perhaps even
               discontinuous in the parameters of the function to be optimized.
               When a statistical model's estimating function (for example, a
               log-likelihood) is nonlinear in the model's parameters, the
               function to be optimized will usually not be globally concave
               and may contain irregularities such as saddlepoints or
               discontinuous jumps. Optimization methods that rely on
               derivatives of the objective function may be unable to find any
               optimum at all. Or multiple local optima may exist, so that
               there is no guarantee that a derivative-based method will
               converge to the global optimum. We discuss the theoretical basis
               for expecting GENOUD to have a high probability of finding
               global optima. We conduct Monte Carlo experiments using scalar
               Normal mixture densities to illustrate this capability. We also
               use a system of four simultaneous nonlinear equations that has
               many parameters and multiple local optima to compare the
               performance of GENOUD to that of the Gauss-Newton algorithm in
               SAS's PROC MODEL.]",
  journal   = "Polit. Anal.",
  volume    =  7,
  pages     = "187--210",
  year      =  1998,
  url       = "https://www.cambridge.org/core/journals/political-analysis/article/genetic-optimization-using-derivatives/9C2ACCE0EF8AA8E7E905DC4130D9660D",
  keywords  = "Genetic algorithm;Module - Randomization",
  issn      = "1047-1987, 1476-4989",
  doi       = "10.1093/pan/7.1.187"
}

@BOOK{Goldberg1989-zk,
  title     = "Genetic Algorithms in Search, Optimization, and Machine Learning",
  author    = "Goldberg, David E",
  publisher = "Addison-Wesley",
  year      =  1989,
  keywords  = "Genetic algorithm;Module - Randomization"
}

@INPROCEEDINGS{Grefenstette1989-qz,
  title     = "How Genetic Algorithms Work: A Critical Look at Implicit
               Parallelism",
  booktitle = "Proceedings of the third international conference on Genetic
               algorithms",
  author    = "Grefenstette, John J and Baker, James E",
  pages     = "20--27",
  year      =  1989,
  keywords  = "Genetic algorithm;Module - Randomization"
}

@BOOK{Holland1975-nz,
  title     = "Adaptation in Natural and Artificial Systems: An Introductory
               Analysis with Applications to Biology, Control, and Artificial
               Intelligence",
  author    = "Holland, John Henry",
  abstract  = "Genetic algorithms are playing an increasingly important role in
               studies of complex adaptive systems, ranging from adaptive
               agents in economic theory to the use of machine learning
               techniques in the design of complex devices such as aircraft
               turbines and integrated circuits. Adaptation in Natural and
               Artificial Systems is the book that initiated this field of
               study, presenting the theoretical foundations and exploring
               applications. In its most familiar form, adaptation is a
               biological process, whereby organisms evolve by rearranging
               genetic material to survive in environments confronting them. In
               this now classic work, Holland presents a mathematical model
               that allows for the nonlinearity of such complex interactions.
               He demonstrates the model's universality by applying it to
               economics, physiological psychology, game theory, and artificial
               intelligence and then outlines the way in which this approach
               modifies the traditional views of mathematical genetics.
               Initially applying his concepts to simply defined artificial
               systems with limited numbers of parameters, Holland goes on to
               explore their use in the study of a wide range of complex,
               naturally occuring processes, concentrating on systems having
               multiple factors that interact in nonlinear ways. Along the way
               he accounts for major effects of coadaptation and coevolution:
               the emergence of building blocks, or schemata, that are
               recombined and passed on to succeeding generations to provide,
               innovations and improvements.",
  publisher = "University of Michigan Press",
  year      =  1975,
  keywords  = "Genetic algorithm;Module - Randomization",
  isbn      = "9780262581110"
}

@ARTICLE{Tavare1997-lg,
  title    = "Inferring coalescence times from {DNA} sequence data",
  author   = "Tavaré, S and Balding, D J and Griffiths, R C and Donnelly, P",
  abstract = "The paper is concerned with methods for the estimation of the
              coalescence time (time since the most recent common ancestor) of
              a sample of intraspecies DNA sequences. The methods take
              advantage of prior knowledge of population demography, in
              addition to the molecular data. While some theoretical results
              are presented, a central focus is on computational methods. These
              methods are easy to implement, and, since explicit formulae tend
              to be either unavailable or unilluminating, they are also more
              useful and more informative in most applications. Extensions are
              presented that allow for the effects of uncertainty in our
              knowledge of population size and mutation rates, for variability
              in population sizes, for regions of different mutation rate, and
              for inference concerning the coalescence time of the entire
              population. The methods are illustrated using recent data from
              the human Y chromosome.",
  journal  = "Genetics",
  volume   =  145,
  number   =  2,
  pages    = "505--518",
  month    =  feb,
  year     =  1997,
  url      = "https://www.ncbi.nlm.nih.gov/pubmed/9071603",
  keywords = "ABC;Module - Randomization",
  language = "en",
  issn     = "0016-6731",
  pmid     = "9071603",
  pmc      = "PMC1207814"
}

@BOOK{Sisson2018-ke,
  title     = "Handbook of Approximate Bayesian Computation",
  author    = "Sisson, Scott A and Fan, Yanan and Beaumont, Mark A",
  abstract  = "As the world becomes increasingly complex, so do the statistical
               models required to analyse the challenging problems ahead. For
               the very first time in a single volume, the Handbook of
               Approximate Bayesian Computation (ABC) presents an extensive
               overview of the theory, practice and application of ABC methods.
               These simple, but powerful statistical techniques, take Bayesian
               statistics beyond the need to specify overly simplified models,
               to the setting where the model is defined only as a process that
               generates data. This process can be arbitrarily complex, to the
               point where standard Bayesian techniques based on working with
               tractable likelihood functions would not be viable. ABC methods
               finesse the problem of model complexity within the Bayesian
               framework by exploiting modern computational power, thereby
               permitting approximate Bayesian analyses of models that would
               otherwise be impossible to implement. The Handbook of ABC
               provides illuminating insight into the world of Bayesian
               modelling for intractable models for both experts and newcomers
               alike. It is an essential reference book for anyone interested
               in learning about and implementing ABC techniques to analyse
               complex models in the modern world.",
  publisher = "CRC Press, Taylor and Francis Group",
  month     =  sep,
  year      =  2018,
  url       = "https://play.google.com/store/books/details?id=t969tQEACAAJ",
  keywords  = "ABC;Module - Randomization",
  language  = "en",
  isbn      = "9781315117195"
}

@ARTICLE{Weiss1998-ce,
  title    = "Inference of population history using a likelihood approach",
  author   = "Weiss, G and von Haeseler, A",
  abstract = "We introduce an approach to revealing the likelihood of different
              population histories that utilizes an explicit model of sequence
              evolution for the DNA segment under study. Based on a
              phylogenetic tree reconstruction method we show that a Tamura-Nei
              model with heterogeneous mutation rates is a fair description of
              the evolutionary process of the hypervariable region I of the
              mitochondrial DNA from humans. Assuming this complex model still
              allows the estimation of population history parameters, we
              suggest a likelihood approach to conducting statistical inference
              within a class of expansion models. More precisely, the
              likelihood of the data is based on the mean pairwise differences
              between DNA sequences and the number of variable sites in a
              sample. The use of likelihood ratios enables comparison of
              different hypotheses about population history, such as constant
              population size during the past or an increase or decrease of
              population size starting at some point back in time. This method
              was applied to show that the population of the Basques has
              expanded, whereas that of the Biaka pygmies is most likely
              decreasing. The Nuu-Chah-Nulth data are consistent with a model
              of constant population.",
  journal  = "Genetics",
  volume   =  149,
  number   =  3,
  pages    = "1539--1546",
  month    =  jul,
  year     =  1998,
  url      = "http://dx.doi.org/10.1093/genetics/149.3.1539",
  keywords = "ABC;Module - Randomization",
  language = "en",
  issn     = "0016-6731",
  pmid     = "9649540",
  doi      = "10.1093/genetics/149.3.1539",
  pmc      = "PMC1460236"
}

@ARTICLE{Scrucca2017-yg,
  title     = "On Some Extensions to {GA} Package: Hybrid Optimisation,
               Parallelisation and Islands {EvolutionOn} some extensions to
               {GA} package: hybrid optimisation, parallelisation and islands
               evolution",
  author    = "Scrucca, Luca",
  abstract  = "Genetic algorithms are stochastic iterative algorithms in which
               a population of individuals evolve by emulating the process of
               biological evolution and natural selection. The R package GA
               provides a collection of general purpose functions for
               optimisation using genetic algorithms. This paper describes some
               enhancements recently introduced in version 3 of the package. In
               particular, hybrid GAs have been implemented by including the
               option to perform local searches during the evolution. This
               allows to combine the power of genetic algorithms with the speed
               of a local optimiser. Another major improvement is the provision
               of facilities for parallel computing. Parallelisation has been
               implemented using both the master-slave approach and the islands
               evolution model. Several examples of usage are presented, with
               both real-world data examples and benchmark functions, showing
               that often high-quality solutions can be obtained more
               efficiently.",
  journal   = "R Journal",
  publisher = "The R Foundation",
  volume    =  9,
  number    =  1,
  pages     = "187",
  year      =  2017,
  url       = "https://journal.r-project.org/archive/2017/RJ-2017-008/index.html",
  keywords  = "Genetic algorithm;Module - Randomization",
  language  = "en",
  issn      = "2073-4859",
  doi       = "10.32614/rj-2017-008"
}
