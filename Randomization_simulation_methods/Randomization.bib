% Generated by Paperpile. Check out https://paperpile.com for more information.
% BibTeX export options can be customized via Settings -> BibTeX.

@ARTICLE{Beaumont2010-vr,
  title       = "Approximate Bayesian Computation in Evolution and Ecology",
  author      = "Beaumont, Mark A",
  journal     = "Annu. Rev. Ecol. Evol. Syst.",
  volume      =  41,
  number      =  1,
  pages       = "379--406",
  year        =  2010,
  url         = "http://www.annualreviews.org/doi/abs/10.1146/annurev-ecolsys-102209-144621",
  keywords    = "ABC;Module - Randomization",
  issn        = "1543-592X",
  doi         = "10.1146/annurev-ecolsys-102209-144621",
  original_id = "855cf87d-7976-0e5e-a3fc-ef5e2f1b10c3"
}

@ARTICLE{Storey2003-mz,
  title       = "Statistical significance for genomewide studies",
  author      = "Storey, John D and Tibshirani, Robert",
  abstract    = "With the increase in genomewide experiments and the sequencing
                 of multiple genomes, the analysis of large data sets has
                 become commonplace in biology. It is often the case that
                 thousands of features in a genomewide data set are tested
                 against some null hypothesis, where a number of features are
                 expected to be significant. Here we propose an approach to
                 measuring statistical significance in these genomewide studies
                 based on the concept of the false discovery rate. This
                 approach offers a sensible balance between the number of true
                 and false positives that is automatically calibrated and
                 easily interpreted. In doing so, a measure of statistical
                 significance called the q value is associated with each tested
                 feature. The q value is similar to the well known p value,
                 except it is a measure of significance in terms of the false
                 discovery rate rather than the false positive rate. Our
                 approach avoids a flood of false positive results, while
                 offering a more liberal criterion than what has been used in
                 genome scans for linkage.",
  journal     = "Proc. Natl. Acad. Sci. U. S. A.",
  volume      =  100,
  number      =  16,
  pages       = "9440--9445",
  month       =  aug,
  year        =  2003,
  url         = "http://dx.doi.org/10.1073/pnas.1530509100",
  keywords    = "xx Archived/FDR;Module - Randomization;Quantitative Methods",
  language    = "en",
  issn        = "0027-8424",
  pmid        = "12883005",
  doi         = "10.1073/pnas.1530509100",
  pmc         = "PMC170937",
  original_id = "cb386ad5-cd57-0385-8c7c-4dbdf850cef6"
}

@ARTICLE{Benjamini1995-cw,
  title       = "Controlling the False Discovery Rate: A Practical and Powerful
                 Approach to Multiple Testing",
  author      = "Benjamini, Yoav and Hochberg, Yosef",
  abstract    = "The common approach to the multiplicity problem calls for
                 controlling the familywise error rate (FWER). This approach,
                 though, has faults, and we point out a few. A different
                 approach to problems of multiple significance testing is
                 presented. It calls for controlling the expected proportion of
                 falsely rejected hypotheses-the false discovery rate. This
                 error rate is equivalent to the FWER when all hypotheses are
                 true but is smaller otherwise. Therefore, in problems where
                 the control of the false discovery rate rather than that of
                 the FWER is desired, there is potential for a gain in power. A
                 simple sequential Bonferroni-type procedure is proved to
                 control the false discovery rate for independent test
                 statistics, and a simulation study shows that the gain in
                 power is substantial. The use of the new procedure and the
                 appropriateness of the criterion are illustrated with
                 examples.",
  journal     = "J. R. Stat. Soc. Series B Stat. Methodol.",
  volume      =  57,
  number      =  1,
  pages       = "289--300",
  year        =  1995,
  url         = "http://www.jstor.org/stable/2346101",
  keywords    = "xx Archived/FDR;Module - Randomization;Quantitative Methods",
  issn        = "1369-7412, 0035-9246",
  original_id = "f82b0881-aae3-0dd0-9d8d-150bd55279dc"
}

@ARTICLE{Curran-Everett2000-qv,
  title       = "Multiple comparisons: philosophies and illustrations",
  author      = "Curran-Everett, Douglas",
  abstract    = "Statistical procedures underpin the process of scientific
                 discovery. As researchers, one way we use these procedures is
                 to test the validity of a null hypothesis. Often, we test the
                 validity of more than one null hypothesis. If we fail to use
                 an appropriate procedure to account for this multiplicity,
                 then we are more likely to reach a wrong scientific
                 conclusion[---]we are more likely to make a mistake. In
                 physiology, experiments that involve multiple comparisons are
                 common: of the original articles published in 1997 by the
                 American Physiological Society, ~40\% cite a multiple
                 comparison procedure. In this review, I demonstrate the
                 statistical issue embedded in multiple comparisons, and I
                 summarize the philosophies of handling this issue. I also
                 illustrate the three procedures[---]Newman-Keuls, Bonferroni,
                 least significant difference[---]cited most often in my
                 literature review; each of these procedures is of limited
                 practical value. Last, I demonstrate the false discovery rate
                 procedure, a promising development in multiple comparisons.
                 The false discovery rate procedure may be the best practical
                 solution to the problems of multiple comparisons that exist
                 within physiology and other scientific disciplines.",
  journal     = "Am. J. Physiol. Regul. Integr. Comp. Physiol.",
  volume      =  279,
  number      =  1,
  pages       = "R1--8",
  month       =  jul,
  year        =  2000,
  url         = "http://ajpregu.physiology.org/content/279/1/R1",
  keywords    = "xx Archived/FDR;Module - Randomization;Quantitative Methods",
  issn        = "0363-6119",
  original_id = "42551e24-cdd3-0e20-8f12-bfcc634958d6"
}

@ARTICLE{Csillery2010-dj,
  title       = "Approximate Bayesian Computation ({ABC}) in practice",
  author      = "Csilléry, Katalin and Blum, Michael G B and Gaggiotti, Oscar E
                 and François, Olivier",
  journal     = "Trends Ecol. Evol.",
  volume      =  25,
  number      =  7,
  pages       = "410--418",
  year        =  2010,
  url         = "http://dx.doi.org/10.1016/j.tree.2010.04.001",
  keywords    = "ABC;Module - Randomization",
  issn        = "0169-5347",
  pmid        = "20488578",
  doi         = "10.1016/j.tree.2010.04.001",
  original_id = "f0698de0-8c5c-03c0-a792-d8fcdd808a2f"
}

@ARTICLE{Beaumont2002-kn,
  title       = "Approximate Bayesian Computation in Population Genetics",
  author      = "Beaumont, Mark A and Zhang, Wenyang and Balding, David J",
  abstract    = "We propose a new method for approximate Bayesian statistical
                 inference on the basis of summary statistics. The method is
                 suited to complex problems that arise in population genetics,
                 extending ideas developed in this setting by earlier authors.
                 Properties of the posterior distribution of a parameter, such
                 as its mean or density curve, are approximated without
                 explicit likelihood calculations. This is achieved by fitting
                 a local-linear regression of simulated parameter values on
                 simulated summary statistics, and then substituting the
                 observed summary statistics into the regression equation. The
                 method combines many of the advantages of Bayesian statistical
                 inference with the computational efficiency of methods based
                 on summary statistics. A key advantage of the method is that
                 the nuisance parameters are automatically integrated out in
                 the simulation step, so that the large numbers of nuisance
                 parameters that arise in population genetics problems can be
                 handled without difficulty. Simulation results indicate
                 computational and statistical efficiency that compares
                 favorably with those of alternative methods previously
                 proposed in the literature. We also compare the relative
                 efficiency of inferences obtained using methods based on
                 summary statistics with those obtained directly from the data
                 using MCMC.",
  journal     = "Genetics",
  volume      =  162,
  number      =  4,
  pages       = "2025--2035",
  month       =  dec,
  year        =  2002,
  url         = "http://www.genetics.org/content/162/4/2025",
  keywords    = "ABC;Module - Randomization",
  language    = "en",
  issn        = "0016-6731, 1943-2631",
  pmid        = "12524368",
  original_id = "76290b01-59ff-0930-8ea5-fb8d032ed3ec"
}

@ARTICLE{Turner2012-rw,
  title    = "A tutorial on approximate Bayesian computation",
  author   = "Turner, Brandon M and Van Zandt, Trisha",
  abstract = "This tutorial explains the foundation of approximate Bayesian
              computation (ABC), an approach to Bayesian inference that does
              not require the specification of a likelihood function, and hence
              that can be used to estimate posterior distributions of
              parameters for simulation-based models. We discuss briefly the
              philosophy of Bayesian inference and then present several
              algorithms for ABC. We then apply these algorithms in a number of
              examples. For most of these examples, the posterior distributions
              are known, and so we can compare the estimated posteriors derived
              from ABC to the true posteriors and verify that the algorithms
              recover the true posteriors accurately. We also consider a
              popular simulation-based model of recognition memory (REM) for
              which the true posteriors are unknown. We conclude with a number
              of recommendations for applying ABC methods to solve real-world
              problems.",
  journal  = "J. Math. Psychol.",
  volume   =  56,
  number   =  2,
  pages    = "69--85",
  month    =  apr,
  year     =  2012,
  url      = "http://www.sciencedirect.com/science/article/pii/S0022249612000272",
  keywords = "Approximate Bayesian computation; Tutorial; Bayesian estimation;
              Population Monte Carlo;ABC;Module - Randomization;Bayes
              Readings;Module - Bayes",
  issn     = "0022-2496",
  doi      = "10.1016/j.jmp.2012.02.005"
}

@ARTICLE{Pritchard1999-uh,
  title    = "Population growth of human {Y} chromosomes: a study of {Y}
              chromosome microsatellites",
  author   = "Pritchard, J K and Seielstad, M T and Perez-Lezaun, A and
              Feldman, M W",
  abstract = "We use variation at a set of eight human Y chromosome
              microsatellite loci to investigate the demographic history of the
              Y chromosome. Instead of assuming a population of constant size,
              as in most of the previous work on the Y chromosome, we consider
              a model which permits a period of recent population growth. We
              show that for most of the populations in our sample this model
              fits the data far better than a model with no growth. We estimate
              the demographic parameters of this model for each population and
              also the time to the most recent common ancestor. Since there is
              some uncertainty about the details of the microsatellite mutation
              process, we consider several plausible mutation schemes and
              estimate the variance in mutation size simultaneously with the
              demographic parameters of interest. Our finding of a recent
              common ancestor (probably in the last 120,000 years), coupled
              with a strong signal of demographic expansion in all populations,
              suggests either a recent human expansion from a small ancestral
              population, or natural selection acting on the Y chromosome.",
  journal  = "Mol. Biol. Evol.",
  volume   =  16,
  number   =  12,
  pages    = "1791--1798",
  month    =  dec,
  year     =  1999,
  url      = "http://dx.doi.org/10.1093/oxfordjournals.molbev.a026091",
  keywords = "ABC;Module - Randomization",
  language = "en",
  issn     = "0737-4038",
  pmid     = "10605120",
  doi      = "10.1093/oxfordjournals.molbev.a026091"
}

@ARTICLE{Tavare1997-lg,
  title    = "Inferring coalescence times from {DNA} sequence data",
  author   = "Tavaré, S and Balding, D J and Griffiths, R C and Donnelly, P",
  abstract = "The paper is concerned with methods for the estimation of the
              coalescence time (time since the most recent common ancestor) of
              a sample of intraspecies DNA sequences. The methods take
              advantage of prior knowledge of population demography, in
              addition to the molecular data. While some theoretical results
              are presented, a central focus is on computational methods. These
              methods are easy to implement, and, since explicit formulae tend
              to be either unavailable or unilluminating, they are also more
              useful and more informative in most applications. Extensions are
              presented that allow for the effects of uncertainty in our
              knowledge of population size and mutation rates, for variability
              in population sizes, for regions of different mutation rate, and
              for inference concerning the coalescence time of the entire
              population. The methods are illustrated using recent data from
              the human Y chromosome.",
  journal  = "Genetics",
  volume   =  145,
  number   =  2,
  pages    = "505--518",
  month    =  feb,
  year     =  1997,
  url      = "https://www.ncbi.nlm.nih.gov/pubmed/9071603",
  keywords = "ABC;Module - Randomization",
  language = "en",
  issn     = "0016-6731",
  pmid     = "9071603",
  pmc      = "PMC1207814"
}

@ARTICLE{Weiss1998-ce,
  title    = "Inference of population history using a likelihood approach",
  author   = "Weiss, G and von Haeseler, A",
  abstract = "We introduce an approach to revealing the likelihood of different
              population histories that utilizes an explicit model of sequence
              evolution for the DNA segment under study. Based on a
              phylogenetic tree reconstruction method we show that a Tamura-Nei
              model with heterogeneous mutation rates is a fair description of
              the evolutionary process of the hypervariable region I of the
              mitochondrial DNA from humans. Assuming this complex model still
              allows the estimation of population history parameters, we
              suggest a likelihood approach to conducting statistical inference
              within a class of expansion models. More precisely, the
              likelihood of the data is based on the mean pairwise differences
              between DNA sequences and the number of variable sites in a
              sample. The use of likelihood ratios enables comparison of
              different hypotheses about population history, such as constant
              population size during the past or an increase or decrease of
              population size starting at some point back in time. This method
              was applied to show that the population of the Basques has
              expanded, whereas that of the Biaka pygmies is most likely
              decreasing. The Nuu-Chah-Nulth data are consistent with a model
              of constant population.",
  journal  = "Genetics",
  volume   =  149,
  number   =  3,
  pages    = "1539--1546",
  month    =  jul,
  year     =  1998,
  url      = "http://dx.doi.org/10.1093/genetics/149.3.1539",
  keywords = "ABC;Module - Randomization",
  language = "en",
  issn     = "0016-6731",
  pmid     = "9649540",
  doi      = "10.1093/genetics/149.3.1539",
  pmc      = "PMC1460236"
}
