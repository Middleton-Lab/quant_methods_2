---
title: "Bayesian Workflow"
author:
  - Elizabeth King
  - Kevin Middleton
format:
  revealjs:
    theme: [default, custom.scss]
    standalone: true
    embed-resources: true
    logo: QMLS_Logo.png
    slide-number: true
    show-slide-number: all
    link-external-newwindow: true
bibliography: Bayes.bib
csl: evolution.csl
---

## This week

1. Bayesian workflow
2. Choosing priors and prior predictive simulation
3. Sampling
4. Diagnostics and posteriors


## Workflow

1. Model specification
2. Prior specification
3. Prior predictive simulation / check
4. Sampling for the posterior
5. Diagnostics
6. Summarizing the posterior


## Modeling Tree Size

- Age
- Height

![](https://openoregon.pressbooks.pub/app/uploads/sites/9/2016/09/dbh-definition.jpg)


## Model specification

How will we model the relationship between the outcome and predictor(s)?

$$DBH \sim \beta_0$$

$$DBH \sim \beta_0 + \beta_1 age$$

$$\log DBH \sim \beta_0 + \beta_1 \log height$$

. . .

These look much like R formula statements.


## Distribution of the *outcome* variable

How will we model the distribution of the outcome variable?

- Normal (Gaussian): Majority of biological processes
- Log-normal: Many biological processes (few large measures)
- Binomial (Bernoulli): Counts
- Poisson: Counts per time or area
- Exponential: Time-to-event processes


## Relationships between distributions

<center>
![](https://i.stack.imgur.com/sau6I.png)
</center>


## Model statements

DBH has a normal distribution with some mean ($\mu$) and standard deviation ($\sigma$):

$$DBH \sim Normal(\mu, \sigma)$$

$\mu$ is the sum of $\beta_0$ and $\beta_1$ times $age$:

$$\mu  = \beta_0 + \beta_1 age$$

. . .

or combined:

$$DBH \sim Normal(\beta_0 + \beta_1 age, \sigma)$$


## Prior specification

What are the distributions and ranges of plausible values for the predictors ($\beta_0$, $\beta_1$, and $\sigma$)?

. . .

- $\beta_0$: probably $\geq$ 0 (DBH at age = 0), use domain knowledge
- $\beta_1$: certainly $\geq$ 0 (DBH increases with age), use domain knowledge
- $\sigma$: *must* be $\geq$ 0, probably a small number unless there is a lot of variation in the samples

. . .

Later we will test these with prior predictive checks.


## Bayes rule: Get the posterior

$$\textrm{posterior} = \frac{\textrm{prior} \cdot \textrm{likelihood}}{\textrm{normalizing constant}}$$

. . .

$$f(\mathbf{\beta}, y) = \frac{f(\mathbf{\beta}) \cdot L(\mathbf{\beta} | y)}{f(y)}$$

$f(y)$ the probability of observing the data across all possible $\beta$, which make the probability sum to 1. [BayesRules! has a good explanation](https://www.bayesrulesbook.com/chapter-2.html#shortcut-2).


## Sampling to avoid intractable math

::: {.incremental}
- Historically, only outcomes with a few distributions were feasible (Gaussian, Binomial, Poisson) for hand-calculations.
    - Conjugate priors
- Beta prior for a Binomial model has a Beta posterior
    - Similar for other distributions in the "binomial family"
- Normal (Gaussian) prior $\rightarrow$ Normal posterior
:::


## Sampling to avoid intractable math

::: {.incremental}
- With "fast computing machines" [@Metropolis1953-tc], we can sample (Monte Carlo) from the posterior directly
  - Metropolis algorithm [@Metropolis1953-tc]
  - Metropolis-Hastings algorithm [@Hastings1970-gr]
  - Gibbs sampling [@Geman1984-bc]
  - Hamiltonian Monte Carlo [@Duane1987-cd; @Neal1994-gb; @Hoffman2014-vv]
:::


## References

::: {#refs}
:::

