% Generated by Paperpile. Check out https://paperpile.com for more information.
% BibTeX export options can be customized via Settings -> BibTeX.

@BOOK{Robinson2017-uv,
  title       = "Introduction to Empirical Bayes",
  author      = "Robinson, David",
  publisher   = "Gumroad",
  year        =  2017,
  keywords    = "Bayes Readings;Quantitative Methods",
  original_id = "4a864e8a-125f-0a0a-ae8c-aa869767a12e"
}

@ARTICLE{Mayer2010-ns,
  title       = "Drawing an elephant with four complex parameters",
  author      = "Mayer, JÃ¼rgen and Khairy, Khaled and Howard, Jonathon",
  abstract    = "We define four complex numbers representing the parameters
                 needed to specify an elephantine shape. The real and imaginary
                 parts of these complex numbers are the coefficients of a
                 Fourier coordinate expansion, a powerful tool for reducing the
                 data required to define shapes.",
  journal     = "Am. J. Phys.",
  volume      =  78,
  number      =  6,
  pages       = "648--649",
  month       =  jun,
  year        =  2010,
  url         = "http://scitation.aip.org/content/aapt/journal/ajp/78/6/10.1119/1.3254017",
  keywords    = "Bayes Readings;Quantitative Methods",
  issn        = "0002-9505, 1943-2909",
  doi         = "10.1119/1.3254017",
  original_id = "2eb9d01a-c54d-0ee0-ac3a-e822c260984b"
}

@ARTICLE{Frank2011-xq,
  title       = "Measurement scale in maximum entropy models of species
                 abundance",
  author      = "Frank, S A",
  abstract    = "The consistency of the species abundance distribution across
                 diverse communities has attracted widespread attention. In
                 this paper, I argue that the consistency of pattern arises
                 because diverse ecological mechanisms share a common symmetry
                 with regard to measurement scale. By symmetry, I mean that
                 different ecological processes preserve the same measure of
                 information and lose all other information in the aggregation
                 of various perturbations. I frame these explanations of
                 symmetry, measurement, and aggregation in terms of a recently
                 developed extension to the theory of maximum entropy. I show
                 that the natural measurement scale for the species abundance
                 distribution is log-linear: the information in observations at
                 small population sizes scales logarithmically and, as
                 population size increases, the scaling of information grades
                 from logarithmic to linear. Such log-linear scaling leads
                 naturally to a gamma distribution for species abundance, which
                 matches well with the observed patterns. Much of the variation
                 between samples can be explained by the magnitude at which the
                 measurement scale grades from logarithmic to linear. This
                 measurement approach can be applied to the similar problem of
                 allelic diversity in population genetics and to a wide variety
                 of other patterns in biology.",
  journal     = "J. Evol. Biol.",
  publisher   = "Wiley Online Library",
  volume      =  24,
  number      =  3,
  pages       = "485--496",
  month       =  mar,
  year        =  2011,
  url         = "http://dx.doi.org/10.1111/j.1420-9101.2010.02209.x",
  keywords    = "Bayes Readings",
  language    = "en",
  issn        = "1010-061X, 1420-9101",
  pmid        = "21265915",
  doi         = "10.1111/j.1420-9101.2010.02209.x",
  pmc         = "PMC3078054",
  original_id = "f187b0d1-b900-0150-824b-d03672571fea"
}

@ARTICLE{Frank2009-bd,
  title       = "The common patterns of nature",
  author      = "Frank, S A",
  abstract    = "We typically observe large-scale outcomes that arise from the
                 interactions of many hidden, small-scale processes. Examples
                 include age of disease onset, rates of amino acid
                 substitutions and composition of ecological communities. The
                 macroscopic patterns in each problem often vary around a
                 characteristic shape that can be generated by neutral
                 processes. A neutral generative model assumes that each
                 microscopic process follows unbiased or random stochastic
                 fluctuations: random connections of network nodes; amino acid
                 substitutions with no effect on fitness; species that arise or
                 disappear from communities randomly. These neutral generative
                 models often match common patterns of nature. In this paper, I
                 present the theoretical background by which we can understand
                 why these neutral generative models are so successful. I show
                 where the classic patterns come from, such as the Poisson
                 pattern, the normal or Gaussian pattern and many others. Each
                 classic pattern was often discovered by a simple neutral
                 generative model. The neutral patterns share a special
                 characteristic: they describe the patterns of nature that
                 follow from simple constraints on information. For example,
                 any aggregation of processes that preserves information only
                 about the mean and variance attracts to the Gaussian pattern;
                 any aggregation that preserves information only about the mean
                 attracts to the exponential pattern; any aggregation that
                 preserves information only about the geometric mean attracts
                 to the power law pattern. I present a simple and consistent
                 informational framework of the common patterns of nature based
                 on the method of maximum entropy. This framework shows that
                 each neutral generative model is a special case that helps to
                 discover a particular set of informational constraints; those
                 informational constraints define a much wider domain of
                 non-neutral generative processes that attract to the same
                 neutral pattern.",
  journal     = "J. Evol. Biol.",
  publisher   = "Wiley Online Library",
  volume      =  22,
  number      =  8,
  pages       = "1563--1585",
  month       =  aug,
  year        =  2009,
  url         = "http://dx.doi.org/10.1111/j.1420-9101.2009.01775.x",
  keywords    = "Bayes Readings",
  language    = "en",
  issn        = "1010-061X, 1420-9101",
  pmid        = "19538344",
  doi         = "10.1111/j.1420-9101.2009.01775.x",
  pmc         = "PMC2824446",
  original_id = "964ae305-975e-0fe8-922c-af29932ac6d0"
}

@ARTICLE{Sorensen2015-ey,
  title         = "Bayesian linear mixed models using Stan: A tutorial for
                   psychologists, linguists, and cognitive scientists",
  author        = "Sorensen, Tanner and Vasishth, Shravan",
  abstract      = "With the arrival of the R packages nlme and lme4, linear
                   mixed models (LMMs) have come to be widely used in
                   experimentally-driven areas like psychology, linguistics,
                   and cognitive science. This tutorial provides a practical
                   introduction to fitting LMMs in a Bayesian framework using
                   the probabilistic programming language Stan. We choose Stan
                   (rather than WinBUGS or JAGS) because it provides an elegant
                   and scalable framework for fitting models in most of the
                   standard applications of LMMs. We ease the reader into
                   fitting increasingly complex LMMs, first using a
                   two-condition repeated measures self-paced reading study,
                   followed by a more complex $2\times 2$ repeated measures
                   factorial design that can be generalized to much more
                   complex designs.",
  journal       = "Tutor. Quant. Methods Psychol.",
  number        =  2,
  pages         = "175--200",
  month         =  jun,
  year          =  2015,
  url           = "http://arxiv.org/abs/1506.06201",
  keywords      = "Bayes Readings",
  archivePrefix = "arXiv",
  eprint        = "1506.06201",
  primaryClass  = "stat.ME",
  arxivid       = "1506.06201",
  doi           = "10.20982/tqmp.12.3.p175",
  original_id   = "61ad1205-916f-0403-a068-684b05ba75b3"
}

@ARTICLE{Kruschke2018-ft,
  title       = "Bayesian data analysis for newcomers",
  author      = "Kruschke, John K and Liddell, Torrin M",
  abstract    = "This article explains the foundational concepts of Bayesian
                 data analysis using virtually no mathematical notation.
                 Bayesian ideas already match your intuitions from everyday
                 reasoning and from traditional data analysis. Simple examples
                 of Bayesian data analysis are presented that illustrate how
                 the information delivered by a Bayesian analysis can be
                 directly interpreted. Bayesian approaches to null-value
                 assessment are discussed. The article clarifies misconceptions
                 about Bayesian methods that newcomers might have acquired
                 elsewhere. We discuss prior distributions and explain how they
                 are not a liability but an important asset. We discuss the
                 relation of Bayesian data analysis to Bayesian models of mind,
                 and we briefly discuss what methodological problems Bayesian
                 data analysis is not meant to solve. After you have read this
                 article, you should have a clear sense of how Bayesian data
                 analysis works and the sort of information it delivers, and
                 why that information is so intuitive and useful for drawing
                 conclusions from data.",
  journal     = "Psychon. Bull. Rev.",
  volume      =  25,
  number      =  1,
  pages       = "155--177",
  month       =  feb,
  year        =  2018,
  url         = "http://dx.doi.org/10.3758/s13423-017-1272-1",
  keywords    = "Bayes factor; Bayesian analysis; Bayesian model; Confidence
                 interval; Highest density interval; Null hypothesis
                 significance test; Region of practical equivalence;
                 Replication crisis; p value;Bayes Readings",
  language    = "en",
  issn        = "1069-9384, 1531-5320",
  pmid        = "28405907",
  doi         = "10.3758/s13423-017-1272-1",
  original_id = "40d67dae-5c37-0a22-bc61-6a557768f77e"
}

@ARTICLE{Chib1995-ok,
  title       = "Understanding the {Metropolis-Hastings} Algorithm",
  author      = "Chib, Siddhartha and Greenberg, Edward",
  abstract    = "We provide a detailed, introductory exposition of the
                 Metropolis-Hastings algorithm, a powerful Markov chain method
                 to simulate multivariate distributions. A simple, intuitive
                 derivation of this method is given along with guidance on
                 implementation. Also discussed are two applications of the
                 algorithm, one for implementing acceptance-rejection sampling
                 when a blanketing function is not available and the other for
                 implementing the algorithm with block-at-a-time scans. In the
                 latter situation, many different algorithms, including the
                 Gibbs sampler, are shown to be special cases of the
                 Metropolis-Hastings algorithm. The methods are illustrated
                 with examples.",
  journal     = "Am. Stat.",
  publisher   = "[American Statistical Association, Taylor \& Francis, Ltd.]",
  volume      =  49,
  number      =  4,
  pages       = "327--335",
  year        =  1995,
  url         = "http://www.jstor.org/stable/2684568",
  keywords    = "Bayes Readings;Quantitative Methods",
  issn        = "0003-1305",
  doi         = "10.2307/2684568",
  original_id = "0b22e223-1af1-0b5f-83f4-164716dec5c3"
}

@MISC{Yarkoni2016-uz,
  title        = "Bambi: A simple interface for fitting Bayesian mixed effects
                  models",
  author       = "Yarkoni, Tal and Westfall, Jake",
  abstract     = "The popularity of Bayesian statistical methods has increased
                  dramatically among psychologists in recent years; however,
                  many common types of mixed effects models remain difficult to
                  fit in a Bayesian setting. Here we introduce an openÂ­source
                  Python package named Bambi (BAyesian Model Building
                  Interface) that is built on top of the powerful PyMC3
                  probabilistic programming framework and makes it easy to
                  specify complex generalized linear mixed effects models using
                  a formula notation similar to those found in packages like
                  lme4 and nlme. We demonstrate Bambi's versatility and
                  ease-Â­of-Â­use by applying it to three social and personality
                  psychology datasets that span a range of common statistical
                  modelsÂ­Â­including multiple regression, logistic regression,
                  and mixedÂ­effects modeling with crossed random effects. We
                  conclude with a discussion of the strengths and weaknesses of
                  a Bayesian approach in the context of common statistical
                  practice in psychology.",
  publisher    = "Open Science Framework",
  month        =  sep,
  year         =  2016,
  url          = "https://files.osf.io/v1/resources/szrru/providers/osfstorage/57df08ba594d9001e51ee13a",
  howpublished = "Open Science Framework",
  keywords     = "Social and Behavioral Sciences,Psychology; Physical Sciences
                  and Mathematics,Statistics and Probability,Statistical
                  Methodology; open-source software; Bayesian inference; mixed
                  models; statistics;Bayes Readings",
  original_id  = "fbd928f3-996e-0612-9cc0-254ea3452368"
}

@ARTICLE{Westfall2017-sn,
  title         = "Statistical details of the default priors in the Bambi
                   library",
  author        = "Westfall, Jacob",
  abstract      = "This is a companion paper to Yarkoni and Westfall (2017),
                   which describes the Python package Bambi for estimating
                   Bayesian generalized linear mixed models using a simple
                   interface. Here I give the statistical details underlying
                   the default, weakly informative priors used in all models
                   when the user does not specify the priors. Our approach is
                   to first deduce what the variances of the slopes would be if
                   we were instead to have defined the priors on the partial
                   correlation scale, and then to set independent Normal priors
                   on the slopes with variances equal to these implied
                   variances. Our approach is similar in spirit to that of
                   Zellner's g-prior (Zellner 1986), in that it involves a
                   multivariate normal prior on the regression slopes, with a
                   tuning parameter to control the width or informativeness of
                   the priors irrespective of the scales of the data and
                   predictors. The primary differences are that here the tuning
                   parameter is directly interpretable as the standard
                   deviation of the distribution of plausible partial
                   correlations, and that this tuning parameter can have
                   different values for different coefficients. The default
                   priors for the intercepts and random effects are ultimately
                   based on the prior slope variances.",
  month         =  feb,
  year          =  2017,
  url           = "http://arxiv.org/abs/1702.01201",
  keywords      = "Bayes Readings",
  archivePrefix = "arXiv",
  eprint        = "1702.01201",
  primaryClass  = "stat.AP",
  arxivid       = "1702.01201",
  original_id   = "f850b5de-31ce-0f01-8a1c-9645677c19ed"
}

@ARTICLE{Puga2015-aq,
  title       = "Points of significance: Bayesian statistics",
  author      = "Puga, Jorge LÃ³pez and Krzywinski, Martin and Altman, Naomi",
  abstract    = "Today's predictions are tomorrow's priors.",
  journal     = "Nat. Methods",
  publisher   = "Nature Research",
  volume      =  12,
  number      =  5,
  pages       = "377--378",
  month       =  apr,
  year        =  2015,
  url         = "http://dx.doi.org/10.1038/nmeth.3368",
  keywords    = "Bayes Readings;Quantitative Methods",
  language    = "en",
  issn        = "1548-7091, 1548-7105",
  doi         = "10.1038/nmeth.3368",
  original_id = "8eeb09aa-4a60-06c1-9f47-20a27a506e43"
}

@ARTICLE{Metropolis1953-tc,
  title       = "Equation of state calculations by fast computing machines",
  author      = "Metropolis, Nicholas and Rosenbluth, Arianna W and Rosenbluth,
                 Marshall N and Teller, Augusta H and Teller, Edward",
  journal     = "J. Chem. Phys.",
  publisher   = "AIP",
  volume      =  21,
  number      =  6,
  pages       = "1087--1092",
  year        =  1953,
  url         = "http://aip.scitation.org/doi/abs/10.1063/1.1699114",
  keywords    = "Bayes Readings",
  issn        = "0021-9606",
  doi         = "10.1063/1.1699114",
  original_id = "d302fa8f-94c1-0dde-9189-c91923529ca0"
}

@ARTICLE{Van_Ravenzwaaij2016-wj,
  title       = "A simple introduction to Markov Chain {Monte-Carlo} sampling",
  author      = "van Ravenzwaaij, Don and Cassey, Pete and Brown, Scott D",
  abstract    = "Markov Chain Monte-Carlo (MCMC) is an increasingly popular
                 method for obtaining information about distributions,
                 especially for estimating posterior distributions in Bayesian
                 inference. This article provides a very basic introduction to
                 MCMC sampling. It describes what MCMC is, and what it can be
                 used for, with simple illustrative examples. Highlighted are
                 some of the benefits and limitations of MCMC sampling, as well
                 as different approaches to circumventing the limitations most
                 likely to trouble cognitive scientists.",
  journal     = "Psychon. Bull. Rev.",
  month       =  mar,
  year        =  2016,
  url         = "http://dx.doi.org/10.3758/s13423-016-1015-8",
  keywords    = "Bayesian inference; MCMC; Markov Chain MonteâCarlo;
                 Tutorial;Bayes Readings",
  language    = "en",
  issn        = "1069-9384, 1531-5320",
  pmid        = "26968853",
  doi         = "10.3758/s13423-016-1015-8",
  original_id = "b638b6ca-8314-032e-a4b9-4c73b11f12d5"
}

@INCOLLECTION{Neal2011-zk,
  title       = "{MCMC} using Hamiltonian dynamics",
  booktitle   = "Handbook of Markov Chain Monte Carlo",
  author      = "Neal, Radford M",
  editor      = "Brooks, Steve and Gelman, Andrew and Jones, Galin L and Meng,
                 Xaio-Li",
  publisher   = "CRC Press",
  pages       = "113--162",
  year        =  2011,
  url         = "https://books.google.com/books?hl=en&lr=&id=qfRsAIKZ4rIC&oi=fnd&pg=PA113&dq=MCMC+Using+Hamiltonian+Dynamics+Neal&ots=RbA9eQ1b-R&sig=hAqU_YzoaLuSaG1A0Z2tNxvfroc",
  keywords    = "Bayes Readings",
  original_id = "76a69667-0f81-0879-95ac-f9104a09c60d"
}

@ARTICLE{Kruschke2018-vc,
  title       = "The Bayesian New Statistics: Hypothesis testing, estimation,
                 meta-analysis, and power analysis from a Bayesian perspective",
  author      = "Kruschke, John K and Liddell, Torrin M",
  abstract    = "In the practice of data analysis, there is a conceptual
                 distinction between hypothesis testing, on the one hand, and
                 estimation with quantified uncertainty on the other. Among
                 frequentists in psychology, a shift of emphasis from
                 hypothesis testing to estimation has been dubbed ``the New
                 Statistics'' (Cumming 2014). A second conceptual distinction
                 is between frequentist methods and Bayesian methods. Our main
                 goal in this article is to explain how Bayesian methods
                 achieve the goals of the New Statistics better than
                 frequentist methods. The article reviews frequentist and
                 Bayesian approaches to hypothesis testing and to estimation
                 with confidence or credible intervals. The article also
                 describes Bayesian approaches to meta-analysis, randomized
                 controlled trials, and power analysis.",
  journal     = "Psychon. Bull. Rev.",
  volume      =  25,
  number      =  1,
  pages       = "178--206",
  month       =  feb,
  year        =  2018,
  url         = "http://dx.doi.org/10.3758/s13423-016-1221-4",
  keywords    = "Bayes factor; Bayesian inference; Confidence interval;
                 Credible interval; Effect size; Equivalence testing; Highest
                 density interval; Meta-analysis; Null hypothesis significance
                 testing; Power analysis; Randomized controlled trial; Region
                 of practical equivalence;Bayes Readings",
  language    = "en",
  issn        = "1069-9384, 1531-5320",
  pmid        = "28176294",
  doi         = "10.3758/s13423-016-1221-4",
  original_id = "103d4b97-f80c-06f7-ae78-3b00dbcf0c1b"
}

@BOOK{Kruschke2015-rp,
  title       = "{Doing Bayesian Data Analysis: a Tutorial with R, {JAGS}, and
                 Stan}",
  author      = "Kruschke, John K",
  abstract    = "Provides an accessible approach to Bayesian data analysis, as
                 material is explained clearly with concrete examples. The book
                 begins with the basics, including essential concepts of
                 probability and random sampling, and gradually progresses to
                 advanced hierarchical modeling methods for realistic data.",
  publisher   = "Academic Press",
  edition     = "2nd",
  year        =  2015,
  url         = "http://www.sciencedirect.com/science/article/pii/B9780124058880099992",
  address     = "Boston, MA",
  keywords    = "Alligator\_Morphology;Bayes Readings;Quantitative Methods",
  language    = "English",
  isbn        = "9780124058880, 9780124058880",
  doi         = "10.1016/B978-0-12-405888-0.09999-2",
  original_id = "169585e5-1d15-0a09-b3e0-aadaa8f8a871"
}

@ARTICLE{Rue2007-vw,
  title       = "Approximate Bayesian inference for latent Gaussian models
                 using integrated nested Laplace approximations. Statistics
                 Report No. 1",
  author      = "Rue, H and Martino, S and Chopin, N",
  journal     = "J. R. Statist. Soc. B",
  publisher   = "Tech. rep., Department of Mathematical Sciences, Norwegian
                 University of Science and Technology Trondheim, Norway",
  volume      =  71,
  number      =  2,
  pages       = "319--392",
  year        =  2007,
  url         = "https://www.math.ntnu.no/~hrue/r-inla.org/papers/inla-rss.pdf",
  keywords    = "Bayes Readings",
  original_id = "0b5b0771-33f5-0808-9216-d47f91b1b1ff"
}

@ARTICLE{Simpson2014-iz,
  title         = "Penalising model component complexity: A principled,
                   practical approach to constructing priors",
  author        = "Simpson, Daniel P and Rue, HÃ¥vard and Martins, Thiago G and
                   Riebler, Andrea and SÃ¸rbye, Sigrunn H",
  abstract      = "In this paper, we introduce a new concept for constructing
                   prior distributions. We exploit the natural nested structure
                   inherent to many model components, which defines the model
                   component to be a flexible extension of a base model. Proper
                   priors are defined to penalise the complexity induced by
                   deviating from the simpler base model and are formulated
                   after the input of a user-defined scaling parameter for that
                   model component, both in the univariate and the multivariate
                   case. These priors are invariant to reparameterisations,
                   have a natural connection to Jeffreys' priors, are designed
                   to support Occam's razor and seem to have excellent
                   robustness properties, all which are highly desirable and
                   allow us to use this approach to define default prior
                   distributions. Through examples and theoretical results, we
                   demonstrate the appropriateness of this approach and how it
                   can be applied in various situations.",
  month         =  mar,
  year          =  2014,
  url           = "http://arxiv.org/abs/1403.4630",
  keywords      = "Bayes Readings",
  archivePrefix = "arXiv",
  eprint        = "1403.4630",
  primaryClass  = "stat.ME",
  arxivid       = "1403.4630",
  original_id   = "f1995ab6-4ef7-0d0c-b23f-5d86cb1aedd8"
}

@ARTICLE{Robert2008-yk,
  title         = "Harold Jeffreys's Theory of Probability Revisited",
  author        = "Robert, Christian P and Chopin, Nicolas and Rousseau, Judith",
  abstract      = "Published exactly seventy years ago, Jeffreys's Theory of
                   Probability (1939) has had a unique impact on the Bayesian
                   community and is now considered to be one of the main
                   classics in Bayesian Statistics as well as the initiator of
                   the objective Bayes school. In particular, its advances on
                   the derivation of noninformative priors as well as on the
                   scaling of Bayes factors have had a lasting impact on the
                   field. However, the book reflects the characteristics of the
                   time, especially in terms of mathematical rigor. In this
                   paper we point out the fundamental aspects of this reference
                   work, especially the thorough coverage of testing problems
                   and the construction of both estimation and testing
                   noninformative priors based on functional divergences. Our
                   major aim here is to help modern readers in navigating in
                   this difficult text and in concentrating on passages that
                   are still relevant today.",
  month         =  apr,
  year          =  2008,
  url           = "http://arxiv.org/abs/0804.3173",
  keywords      = "Bayes Readings",
  archivePrefix = "arXiv",
  eprint        = "0804.3173",
  primaryClass  = "math.ST",
  arxivid       = "0804.3173",
  doi           = "10.1214/09-STS284",
  original_id   = "6a0f8afe-0e63-06fd-b9a6-17c2323a60a1"
}

@ARTICLE{Bates2015-ej,
  title       = "Parsimonious Mixed Models",
  author      = "Bates, Douglas and Kliegl, Reinhold and Vasishth, Shravan and
                 Baayen, Harald",
  abstract    = "The analysis of experimental data with mixed-effects models
                 requires decisions about the specification of the appropriate
                 random-effects structure. Recently, Barr et al. (2013)
                 recommended fitting 'maximal' models with all possible random
                 effect components included. Estimation of maximal models,
                 however, may not converge. We show that failure to converge
                 typically is not due to a suboptimal estimation algorithm, but
                 is a consequence of attempting to fit a model that is too
                 complex to be properly supported by the data, irrespective of
                 whether estimation is based on maximum likelihood or on
                 Bayesian hierarchical modeling with uninformative or weakly
                 informative priors. Importantly, even under convergence,
                 overparameterization may lead to uninterpretable models. We
                 provide diagnostic tools for detecting overparameterization
                 and guiding model simplification. Finally, we clarify that the
                 simulations on which Barr et al. base their recommendations
                 are atypical for real data. A detailed example is provided of
                 how subject-related attentional fluctuation across trials may
                 further qualify statistical inferences about fixed effects,
                 and of how such nonlinear effects can be accommodated within
                 the mixed-effects modeling framework.",
  journal     = "arXiv:1506.04967 [stat]",
  month       =  jun,
  year        =  2015,
  url         = "http://arxiv.org/abs/1506.04967",
  keywords    = "Bayes Readings",
  original_id = "205ada35-3d04-0367-9db1-bc1afbac60c1"
}

@ARTICLE{Dienes2011-wm,
  title       = "Bayesian Versus Orthodox Statistics: Which Side Are You On?",
  author      = "Dienes, Z",
  journal     = "Perspect. Psychol. Sci.",
  volume      =  6,
  number      =  3,
  pages       = "274--290",
  month       =  may,
  year        =  2011,
  url         = "http://pps.sagepub.com/lookup/doi/10.1177/1745691611406920",
  keywords    = "Bayes Readings",
  language    = "en",
  issn        = "1745-6916, 1745-6924",
  doi         = "10.1177/1745691611406920",
  original_id = "b7ab93c5-08f3-01b3-87c2-bd94238a1695"
}

@ARTICLE{Lee2008-ir,
  title       = "Three case studies in the Bayesian analysis of cognitive
                 models",
  author      = "Lee, M D",
  journal     = "Psychon. Bull. Rev.",
  volume      =  15,
  number      =  1,
  pages       = "1--15",
  month       =  feb,
  year        =  2008,
  url         = "http://www.springerlink.com/index/10.3758/PBR.15.1.1",
  keywords    = "Bayes Readings",
  language    = "en",
  issn        = "1069-9384, 1531-5320",
  doi         = "10.3758/PBR.15.1.1",
  original_id = "0c68cff9-d294-08fa-89df-16160da8859f"
}

@ARTICLE{Lindley1993-ta,
  title       = "The analysis of experimental data: The appreciation of tea and
                 wine",
  author      = "Lindley, Dennis V",
  journal     = "Teach. Stat.",
  volume      =  15,
  number      =  1,
  pages       = "22â25",
  year        =  1993,
  url         = "http://onlinelibrary.wiley.com/doi/10.1111/j.1467-9639.1993.tb00252.x/abstract",
  keywords    = "Bayes Readings",
  issn        = "0141-982X",
  original_id = "e722b9d0-cba7-0c45-9830-7b5477c2bd25"
}

@UNPUBLISHED{Lee2015-qd,
  title       = "Determining prior for cognitive models",
  author      = "Lee, Michael D and Vanpaemel, Wolf",
  year        =  2015,
  keywords    = "Bayes Readings",
  original_id = "8d80ff53-e901-0133-9438-84353c6d9589"
}

@ARTICLE{Dienes2014-ol,
  title       = "Using Bayes to get the most out of non-significant results",
  author      = "Dienes, Zoltan",
  abstract    = "No scientific conclusion follows automatically from a
                 statistically non-significant result, yet people routinely use
                 non-significant results to guide conclusions about the status
                 of theories (or the effectiveness of practices). To know
                 whether a non-significant result counts against a theory, or
                 if it just indicates data insensitivity, researchers must use
                 one of: power, intervals (such as confidence or credibility
                 intervals), or else an indicator of the relative evidence for
                 one theory over another, such as a Bayes factor. I argue Bayes
                 factors allow theory to be linked to data in a way that
                 overcomes the weaknesses of the other approaches.
                 Specifically, Bayes factors use the data themselves to
                 determine their sensitivity in distinguishing theories (unlike
                 power), and they make use of those aspects of a theory's
                 predictions that are often easiest to specify (unlike power
                 and intervals, which require specifying the minimal
                 interesting value in order to address theory). Bayes factors
                 provide a coherent approach to determining whether
                 non-significant results support a null hypothesis over a
                 theory, or whether the data are just insensitive. They allow
                 accepting and rejecting the null hypothesis to be put on an
                 equal footing. Concrete examples are provided to indicate the
                 range of application of a simple online Bayes calculator,
                 which reveal both the strengths and weaknesses of Bayes
                 factors.",
  journal     = "Front. Psychol.",
  volume      =  5,
  pages       = "781",
  year        =  2014,
  url         = "http://journal.frontiersin.org/article/10.3389/fpsyg.2014.00781/full",
  keywords    = "Bayes Readings",
  doi         = "10.3389/fpsyg.2014.00781",
  original_id = "2f6d541a-9ad9-0fcd-9943-fe66bc04c816"
}

@ARTICLE{Vehtari2017-we,
  title       = "Practical Bayesian model evaluation using leave-one-out
                 cross-validation and {WAIC}",
  author      = "Vehtari, Aki and Gelman, Andrew and Gabry, Jonah",
  abstract    = "Leave-one-out cross-validation (LOO) and the widely applicable
                 information criterion (WAIC) are methods for estimating
                 pointwise out-of-sample prediction accuracy from a fitted
                 Bayesian model using the log-likelihood evaluated at the
                 posterior simulations of the parameter values. LOO and WAIC
                 have various advantages over simpler estimates of predictive
                 error such as AIC and DIC but are less used in practice
                 because they involve additional computational steps. Here we
                 lay out fast and stable computations for LOO and WAIC that can
                 be performed using existing simulation draws. We introduce an
                 efficient computation of LOO using Pareto-smoothed importance
                 sampling (PSIS), a new procedure for regularizing importance
                 weights. Although WAIC is asymptotically equal to LOO, we
                 demonstrate that PSIS-LOO is more robust in the finite case
                 with weak priors or influential observations. As a byproduct
                 of our calculations, we also obtain approximate standard
                 errors for estimated predictive errors and for comparison of
                 predictive errors between two models. We implement the
                 computations in an R package called loo and demonstrate using
                 models fit with the Bayesian inference package Stan.",
  journal     = "Stat. Comput.",
  volume      =  27,
  number      =  5,
  pages       = "1413--1432",
  month       =  sep,
  year        =  2017,
  url         = "https://doi.org/10.1007/s11222-016-9696-4",
  keywords    = "xx Archived/AJPA Reply;CF Growth/Bayes general;Bayes
                 Readings;Quantitative Methods",
  issn        = "0960-3174",
  doi         = "10.1007/s11222-016-9696-4",
  original_id = "c7c0bafe-8178-0415-9e5d-49d6fdc986f0"
}

@ARTICLE{Wagenmakers2006-nr,
  title       = "A Bayesian perspective on hypothesis testing: a comment on
                 Killeen (2005)",
  author      = "Wagenmakers, Eric-Jan and GrÃ¼nwald, Peter",
  journal     = "Psychol. Sci.",
  volume      =  17,
  number      =  7,
  pages       = "641--642; author reply 643--644",
  month       =  jul,
  year        =  2006,
  url         = "http://dx.doi.org/10.1111/j.1467-9280.2006.01757.x",
  keywords    = "Bayes Readings",
  language    = "eng",
  issn        = "0956-7976",
  pmid        = "16866752",
  doi         = "10.1111/j.1467-9280.2006.01757.x",
  original_id = "4c945060-d1d5-09b2-837d-4a6d2a734b2d"
}

@ARTICLE{Gelman2014-jh,
  title       = "Understanding predictive information criteria for Bayesian
                 models",
  author      = "Gelman, Andrew and Hwang, Jessica and Vehtari, Aki",
  abstract    = "We review the Akaike, deviance, and Watanabe-Akaike
                 information criteria from a Bayesian perspective, where the
                 goal is to estimate expected out-of-sample-prediction error
                 using a bias-corrected adjustment of within-sample error. We
                 focus on the choices involved in setting up these measures,
                 and we compare them in three simple examples, one theoretical
                 and two applied. The contribution of this paper is to put all
                 these information criteria into a Bayesian predictive context
                 and to better understand, through small examples, how these
                 methods can apply in practice.",
  journal     = "Stat. Comput.",
  publisher   = "Springer US",
  volume      =  24,
  number      =  6,
  pages       = "997--1016",
  month       =  nov,
  year        =  2014,
  url         = "https://link.springer.com/article/10.1007/s11222-013-9416-2",
  keywords    = "Bayes Readings;Quantitative Methods",
  language    = "en",
  issn        = "0960-3174, 1573-1375",
  doi         = "10.1007/s11222-013-9416-2",
  original_id = "af26e4ef-4903-0725-82de-9b9738c3736a"
}

@ARTICLE{De_Villemereuil2012-js,
  title       = "Bayesian models for comparative analysis integrating
                 phylogenetic uncertainty",
  author      = "de Villemereuil, Pierre and Wells, Jessie A and Edwards,
                 Robert D and Blomberg, Simon P",
  abstract    = "BACKGROUND: Uncertainty in comparative analyses can come from
                 at least two sources: a) phylogenetic uncertainty in the tree
                 topology or branch lengths, and b) uncertainty due to
                 intraspecific variation in trait values, either due to
                 measurement error or natural individual variation. Most
                 phylogenetic comparative methods do not account for such
                 uncertainties. Not accounting for these sources of uncertainty
                 leads to false perceptions of precision (confidence intervals
                 will be too narrow) and inflated significance in hypothesis
                 testing (e.g. p-values will be too small). Although there is
                 some application-specific software for fitting Bayesian models
                 accounting for phylogenetic error, more general and flexible
                 software is desirable. METHODS: We developed models to
                 directly incorporate phylogenetic uncertainty into a range of
                 analyses that biologists commonly perform, using a Bayesian
                 framework and Markov Chain Monte Carlo analyses. RESULTS: We
                 demonstrate applications in linear regression, quantification
                 of phylogenetic signal, and measurement error models.
                 Phylogenetic uncertainty was incorporated by applying a prior
                 distribution for the phylogeny, where this distribution
                 consisted of the posterior tree sets from Bayesian
                 phylogenetic tree estimation programs. The models were
                 analysed using simulated data sets, and applied to a real data
                 set on plant traits, from rainforest plant species in Northern
                 Australia. Analyses were performed using the free and open
                 source software OpenBUGS and JAGS. CONCLUSIONS: Incorporating
                 phylogenetic uncertainty through an empirical prior
                 distribution of trees leads to more precise estimation of
                 regression model parameters than using a single consensus tree
                 and enables a more realistic estimation of confidence
                 intervals. In addition, models incorporating measurement
                 errors and/or individual variation, in one or both variables,
                 are easily formulated in the Bayesian framework. We show that
                 BUGS is a useful, flexible general purpose tool for
                 phylogenetic comparative analyses, particularly for modelling
                 in the face of phylogenetic uncertainty and accounting for
                 measurement error or individual variation in explanatory
                 variables. Code for all models is provided in the BUGS model
                 description language.",
  journal     = "BMC Evol. Biol.",
  volume      =  12,
  pages       = "102",
  month       =  jun,
  year        =  2012,
  url         = "http://dx.doi.org/10.1186/1471-2148-12-102",
  keywords    = "Bayes Readings;Quantitative Methods",
  language    = "en",
  issn        = "1471-2148",
  pmid        = "22741602",
  doi         = "10.1186/1471-2148-12-102",
  pmc         = "PMC3582467",
  original_id = "d4368e65-f759-057b-9139-2a20c4d09426"
}

@ARTICLE{Logacev2016-vh,
  title       = "Understanding underspecification: A comparison of two
                 computational implementations",
  author      = "LogaÄev, Pavel and Vasishth, Shravan",
  abstract    = "Swets et al. (2008. Underspecification of syntactic
                 ambiguities: Evidence from self-paced reading. Memory and
                 Cognition, 36(1), 201-216) presented evidence that the
                 so-called ambiguity advantage [Traxler et al. (1998). Adjunct
                 attachment is not a form of lexical ambiguity resolution.
                 Journal of Memory and Language, 39(4), 558-592], which has
                 been explained in terms of the Unrestricted Race Model, can
                 equally well be explained by assuming underspecification in
                 ambiguous conditions driven by task-demands. Specifically, if
                 comprehension questions require that ambiguities be resolved,
                 the parser tends to make an attachment: when questions are
                 about superficial aspects of the target sentence, readers tend
                 to pursue an underspecification strategy. It is reasonable to
                 assume that individual differences in strategy will play a
                 significant role in the application of such strategies, so
                 that studying average behaviour may not be informative. In
                 order to study the predictions of the good-enough processing
                 theory, we implemented two versions of underspecification: the
                 partial specification model (PSM), which is an implementation
                 of the Swets et al. proposal, and a more parsimonious version,
                 the non-specification model (NSM). We evaluate the relative
                 fit of these two kinds of underspecification to Swets et al.'s
                 data; as a baseline, we also fitted three models that assume
                 no underspecification. We find that a model without
                 underspecification provides a somewhat better fit than both
                 underspecification models, while the NSM model provides a
                 better fit than the PSM. We interpret the results as lack of
                 unambiguous evidence in favour of underspecification; however,
                 given that there is considerable existing evidence for
                 good-enough processing in the literature, it is reasonable to
                 assume that some underspecification might occur. Under this
                 assumption, the results can be interpreted as tentative
                 evidence for NSM over PSM. More generally, our work provides a
                 method for choosing between models of real-time processes in
                 sentence comprehension that make qualitative predictions about
                 the relationship between several dependent variables. We
                 believe that sentence processing research will greatly benefit
                 from a wider use of such methods.",
  journal     = "Q. J. Exp. Psychol.",
  volume      =  69,
  number      =  5,
  pages       = "996--1012",
  year        =  2016,
  url         = "http://dx.doi.org/10.1080/17470218.2015.1134602",
  keywords    = "Computational modelling; Shallow processing;
                 Underspecification;Bayes Readings",
  language    = "en",
  issn        = "1747-0218, 1747-0226",
  pmid        = "26960441",
  doi         = "10.1080/17470218.2015.1134602",
  pmc         = "PMC4926776",
  original_id = "c3c89f74-f060-0410-84a2-6c46e8cd5229"
}

@ARTICLE{Safavi2016-xp,
  title       = "Dependency Resolution Difficulty Increases with Distance in
                 Persian Separable Complex Predicates: Evidence for Expectation
                 and {Memory-Based} Accounts",
  author      = "Safavi, Molood S and Husain, Samar and Vasishth, Shravan",
  abstract    = "Delaying the appearance of a verb in a noun-verb dependency
                 tends to increase processing difficulty at the verb; one
                 explanation for this locality effect is decay and/or
                 interference of the noun in working memory. Surprisal, an
                 expectation-based account, predicts that delaying the
                 appearance of a verb either renders it no more predictable or
                 more predictable, leading respectively to a prediction of no
                 effect of distance or a facilitation. Recently, Husain et al.
                 (2014) suggested that when the exact identity of the upcoming
                 verb is predictable (strong predictability), increasing
                 argument-verb distance leads to facilitation effects, which is
                 consistent with surprisal; but when the exact identity of the
                 upcoming verb is not predictable (weak predictability),
                 locality effects are seen. We investigated Husain et al.'s
                 proposal using Persian complex predicates (CPs), which consist
                 of a non-verbal element-a noun in the current study-and a
                 verb. In CPs, once the noun has been read, the exact identity
                 of the verb is highly predictable (strong predictability);
                 this was confirmed using a sentence completion study. In two
                 self-paced reading (SPR) and two eye-tracking (ET)
                 experiments, we delayed the appearance of the verb by
                 interposing a relative clause (Experiments 1 and 3) or a long
                 PP (Experiments 2 and 4). We also included a simple Noun-Verb
                 predicate configuration with the same distance manipulation;
                 here, the exact identity of the verb was not predictable (weak
                 predictability). Thus, the design crossed Predictability
                 Strength and Distance. We found that, consistent with
                 surprisal, the verb in the strong predictability conditions
                 was read faster than in the weak predictability conditions.
                 Furthermore, greater verb-argument distance led to slower
                 reading times; strong predictability did not neutralize or
                 attenuate the locality effects. As regards the effect of
                 distance on dependency resolution difficulty, these four
                 experiments present evidence in favor of working memory
                 accounts of argument-verb dependency resolution, and against
                 the surprisal-based expectation account of Levy (2008).
                 However, another expectation-based measure, entropy, which was
                 computed using the offline sentence completion data, predicts
                 reading times in Experiment 1 but not in the other
                 experiments. Because participants tend to produce more
                 ungrammatical continuations in the long-distance condition in
                 Experiment 1, we suggest that forgetting due to memory
                 overload leads to greater entropy at the verb.",
  journal     = "Front. Psychol.",
  volume      =  7,
  pages       = "403",
  month       =  mar,
  year        =  2016,
  url         = "http://dx.doi.org/10.3389/fpsyg.2016.00403",
  keywords    = "Persian; complex predicates; entropy; expectation;
                 eye-tracking; locality; self-paced reading; surprisal;Bayes
                 Readings",
  language    = "en",
  issn        = "1664-1078",
  pmid        = "27064660",
  doi         = "10.3389/fpsyg.2016.00403",
  pmc         = "PMC4812816",
  original_id = "4393ea91-ac7d-08a7-b121-3f16329757b4"
}

@ARTICLE{Vasishth2016-or,
  title         = "Statistical methods for linguistic research: Foundational
                   Ideas - Part {I}",
  author        = "Vasishth, Shravan and Nicenboim, Bruno",
  abstract      = "We present the fundamental ideas underlying statistical
                   hypothesis testing using the frequentist framework. We begin
                   with a simple example that builds up the one-sample t-test
                   from the beginning, explaining important concepts such as
                   the sampling distribution of the sample mean, and the iid
                   assumption. Then we examine the p-value in detail, and
                   discuss several important misconceptions about what a
                   p-value does and does not tell us. This leads to a
                   discussion of Type I, II error and power, and Type S and M
                   error. An important conclusion from this discussion is that
                   one should aim to carry out appropriately powered studies.
                   Next, we discuss two common issues we have encountered in
                   psycholinguistics and linguistics: running experiments until
                   significance is reached, and the ``garden-of-forking-paths''
                   problem discussed by Gelman and others, whereby the
                   researcher attempts to find statistical significance by
                   analyzing the data in different ways. The best way to use
                   frequentist methods is to run appropriately powered studies,
                   check model assumptions, clearly separate exploratory data
                   analysis from confirmatory hypothesis testing, and always
                   attempt to replicate results.",
  month         =  jan,
  year          =  2016,
  url           = "http://arxiv.org/abs/1601.01126",
  keywords      = "Bayes Readings",
  archivePrefix = "arXiv",
  eprint        = "1601.01126",
  primaryClass  = "stat.AP",
  arxivid       = "1601.01126",
  original_id   = "bbb34d03-c9ad-02ff-b6f0-fa96c14a5e46"
}

@ARTICLE{Nicenboim2016-uu,
  title         = "Statistical methods for linguistic research: Foundational
                   Ideas - Part {II}",
  author        = "Nicenboim, Bruno and Vasishth, Shravan",
  abstract      = "We provide an introductory review of Bayesian data
                   analytical methods, with a focus on applications for
                   linguistics, psychology, psycholinguistics, and cognitive
                   science. The empirically oriented researcher will benefit
                   from making Bayesian methods part of their statistical
                   toolkit due to the many advantages of this framework, among
                   them easier interpretation of results relative to research
                   hypotheses, and flexible model specification. We present an
                   informal introduction to the foundational ideas behind
                   Bayesian data analysis, using, as an example, a linear mixed
                   models analysis of data from a typical psycholinguistics
                   experiment. We discuss hypothesis testing using the Bayes
                   factor, and model selection using cross-validation. We close
                   with some examples illustrating the flexibility of model
                   specification in the Bayesian framework. Suggestions for
                   further reading are also provided.",
  month         =  jan,
  year          =  2016,
  url           = "http://arxiv.org/abs/1602.00245",
  keywords      = "Bayes Readings",
  archivePrefix = "arXiv",
  eprint        = "1602.00245",
  primaryClass  = "stat.AP",
  arxivid       = "1602.00245",
  original_id   = "fa5987f4-1ca1-0c19-8cf4-4c8b65344ed9"
}

@ARTICLE{Matuschek2015-vn,
  title         = "Balancing Type {I} Error and Power in Linear Mixed Models",
  author        = "Matuschek, Hannes and Kliegl, Reinhold and Vasishth, Shravan
                   and Baayen, Harald and Bates, Douglas",
  abstract      = "Linear mixed-effects models have increasingly replaced
                   mixed-model analyses of variance for statistical inference
                   in factorial psycholinguistic experiments. Although LMMs
                   have many advantages over ANOVA, like ANOVAs, setting them
                   up for data analysis also requires some care. One simple
                   option, when numerically possible, is to fit the full
                   variance-covariance structure of random effects (the maximal
                   model; Barr et al. 2013), presumably to keep Type I error
                   down to the nominal alpha in the presence of random effects.
                   Although it is true that fitting a model with only random
                   intercepts may lead to higher Type I error, fitting a
                   maximal model also has a cost: it can lead to a significant
                   loss of power. We demonstrate this with simulations and
                   suggest that for typical psychological and psycholinguistic
                   data, higher power is achieved without inflating Type I
                   error rate if a model selection criterion is used to select
                   a random effect structure that is supported by the data.",
  month         =  nov,
  year          =  2015,
  url           = "http://arxiv.org/abs/1511.01864",
  keywords      = "Bayes Readings",
  archivePrefix = "arXiv",
  eprint        = "1511.01864",
  primaryClass  = "stat.AP",
  arxivid       = "1511.01864",
  original_id   = "cd72a152-2b8a-05da-8751-c127cdc65041"
}

@ARTICLE{Baayen2015-gg,
  title         = "The cave of Shadows. Addressing the human factor with
                   generalized additive mixed models",
  author        = "Baayen, Harald and Vasishth, Shravan and Bates, Douglas and
                   Kliegl, Reinhold",
  abstract      = "Generalized additive mixed models are introduced as an
                   extension of the generalized linear mixed model which makes
                   it possible to deal with temporal autocorrelational
                   structure in experimental data. This autocorrelational
                   structure is likely to be a consequence of learning,
                   fatigue, or the ebb and flow of attention within an
                   experiment (the `human factor'). Unlike molecules or plots
                   of barley, subjects in psycholinguistic experiments are
                   intelligent beings that depend for their survival on
                   constant adaptation to their environment, including the
                   environment of an experiment. Three data sets illustrate
                   that the human factor may interact with predictors of
                   interest, both factorial and metric. We also show that,
                   especially within the framework of the generalized additive
                   model, in the nonlinear world, fitting maximally complex
                   models that take every possible contingency into account is
                   ill-advised as a modeling strategy. Alternative modeling
                   strategies are discussed for both confirmatory and
                   exploratory data analysis.",
  month         =  nov,
  year          =  2015,
  url           = "http://arxiv.org/abs/1511.03120",
  keywords      = "Bayes Readings",
  archivePrefix = "arXiv",
  eprint        = "1511.03120",
  primaryClass  = "stat.AP",
  arxivid       = "1511.03120",
  original_id   = "87ff9420-1469-002a-9d8a-4eafd9b0149e"
}

@ARTICLE{Aho2014-ke,
  title       = "Model selection for ecologists: the worldviews of {AIC} and
                 {BIC}",
  author      = "Aho, Ken and Derryberry, Dewayne and Peterson, Teri",
  journal     = "Ecology",
  volume      =  95,
  number      =  3,
  pages       = "631--636",
  month       =  mar,
  year        =  2014,
  url         = "https://www.ncbi.nlm.nih.gov/pubmed/24804445",
  keywords    = "Bayes Readings",
  language    = "en",
  issn        = "0012-9658, 1939-9170",
  pmid        = "24804445",
  doi         = "10.1890/13-1452.1",
  original_id = "d2be1453-0ca6-0f1e-bf87-0d4af322cdef"
}

@ARTICLE{Etz2015-yp,
  title       = "J. B. S. Haldane's Contribution to the Bayes Factor Hypothesis
                 Test",
  author      = "Etz, Alexander and Wagenmakers, Eric-Jan",
  abstract    = "This article brings attention to some historical developments
                 that gave rise to the Bayes factor for testing a point null
                 hypothesis against a composite alternative. In line with
                 current thinking, we find that the conceptual innovation - to
                 assign prior mass to a general law - is due to a series of
                 three articles by Dorothy Wrinch and Sir Harold Jeffreys
                 (1919, 1921, 1923). However, our historical investigation also
                 suggests that in 1932 J. B. S. Haldane made an important
                 contribution to the development of the Bayes factor by
                 proposing the use of a mixture prior comprising a point mass
                 and a continuous probability density. Jeffreys was aware of
                 Haldane's work and it may have inspired him to pursue a more
                 concrete statistical implementation for his conceptual ideas.
                 It thus appears that Haldane may have played a much bigger
                 role in the statistical development of the Bayes factor than
                 has hitherto been assumed.",
  year        =  2015,
  url         = "http://adsabs.harvard.edu/abs/2015arXiv151108180E",
  keywords    = "Statistics - Other Statistics, Primary 62F03, secondary
                 62-03;Bayes Readings",
  language    = "en",
  arxivid     = "1511.08180",
  original_id = "bf2badeb-0b27-0b38-93b3-4b9326229a95"
}

@ARTICLE{Rouder2014-ch,
  title       = "Optional stopping: no problem for Bayesians",
  author      = "Rouder, Jeffrey N",
  abstract    = "Optional stopping refers to the practice of peeking at data
                 and then, based on the results, deciding whether or not to
                 continue an experiment. In the context of ordinary
                 significance-testing analysis, optional stopping is
                 discouraged, because it necessarily leads to increased type I
                 error rates over nominal values. This article addresses
                 whether optional stopping is problematic for Bayesian
                 inference with Bayes factors. Statisticians who developed
                 Bayesian methods thought not, but this wisdom has been
                 challenged by recent simulation results of Yu, Sprenger,
                 Thomas, and Dougherty (2013) and Sanborn and Hills (2013). In
                 this article, I show through simulation that the
                 interpretation of Bayesian quantities does not depend on the
                 stopping rule. Researchers using Bayesian methods may employ
                 optional stopping in their own research and may provide
                 Bayesian analysis of secondary data regardless of the employed
                 stopping rule. I emphasize here the proper interpretation of
                 Bayesian quantities as measures of subjective belief on
                 theoretical positions, the difference between frequentist and
                 Bayesian interpretations, and the difficulty of using
                 frequentist intuition to conceptualize the Bayesian approach.",
  journal     = "Psychon. Bull. Rev.",
  volume      =  21,
  number      =  2,
  pages       = "301--308",
  month       =  apr,
  year        =  2014,
  url         = "http://dx.doi.org/10.3758/s13423-014-0595-4",
  keywords    = "Bayes Readings",
  language    = "en",
  issn        = "1069-9384, 1531-5320",
  pmid        = "24659049",
  doi         = "10.3758/s13423-014-0595-4",
  original_id = "5de24a0b-f85a-0f35-9969-4feb0888c549"
}

@ARTICLE{Rouder2005-kz,
  title       = "An introduction to Bayesian hierarchical models with an
                 application in the theory of signal detection",
  author      = "Rouder, Jeffrey N and Lu, Jun",
  abstract    = "Although many nonlinear models of cognition have been proposed
                 in the past 50 years, there has been little consideration of
                 corresponding statistical techniques for their analysis. In
                 analyses with nonlinear models, unmodeled variability from the
                 selection of items or participants may lead to asymptotically
                 biased estimation. This asymptotic bias, in turn, renders
                 inference problematic. We show, for example, that a signal
                 detection analysis of recognition memory data leads to
                 asymptotic underestimation of sensitivity. To eliminate
                 asymptotic bias, we advocate hierarchical models in which
                 participant variability, item variability, and measurement
                 error are modeled simultaneously. By accounting for multiple
                 sources of variability, hierarchical models yield consistent
                 and accurate estimates of participant and item effects in
                 recognition memory. This article is written in tutorial
                 format; we provide an introduction to Bayesian statistics,
                 hierarchical modeling, and Markov chain Monte Carlo
                 computational techniques.",
  journal     = "Psychon. Bull. Rev.",
  volume      =  12,
  number      =  4,
  pages       = "573--604",
  month       =  aug,
  year        =  2005,
  url         = "https://www.ncbi.nlm.nih.gov/pubmed/16447374",
  keywords    = "Bayes Readings",
  language    = "en",
  issn        = "1069-9384, 1531-5320",
  pmid        = "16447374",
  doi         = "10.3758/BF03196750",
  original_id = "594d6785-7e67-029c-a23d-752857b23d00"
}

@ARTICLE{Rouder2012-vf,
  title       = "Default Bayes Factors for Model Selection in Regression",
  author      = "Rouder, Jeffrey N and Morey, Richard D",
  abstract    = "In this article, we present a Bayes factor solution for
                 inference in multiple regression. Bayes factors are principled
                 measures of the relative evidence from data for various models
                 or positions, including models that embed null hypotheses. In
                 this regard, they may be used to state positive evidence for a
                 lack of an effect, which is not possible in conventional
                 significance testing. One obstacle to the adoption of Bayes
                 factor in psychological science is a lack of guidance and
                 software. Recently, Liang, Paulo, Molina, Clyde, and Berger
                 (2008) developed computationally attractive default Bayes
                 factors for multiple regression designs. We provide a web
                 applet for convenient computation and guidance and context for
                 use of these priors. We discuss the interpretation and
                 advantages of the advocated Bayes factor evidence measures.",
  journal     = "Multivariate Behav. Res.",
  volume      =  47,
  number      =  6,
  pages       = "877--903",
  month       =  nov,
  year        =  2012,
  url         = "http://dx.doi.org/10.1080/00273171.2012.734737",
  keywords    = "Bayes Readings",
  language    = "en",
  issn        = "0027-3171",
  pmid        = "26735007",
  doi         = "10.1080/00273171.2012.734737",
  original_id = "d4f24a0d-5db6-0d03-a73c-3a9f477c8fa5"
}

@ARTICLE{Rouder2012-sg,
  title       = "Default Bayes factors for {ANOVA} designs",
  author      = "Rouder, Jeffrey N and Morey, Richard D and Speckman, Paul L
                 and Province, Jordan M",
  abstract    = "Bayes factors have been advocated as superior to p -values for
                 assessing statistical evidence in data. Despite the advantages
                 of Bayes factors and the drawbacks of p -values, inference by
                 p -values is still nearly ubiquitous. One impediment to the
                 adoption of Bayes factors is a lack of practical development,
                 particularly a lack of ready-to-use formulas and algorithms.
                 In this paper, we discuss and expand a set of default Bayes
                 factor tests for ANOVA designs. These tests are based on
                 multivariate generalizations of Cauchy priors on standardized
                 effects, and have the desirable properties of being invariant
                 with respect to linear transformations of measurement units.
                 Moreover, these Bayes factors are computationally convenient,
                 and straightforward sampling algorithms are provided. We cover
                 models with fixed, random, and mixed effects, including random
                 interactions, and do so for within-subject, between-subject,
                 and mixed designs. We extend the discussion to regression
                 models with continuous covariates. We also discuss how these
                 Bayes factors may be applied in nonlinear settings, and show
                 how they are useful in differentiating between the power law
                 and the exponential law of skill acquisition. In sum, the
                 current development makes the computation of Bayes factors
                 straightforward for the vast majority of designs in
                 experimental psychology.",
  journal     = "J. Math. Psychol.",
  volume      =  56,
  number      =  5,
  pages       = "356--374",
  month       =  oct,
  year        =  2012,
  url         = "http://dx.doi.org/10.1016/j.jmp.2012.08.001",
  keywords    = "Bayes factor; Model selection; Bayesian statistics; Linear
                 models;Bayes Readings",
  issn        = "0022-2496",
  doi         = "10.1016/j.jmp.2012.08.001",
  original_id = "0ee57b90-cfd2-0189-b91e-84c2c2f8f004"
}

@ARTICLE{Rouder2009-jh,
  title       = "Bayesian t tests for accepting and rejecting the null
                 hypothesis",
  author      = "Rouder, Jeffrey N and Speckman, Paul L and Sun, Dongchu and
                 Morey, Richard D and Iverson, Geoffrey",
  abstract    = "Progress in science often comes from discovering invariances
                 in relationships among variables; these invariances often
                 correspond to null hypotheses. As is commonly known, it is not
                 possible to state evidence for the null hypothesis in
                 conventional significance testing. Here we highlight a Bayes
                 factor alternative to the conventional t test that will allow
                 researchers to express preference for either the null
                 hypothesis or the alternative. The Bayes factor has a natural
                 and straightforward interpretation, is based on reasonable
                 assumptions, and has better properties than other methods of
                 inference that have been advocated in the psychological
                 literature. To facilitate use of the Bayes factor, we provide
                 an easy-to-use, Web-based program that performs the necessary
                 calculations.",
  journal     = "Psychon. Bull. Rev.",
  volume      =  16,
  number      =  2,
  pages       = "225--237",
  month       =  apr,
  year        =  2009,
  url         = "http://dx.doi.org/10.3758/PBR.16.2.225",
  keywords    = "Bayes Readings",
  language    = "en",
  issn        = "1069-9384, 1531-5320",
  pmid        = "19293088",
  doi         = "10.3758/PBR.16.2.225",
  original_id = "877e5721-7678-0e13-8274-e54fedb5bc69"
}

@ARTICLE{Van_de_Schoot2014-jk,
  title       = "A gentle introduction to Bayesian analysis: applications to
                 developmental research",
  author      = "van de Schoot, Rens and Kaplan, David and Denissen, Jaap and
                 Asendorpf, Jens B and Neyer, Franz J and van Aken, Marcel A G",
  abstract    = "Bayesian statistical methods are becoming ever more popular in
                 applied and fundamental research. In this study a gentle
                 introduction to Bayesian analysis is provided. It is shown
                 under what circumstances it is attractive to use Bayesian
                 estimation, and how to interpret properly the results. First,
                 the ingredients underlying Bayesian methods are introduced
                 using a simplified example. Thereafter, the advantages and
                 pitfalls of the specification of prior knowledge are
                 discussed. To illustrate Bayesian methods explained in this
                 study, in a second example a series of studies that examine
                 the theoretical framework of dynamic interactionism are
                 considered. In the Discussion the advantages and disadvantages
                 of using Bayesian statistics are reviewed, and guidelines on
                 how to report on Bayesian statistics are provided.",
  journal     = "Child Dev.",
  volume      =  85,
  number      =  3,
  pages       = "842--860",
  month       =  may,
  year        =  2014,
  url         = "http://dx.doi.org/10.1111/cdev.12169",
  keywords    = "Bayes Readings",
  language    = "en",
  issn        = "0009-3920, 1467-8624",
  pmid        = "24116396",
  doi         = "10.1111/cdev.12169",
  pmc         = "PMC4158865",
  original_id = "c21358f0-b571-0bc4-84af-90cb75a91f34"
}

@ARTICLE{Wagenmakers2007-tv,
  title       = "A practical solution to the pervasive problems of p values",
  author      = "Wagenmakers, Eric-Jan",
  abstract    = "In the field of psychology, the practice of p value
                 null-hypothesis testing is as widespread as ever. Despite this
                 popularity, or perhaps because of it, most psychologists are
                 not aware of the statistical peculiarities of the p value
                 procedure. In particular, p values are based on data that were
                 never observed, and these hypothetical data are themselves
                 influenced by subjective intentions. Moreover, p values do not
                 quantify statistical evidence. This article reviews these p
                 value problems and illustrates each problem with concrete
                 examples. The three problems are familiar to statisticians but
                 may be new to psychologists. A practical solution to these p
                 value problems is to adopt a model selection perspective and
                 use the Bayesian information criterion (BIC) for statistical
                 inference (Raftery, 1995). The BIC provides an approximation
                 to a Bayesian hypothesis test, does not require the
                 specification of priors, and can be easily calculated from
                 SPSS output.",
  journal     = "Psychon. Bull. Rev.",
  volume      =  14,
  number      =  5,
  pages       = "779--804",
  month       =  oct,
  year        =  2007,
  url         = "https://www.ncbi.nlm.nih.gov/pubmed/18087943",
  keywords    = "Bayes Readings",
  language    = "en",
  issn        = "1069-9384, 1531-5320",
  pmid        = "18087943",
  doi         = "10.3758/BF03194105",
  original_id = "d0cdd0b9-bc1c-0371-8723-f110c1cee652"
}

@ARTICLE{Wetzels2011-am,
  title       = "Statistical Evidence in Experimental Psychology: An Empirical
                 Comparison Using 855 t Tests",
  author      = "Wetzels, Ruud and Matzke, Dora and Lee, Michael D and Rouder,
                 Jeffrey N and Iverson, Geoffrey J and Wagenmakers, Eric-Jan",
  abstract    = "Statistical inference in psychology has traditionally relied
                 heavily on p-value significance testing. This approach to
                 drawing conclusions from data, however, has been widely
                 criticized, and two types of remedies have been advocated. The
                 first proposal is to supplement p values with complementary
                 measures of evidence, such as effect sizes. The second is to
                 replace inference with Bayesian measures of evidence, such as
                 the Bayes factor. The authors provide a practical comparison
                 of p values, effect sizes, and default Bayes factors as
                 measures of statistical evidence, using 855 recently published
                 t tests in psychology. The comparison yields two main results.
                 First, although p values and default Bayes factors almost
                 always agree about what hypothesis is better supported by the
                 data, the measures often disagree about the strength of this
                 support; for 70\% of the data sets for which the p value falls
                 between .01 and .05, the default Bayes factor indicates that
                 the evidence is only anecdotal. Second, effect sizes can
                 provide additional evidence to p values and default Bayes
                 factors. The authors conclude that the Bayesian approach is
                 comparatively prudent, preventing researchers from
                 overestimating the evidence in favor of an effect.",
  journal     = "Perspect. Psychol. Sci.",
  volume      =  6,
  number      =  3,
  pages       = "291--298",
  month       =  may,
  year        =  2011,
  url         = "http://dx.doi.org/10.1177/1745691611406923",
  keywords    = "t test; p value; Bayes factor; effect size; hypothesis
                 testing;Bayes Readings",
  language    = "en",
  issn        = "1745-6916, 1745-6924",
  pmid        = "26168519",
  doi         = "10.1177/1745691611406923",
  original_id = "4e4450bb-f365-0be7-9dec-5031efbe261f"
}

@ARTICLE{Stamps2016-ao,
  title       = "Bayesian Models of Development",
  author      = "Stamps, Judy A and Frankenhuis, Willem E",
  abstract    = "Until recently, biology lacked a framework for studying how
                 information from genes, parental effects, and different
                 personal experiences is combined across the lifetime to affect
                 phenotypic development. Over the past few years, researchers
                 have begun to build such a framework, using models that
                 incorporate Bayesian updating to study the evolution of
                 developmental plasticity and developmental trajectories. Here,
                 we describe the merits of a Bayesian approach to development,
                 review the main findings and implications of the current set
                 of models, and describe predictions that can be tested using
                 protocols already used by empiricists. We suggest that a
                 Bayesian perspective affords a simple and tractable way to
                 conceptualize, explain, and predict how information combines
                 across the lifetime to affect development.",
  journal     = "Trends Ecol. Evol.",
  volume      =  31,
  number      =  4,
  pages       = "260--268",
  month       =  apr,
  year        =  2016,
  url         = "http://dx.doi.org/10.1016/j.tree.2016.01.012",
  keywords    = "Bayes Readings",
  language    = "en",
  issn        = "0169-5347, 1872-8383",
  pmid        = "26896042",
  doi         = "10.1016/j.tree.2016.01.012",
  original_id = "144fe31e-83f1-0f84-a32f-600cfadb748a"
}

@ARTICLE{Panchanathan2016-ft,
  title       = "The evolution of sensitive periods in a model of incremental
                 development",
  author      = "Panchanathan, Karthik and Frankenhuis, Willem E",
  abstract    = "Sensitive periods, in which experience shapes phenotypic
                 development to a larger extent than other periods, are
                 widespread in nature. Despite a recent focus on
                 neural-physiological explanation, few formal models have
                 examined the evolutionary selection pressures that result in
                 developmental mechanisms that produce sensitive periods. Here,
                 we present such a model. We model development as a
                 specialization process during which individuals incrementally
                 adapt to local environmental conditions, while receiving a
                 constant stream of cost-free, imperfect cues to the
                 environmental state. We compute optimal developmental
                 programmes across a range of ecological conditions and use
                 these programmes to simulate developmental trajectories and
                 obtain distributions of mature phenotypes. We highlight four
                 main results. First, matching the empirical record, sensitive
                 periods often result from experience or from a combination of
                 age and experience, but rarely from age alone. Second,
                 individual differences in sensitive periods emerge as a result
                 of stochasticity in cues: individuals who obtain more
                 consistent cue sets lose their plasticity at faster rates.
                 Third, in some cases, experience shapes phenotypes only at a
                 later life stage (lagged effects). Fourth, individuals might
                 perseverate along developmental trajectories despite
                 accumulating evidence suggesting the alternate trajectory is
                 more likely to match the ecology.",
  journal     = "Proc. Biol. Sci.",
  volume      =  283,
  number      =  1823,
  pages       = "20152439",
  month       =  jan,
  year        =  2016,
  url         = "http://dx.doi.org/10.1098/rspb.2015.2439",
  keywords    = "Bayesian updating; adaptation; cues; development; evolution;
                 sensitive periods;Bayes Readings",
  language    = "en",
  issn        = "0962-8452, 1471-2954",
  pmid        = "26817766",
  doi         = "10.1098/rspb.2015.2439",
  pmc         = "PMC4795025",
  original_id = "59021e13-9612-047e-9f68-bf955ad741ea"
}

@TECHREPORT{Clark_undated-jj,
  title       = "Bayesian Basics",
  author      = "Clark, Michael",
  url         = "http://www3.nd.edu/~mclark19/learn/IntroBayes.pdf",
  keywords    = "Bayes Readings",
  original_id = "d7caa949-a20d-05dd-9608-e06faba4f4c3"
}

@ARTICLE{Homan2014-kb,
  title       = "{The no-U-turn sampler: adaptively setting path lengths in
                 Hamiltonian Monte Carlo}",
  author      = "Homan, Matthew D and Gelman, Andrew",
  journal     = "J. Mach. Learn. Res.",
  publisher   = "JMLR.org",
  volume      =  15,
  number      =  1,
  pages       = "1593--1623",
  month       =  jan,
  year        =  2014,
  url         = "http://dl.acm.org/citation.cfm?id=2627435.2638586",
  keywords    = "Bayesian inference, Hamiltonian Monte Carlo, Markov chain
                 Monte Carlo, adaptive Monte Carlo, dual
                 averaging;Alligator\_Morphology;Bayes Readings",
  issn        = "1532-4435",
  original_id = "f66736c0-c60d-0939-9e73-50b57a486433"
}

@ARTICLE{Gelman2015-mt,
  title       = "{Stan: a probabilistic programming language for Bayesian
                 inference and optimization}",
  author      = "Gelman, A and Lee, D and Guo, J",
  abstract    = "Abstract Stan is a free and open-source C++ program that
                 performs Bayesian inference or optimization for arbitrary
                 user-specified models and can be called from the command line,
                 R, Python, Matlab, or Julia and has great promise for fitting
                 large and complex statistical models in many areas of
                 application. We discuss Stan from users' and developers'
                 perspectives and illustrate with a simple but nontrivial
                 nonlinear regression example.",
  journal     = "J. Educ. Behav. Stat.",
  pages       = "1076998615606113",
  year        =  2015,
  url         = "http://jeb.sagepub.com/content/early/2015/10/09/1076998615606113.abstract",
  keywords    = "Alligator\_Morphology;Bayes Readings",
  issn        = "1076-9986",
  original_id = "b0f689f9-b66a-0d67-a9cb-e3e73943b2ee"
}

@ARTICLE{Blum2010-ol,
  title       = "Non-linear regression models for Approximate Bayesian
                 Computation",
  author      = "Blum, Michael G B and FranÃ§ois, Olivier",
  abstract    = "Approximate Bayesian inference on the basis of summary
                 statistics is well-suited to complex problems for which the
                 likelihood is either mathematically or computationally
                 intractable. However the methods that use rejection suffer
                 from the curse of dimensionality when the number of summary
                 statistics is increased. Here we propose a machine-learning
                 approach to the estimation of the posterior density by
                 introducing two innovations. The new method fits a nonlinear
                 conditional heteroscedastic regression of the parameter on the
                 summary statistics, and then adaptively improves estimation
                 using importance sampling. The new algorithm is compared to
                 the state-of-the-art approximate Bayesian methods, and
                 achieves considerable reduction of the computational burden in
                 two examples of inference in statistical genetics and in a
                 queueing model.",
  journal     = "Stat. Comput.",
  publisher   = "Springer US",
  volume      =  20,
  number      =  1,
  pages       = "63--73",
  month       =  jan,
  year        =  2010,
  url         = "http://link.springer.com/article/10.1007/s11222-009-9116-0",
  keywords    = "Bayes Readings",
  language    = "en",
  issn        = "0960-3174, 1573-1375",
  doi         = "10.1007/s11222-009-9116-0",
  original_id = "3154d38c-98ee-09d6-b910-2d5db2013a92"
}

@ARTICLE{VanderPlas2014-km,
  title         = "Frequentism and Bayesianism: A Python-driven Primer",
  author        = "VanderPlas, Jake",
  abstract      = "This paper presents a brief, semi-technical comparison of
                   the essential features of the frequentist and Bayesian
                   approaches to statistical inference, with several
                   illustrative examples implemented in Python. The differences
                   between frequentism and Bayesianism fundamentally stem from
                   differing definitions of probability, a philosophical divide
                   which leads to distinct approaches to the solution of
                   statistical problems as well as contrasting ways of asking
                   and answering questions about unknown parameters. After an
                   example-driven discussion of these differences, we briefly
                   compare several leading Python statistical packages which
                   implement frequentist inference using classical methods and
                   Bayesian inference using Markov Chain Monte Carlo.",
  month         =  nov,
  year          =  2014,
  url           = "http://arxiv.org/abs/1411.5018",
  keywords      = "Bayes Readings;Quantitative Methods",
  archivePrefix = "arXiv",
  eprint        = "1411.5018",
  primaryClass  = "astro-ph.IM",
  arxivid       = "1411.5018",
  original_id   = "91489cbf-aeb0-0efd-9787-1b396a872f41"
}

@ARTICLE{Patil2010-ez,
  title       = "{PyMC}: Bayesian Stochastic Modelling in Python",
  author      = "Patil, Anand and Huard, David and Fonnesbeck, Christopher J",
  abstract    = "This user guide describes a Python package, PyMC, that allows
                 users to efficiently code a probabilistic model and draw
                 samples from its posterior distribution using Markov chain
                 Monte Carlo techniques.",
  journal     = "J. Stat. Softw.",
  volume      =  35,
  number      =  4,
  pages       = "1--81",
  month       =  jul,
  year        =  2010,
  url         = "https://www.ncbi.nlm.nih.gov/pubmed/21603108",
  keywords    = "Bayes Readings",
  language    = "en",
  issn        = "1548-7660",
  pmid        = "21603108",
  pmc         = "PMC3097064",
  original_id = "39aea828-51b6-05ed-a832-8e3a40032ffa"
}

@ARTICLE{Neal2003-ip,
  title       = "Slice sampling",
  author      = "Neal, Radford M",
  abstract    = "Project Euclid - mathematics and statistics online",
  journal     = "Ann. Stat.",
  publisher   = "Institute of Mathematical Statistics",
  volume      =  31,
  number      =  3,
  pages       = "705--767",
  month       =  jun,
  year        =  2003,
  url         = "http://projecteuclid.org/euclid.aos/1056562461",
  keywords    = "Markov chain Monte Carlo; auxiliary variables; adaptive
                 methods; Gibbs sampling; Metropolis algorithm; overrelaxation;
                 dynamical methods;Bayes Readings",
  issn        = "0090-5364, 2168-8966",
  doi         = "10.1214/aos/1056562461",
  original_id = "320161ae-5acb-09db-b3f7-009ffdee95b2"
}

@ARTICLE{Carpenter2017-yg,
  title       = "Stan: A probabilistic programming language",
  author      = "Carpenter, Bob and Gelman, Andrew and Hoffman, Matthew D and
                 Lee, Daniel and Goodrich, Ben and Betancourt, Michael and
                 Brubaker, Marcus and Guo, Jiqiang and Li, Peter and Riddell,
                 Allen",
  journal     = "J. Stat. Softw.",
  publisher   = "Columbia Univ., New York, NY (United States); Harvard Univ.,
                 Cambridge, MA (United States)",
  volume      =  76,
  number      =  1,
  year        =  2017,
  keywords    = "Alligator\_Morphology;xx Archived/AJPA Reply;CF Growth/Bayes
                 general;Bayes Readings",
  original_id = "40060833-1213-0e37-9de7-b9d872c8dc4a"
}

@ARTICLE{Jirsa2017-zm,
  title       = "The Virtual Epileptic Patient: Individualized whole-brain
                 models of epilepsy spread",
  author      = "Jirsa, V K and Proix, T and Perdikis, D and Woodman, M M and
                 Wang, H and Gonzalez-Martinez, J and Bernard, C and BÃ©nar, C
                 and Guye, M and Chauvel, P and Bartolomei, F",
  abstract    = "Individual variability has clear effects upon the outcome of
                 therapies and treatment approaches. The customization of
                 healthcare options to the individual patient should
                 accordingly improve treatment results. We propose a novel
                 approach to brain interventions based on personalized brain
                 network models derived from non-invasive structural data of
                 individual patients. Along the example of a patient with
                 bitemporal epilepsy, we show step by step how to develop a
                 Virtual Epileptic Patient (VEP) brain model and integrate
                 patient-specific information such as brain connectivity,
                 epileptogenic zone and MRI lesions. Using high-performance
                 computing, we systematically carry out parameter space
                 explorations, fit and validate the brain model against the
                 patient's empirical stereotactic EEG (SEEG) data and
                 demonstrate how to develop novel personalized strategies
                 towards therapy and intervention.",
  journal     = "Neuroimage",
  volume      =  145,
  number      = "Pt B",
  pages       = "377--388",
  month       =  jan,
  year        =  2017,
  url         = "http://dx.doi.org/10.1016/j.neuroimage.2016.04.049",
  keywords    = "Bayes Readings",
  language    = "en",
  issn        = "1053-8119, 1095-9572",
  pmid        = "27477535",
  doi         = "10.1016/j.neuroimage.2016.04.049",
  original_id = "05601ace-1ded-0619-98f8-4ae8ddaef94e"
}

@ARTICLE{Nay2016-zb,
  title       = "Betting and Belief: Prediction Markets and Attribution of
                 Climate Change",
  author      = "Nay, John J and Van der Linden, Martin and Gilligan, Jonathan
                 M",
  abstract    = "Despite much scientific evidence, a large fraction of the
                 American public doubts that greenhouse gases are causing
                 global warming. We present a simulation model as a
                 computational test-bed for climate prediction markets. Traders
                 adapt their beliefs about future temperatures based on the
                 profits of other traders in their social network. We simulate
                 two alternative climate futures, in which global temperatures
                 are primarily driven either by carbon dioxide or by solar
                 irradiance. These represent, respectively, the scientific
                 consensus and a hypothesis advanced by prominent skeptics. We
                 conduct sensitivity analyses to determine how a variety of
                 factors describing both the market and the physical climate
                 may affect traders' beliefs about the cause of global climate
                 change. Market participation causes most traders to converge
                 quickly toward believing the ``true'' climate model,
                 suggesting that a climate market could be useful for building
                 public consensus.",
  journal     = "arXiv:1603.08961 [physics, q-fin]",
  month       =  mar,
  year        =  2016,
  url         = "http://arxiv.org/abs/1603.08961",
  keywords    = "Bayes Readings",
  original_id = "0ec95d8e-3cfb-0926-940b-ca808255f2d7"
}

@ARTICLE{Fawcett2016-ne,
  title       = "The representational consequences of intentional forgetting:
                 Impairments to both the probability and fidelity of long-term
                 memory",
  author      = "Fawcett, Jonathan M and Lawrence, Michael A and Taylor, Tracy
                 L",
  abstract    = "We investigated whether intentional forgetting impacts only
                 the likelihood of later retrieval from long-term memory or
                 whether it also impacts the fidelity of those representations
                 that are successfully retrieved. We accomplished this by
                 combining an item-method directed forgetting task with a
                 testing procedure and modeling approach inspired by the
                 delayed-estimation paradigm used in the study of visual
                 short-term memory (STM). Abstract or concrete colored images
                 were each followed by a remember (R) or forget (F) instruction
                 and sometimes by a visual probe requiring a speeded detection
                 response (E1âE3). Memory was tested using an oldânew (E1âE2)
                 or remember-know-no (E3) recognition task followed by a
                 continuous color judgment task (E2âE3); a final experiment
                 included only the color judgment task (E4). Replicating the
                 existing literature, more ``old'' or ``remember'' responses
                 were made to R than F items and RTs to postinstruction visual
                 probes were longer following F than R instructions. Color
                 judgments were more accurate for successfully recognized or
                 recollected R than F items (E2âE3); a mixture model confirmed
                 a decrease to both the probability of retrieving the F items
                 as well as the fidelity of the representation of those F items
                 that were retrieved (E4). We conclude that intentional
                 forgetting is an effortful process that not only reduces the
                 likelihood of successfully encoding an item for later
                 retrieval, but also produces an impoverished memory trace even
                 when those items are retrieved; these findings draw a parallel
                 between the control of memory representations within working
                 and long-term memory.",
  journal     = "J. Exp. Psychol. Gen.",
  volume      =  145,
  number      =  1,
  pages       = "56--81",
  year        =  2016,
  url         = "http://dx.doi.org/10.1037/xge0000128",
  keywords    = "Bayes Readings",
  language    = "English",
  issn        = "0096-3445",
  doi         = "10.1037/xge0000128",
  original_id = "b09e7095-b91c-0fc7-a442-adfee620e2c6"
}

@ARTICLE{Shantz2016-nd,
  title       = "Nutrient loading alters the performance of key nutrient
                 exchange mutualisms",
  author      = "Shantz, Andrew A and Lemoine, Nathan P and Burkepile, Deron E",
  abstract    = "Nutrient exchange mutualisms between phototrophs and
                 heterotrophs, such as plants and mycorrhizal fungi or
                 symbiotic algae and corals, underpin the functioning of many
                 ecosystems. These relationships structure communities, promote
                 biodiversity and help maintain food security. Nutrient loading
                 may destabilise these mutualisms by altering the costs and
                 benefits each partner incurs from interacting. Using
                 meta-analyses, we show a near ubiquitous decoupling in
                 mutualism performance across terrestrial and marine
                 environments in which phototrophs benefit from enrichment at
                 the expense of their heterotrophic partners. Importantly,
                 heterotroph identity, their dependence on phototroph-derived C
                 and the type of nutrient enrichment (e.g. nitrogen vs.
                 phosphorus) mediated the responses of different mutualisms to
                 enrichment. Nutrient-driven changes in mutualism performance
                 may alter community organisation and ecosystem processes and
                 increase costs of food production. Consequently, the
                 decoupling of nutrient exchange mutualisms via alterations of
                 the world's nitrogen and phosphorus cycles may represent an
                 emerging threat of global change.",
  journal     = "Ecol. Lett.",
  volume      =  19,
  number      =  1,
  pages       = "20--28",
  month       =  jan,
  year        =  2016,
  url         = "http://onlinelibrary.wiley.com/doi/10.1111/ele.12538/abstract",
  keywords    = "Bayes Readings",
  language    = "en",
  issn        = "1461-023X, 1461-0248",
  doi         = "10.1111/ele.12538",
  original_id = "ab0b3b97-b3ce-01aa-98ca-fc7a833d6e19"
}

@ARTICLE{Peltier2016-tx,
  title       = "Small cetacean bycatch as estimated from stranding schemes:
                 The common dolphin case in the northeast Atlantic",
  author      = "Peltier, HÃ©lÃ¨ne and Authier, Matthieu and Deaville, Rob and
                 Dabin, Willy and Jepson, Paul D and van Canneyt, Olivier and
                 Daniel, Pierre and Ridoux, Vincent",
  abstract    = "Death in fishing gear of non-target species (called `bycatch')
                 is a major concern for marine wildlife, and mostly worrying
                 for long-lived species like cetaceans, considering their
                 demographic characteristics (slow population growth rates and
                 low fecundity). In European waters, cetaceans are highly
                 impacted by this phenomenon. Under the Common Fishery Policy,
                 the EC 812/2004 regulation constitutes a legal frame for
                 bycatch monitoring on 5â10\% of fishing vessels >15 m. The aim
                 of this work was to compare parameters and bycatch estimates
                 of common dolphins (Delphinus delphis) provided by observer
                 programmes in France and UK national reports and those
                 inferred from stranding data, through two approaches. Bycatch
                 was estimated from stranding data, first by correcting
                 effectives from drift conditions (using a drift prediction
                 model) and then by estimating the probability of being
                 buoyant. Observer programmes on fishing vessels allowed us to
                 identify the specificity of the interaction between common
                 dolphins and fishing gear, and provided low estimates of
                 annual bycaught animals (around 550 animals yearâ1). However,
                 observer programmes are hindered by logistical and
                 administrative constraints, and the sampling scheme seems to
                 be poorly designed for the detection of marine mammal
                 bycatches. The analyses of strandings by considering drift
                 conditions highlighted areas with high levels of interactions
                 between common dolphins and fisheries. Since 1997, the highest
                 densities of bycaught dolphins at sea were located in the
                 southern part of the continental shelf and slope of the Bay of
                 Biscay. Bycatch numbers inferred from strandings suggested
                 very high levels, ranging from 3650 dolphins yearâ1
                 [2250â7000] to 4700 [3850â5750] dolphins yearâ1, depending on
                 methodological choices. The main advantage of stranding data
                 is its large spatial scale, cutting across administrative
                 boundaries. Diverging estimates between observer programmes
                 and stranding interpretation can set very different management
                 consequences: observer programmes suggest a sustainable
                 situation for common dolphins, whereas estimates based on
                 strandings highlight a very worrying and unsustainable
                 process.",
  journal     = "Environ. Sci. Policy",
  volume      =  63,
  pages       = "7--18",
  month       =  sep,
  year        =  2016,
  url         = "http://www.sciencedirect.com/science/article/pii/S1462901116301514",
  keywords    = "Bayes Readings",
  issn        = "1462-9011",
  doi         = "10.1016/j.envsci.2016.05.004",
  original_id = "1acf9155-43c2-0309-90c0-f8c18da5c651"
}

@ARTICLE{Beheim2014-zy,
  title       = "Strategic social learning and the population dynamics of human
                 behavior: the game of Go",
  author      = "Beheim, Bret Alexander and Thigpen, Calvin and McElreath,
                 Richard",
  abstract    = "Human culture is widely believed to undergo evolution, via
                 mechanisms rooted in the nature of human cognition. A number
                 of theories predict the kinds of human learning strategies, as
                 well as the population dynamics that result from their action.
                 There is little work, however, that quantitatively examines
                 the evidence for these strategies and resulting cultural
                 evolution within human populations. One of the obstacles is
                 the lack of individual-level data with which to link
                 transmission events to larger cultural dynamics. Here, we
                 address this problem with a rich quantitative database from
                 the East Asian board game known as Go. We draw from a large
                 archive of Go games spanning the last six decades of
                 professional play, and find evidence that the evolutionary
                 dynamics of particular cultural variants are driven by a mix
                 of individual and social learning processes. Particular
                 players vary dramatically in their sensitivity to population
                 knowledge, which also varies by age and nationality. The
                 dynamic patterns of opening Go moves are consistent with an
                 ancient, ongoing arms race within the game itself.",
  journal     = "Evol. Hum. Behav.",
  volume      =  35,
  number      =  5,
  pages       = "351--357",
  month       =  sep,
  year        =  2014,
  url         = "http://www.sciencedirect.com/science/article/pii/S109051381400049X",
  keywords    = "Bayes Readings",
  issn        = "1090-5138",
  doi         = "10.1016/j.evolhumbehav.2014.04.001",
  original_id = "e678a203-5d13-0e0d-beef-2580431be7f6"
}

@ARTICLE{Bigorgne2015-pi,
  title       = "Chromosomal damage and {EROD} induction in tree swallows
                 (Tachycineta bicolor) along the Upper Mississippi River,
                 Minnesota, {USA}",
  author      = "Bigorgne, Emilie and Custer, Thomas W and Dummer, Paul M and
                 Erickson, Richard A and Karouna-Renier, Natalie and Schultz,
                 Sandra and Custer, Christine M and Thogmartin, Wayne E and
                 Matson, Cole W",
  abstract    = "The health of tree swallows, Tachycineta bicolor, on the Upper
                 Mississippi River (UMR) was assessed in 2010 and 2011 using
                 biomarkers at six sites downriver of Minneapolis/St. Paul, MN
                 metropolitan area, a tributary into the UMR, and a nearby
                 lake. Chromosomal damage was evaluated in nestling blood by
                 measuring the coefficient of variation of DNA content (DNA CV)
                 using flow cytometry. Cytochrome P450 1A activity in nestling
                 liver was measured using the ethoxyresorufin-O-dealkylase
                 (EROD) assay, and oxidative stress was estimated in nestling
                 livers via determination of thiobarbituric acid reacting
                 substances (TBARS), reduced glutathione (GSH), oxidized
                 glutathione (GSSG), the ratio GSSG/GSH, total sulfhydryl, and
                 protein bound sulfhydryl (PBSH). A multilevel regression model
                 (DNA CV) and simple regressions (EROD and oxidative stress)
                 were used to evaluate biomarker responses for each location.
                 Chromosomal damage was significantly elevated at two sites on
                 the UMR (Pigs Eye and Pool 2) relative to the Green Mountain
                 Lake reference site, while the induction of EROD activity was
                 only observed at Pigs Eye. No measures of oxidative stress
                 differed among sites. Multivariate analysis confirmed an
                 increased DNA CV at Pigs Eye and Pool 2, and elevated EROD
                 activity at Pigs Eye. These results suggest that the health of
                 tree swallows has been altered at the DNA level at Pigs Eye
                 and Pool 2 sites, and at the physiological level at Pigs Eye
                 site only.",
  journal     = "Ecotoxicology",
  volume      =  24,
  number      =  5,
  pages       = "1028--1039",
  month       =  mar,
  year        =  2015,
  url         = "http://link.springer.com/article/10.1007/s10646-015-1443-7",
  keywords    = "Bayes Readings",
  language    = "en",
  issn        = "0963-9292, 1573-3017",
  doi         = "10.1007/s10646-015-1443-7",
  original_id = "130cb450-8d1f-02eb-b86a-e42dd894efee"
}

@ARTICLE{Smits2015-sh,
  title       = "Expected time-invariant effects of biological traits on mammal
                 species duration",
  author      = "Smits, Peter D",
  abstract    = "Determining which biological traits influence differences in
                 extinction risk is vital for understanding the differential
                 diversification of life and for making predictions about
                 species' vulnerability to anthropogenic impacts. Here I
                 present a hierarchical Bayesian survival model of North
                 American Cenozoic mammal species durations in relation to
                 species-level ecological factors, time of origination, and
                 phylogenetic relationships. I find support for the survival of
                 the unspecialized as a time-invariant generalization of
                 trait-based extinction risk. Furthermore, I find that
                 phylogenetic and temporal effects are both substantial factors
                 associated with differences in species durations. Finally, I
                 find that the estimated effects of these factors are partially
                 incongruous with how these factors are correlated with
                 extinction risk of the extant species. These findings parallel
                 previous observations that background extinction is a poor
                 predictor of mass extinction events and suggest that attention
                 should be focused on mass extinctions to gain insight into
                 modern species loss.",
  journal     = "Proc. Natl. Acad. Sci. U. S. A.",
  volume      =  112,
  number      =  42,
  pages       = "13015--13020",
  month       =  oct,
  year        =  2015,
  url         = "http://www.pnas.org/content/112/42/13015",
  keywords    = "Bayes Readings",
  language    = "en",
  issn        = "0027-8424, 1091-6490",
  pmid        = "26438873",
  doi         = "10.1073/pnas.1510482112",
  original_id = "d8f02e5d-9c7f-0653-a5b8-7d41704f4a4b"
}

@ARTICLE{Lemoine2015-li,
  title       = "Phylogenetic relatedness and leaf functional traits, not
                 introduced status, influence community assembly",
  author      = "Lemoine, Nathan P and Shue, Jessica and Verrico, Brittany and
                 Erickson, David and Kress, W John and Parker, John D",
  abstract    = "Considerable debate focuses on whether invasive species
                 establish and become abundant by being functionally and
                 phylogenetically distinct from native species, leading to a
                 host of invasion-specific hypotheses of community assembly.
                 Few studies, however, have quantitatively assessed whether
                 similar patterns of phylogenetic and functional similarity
                 explain local abundance of both native and introduced species,
                 which would suggest similar assembly mechanisms regardless of
                 origin. Using a chronosequence of invaded temperate forest
                 stands, we tested whether the occurrence and abundance of both
                 introduced and native species were predicted by phylogenetic
                 relatedness, functional overlap, and key environmental
                 characteristics including forest age. Environmental filtering
                 against functionally and phylogenetically distinct species
                 strongly dictated the occurrence and abundance of both
                 introduced and native species, with slight modifications of
                 these patterns according to forest age. Thus, once functional
                 and evolutionary novelty were quantified, introduced status
                 provided little information about species' presence or
                 abundance, indicating largely similar sorting mechanisms for
                 both native and introduced species.",
  journal     = "Ecology",
  volume      =  96,
  number      =  10,
  pages       = "2605--2612",
  month       =  oct,
  year        =  2015,
  url         = "http://onlinelibrary.wiley.com/doi/10.1890/14-1883.1/abstract",
  keywords    = "Bayes Readings",
  language    = "en",
  issn        = "0012-9658, 1939-9170",
  doi         = "10.1890/14-1883.1",
  original_id = "42741d33-cac0-0a1d-a388-a69fd7e2c072"
}

@ARTICLE{Lemoine2014-wo,
  title       = "Variable effects of temperature on insect herbivory",
  author      = "Lemoine, Nathan P and Burkepile, Deron E and Parker, John D",
  journal     = "PeerJ",
  volume      =  2,
  pages       = "e376",
  month       =  may,
  year        =  2014,
  url         = "https://peerj.com/articles/376",
  keywords    = "Bayes Readings",
  language    = "en",
  issn        = "2167-8359",
  doi         = "10.7717/peerj.376",
  original_id = "09296eb3-93c2-02da-b630-f50c7038edf0"
}

@ARTICLE{Hartig2011-zx,
  title       = "Statistical inference for stochastic simulation modelsâtheory
                 and application",
  author      = "Hartig, Florian and Calabrese, Justin M and Reineking, BjÃ¶rn
                 and Wiegand, Thorsten and Huth, Andreas",
  abstract    = "Statistical models are the traditional choice to test
                 scientific theories when observations, processes or boundary
                 conditions are subject to stochasticity. Many important
                 systems in ecology and biology, however, are difficult to
                 capture with statistical models. Stochastic simulation models
                 offer an alternative, but they were hitherto associated with a
                 major disadvantage: their likelihood functions can usually not
                 be calculated explicitly, and thus it is difficult to couple
                 them to well-established statistical theory such as maximum
                 likelihood and Bayesian statistics. A number of new methods,
                 among them Approximate Bayesian Computing and Pattern-Oriented
                 Modelling, bypass this limitation. These methods share three
                 main principles: aggregation of simulated and observed data
                 via summary statistics, likelihood approximation based on the
                 summary statistics, and efficient sampling. We discuss
                 principles as well as advantages and caveats of these methods,
                 and demonstrate their potential for integrating stochastic
                 simulation models into a unified framework for statistical
                 modelling.",
  journal     = "Ecol. Lett.",
  volume      =  14,
  number      =  8,
  pages       = "816--827",
  month       =  aug,
  year        =  2011,
  url         = "http://dx.doi.org/10.1111/j.1461-0248.2011.01640.x",
  keywords    = "Bayes Readings",
  language    = "en",
  issn        = "1461-023X, 1461-0248",
  pmid        = "21679289",
  doi         = "10.1111/j.1461-0248.2011.01640.x",
  original_id = "be1929e4-02c7-0c79-ac39-35c56622cfea"
}

@BOOK{Gelman2013-hj,
  title       = "{Bayesian Data Analysis}",
  author      = "Gelman, Andrew and Carlin, John B and Stern, Hal S and Dunson,
                 David B and Vehtari, Aki and Rubin, Donald B",
  abstract    = "Now in its third edition, this classic book is widely
                 considered the leading text on Bayesian methods, lauded for
                 its accessible, practical approach to analyzing data and
                 solving research problems. Bayesian Data Analysis, Third
                 Edition continues to take an applied approach to analysis
                 using up-to-date Bayesian methods. The authorsâall leaders in
                 the statistics communityâintroduce basic concepts from a
                 data-analytic perspective before presenting advanced methods.
                 Throughout the text, numerous worked examples drawn from real
                 applications and research emphasize the use of Bayesian
                 inference in practice. New to the Third Edition Four new
                 chapters on nonparametric modeling Coverage of weakly
                 informative priors and boundary-avoiding priors Updated
                 discussion of cross-validation and predictive information
                 criteria Improved convergence monitoring and effective sample
                 size calculations for iterative simulation Presentations of
                 Hamiltonian Monte Carlo, variational Bayes, and expectation
                 propagation New and revised software code The book can be used
                 in three different ways. For undergraduate students, it
                 introduces Bayesian inference starting from first principles.
                 For graduate students, the text presents effective current
                 approaches to Bayesian modeling and computation in statistics
                 and related fields. For researchers, it provides an assortment
                 of Bayesian methods in applied statistics. Additional
                 materials, including data sets used in the examples, solutions
                 to selected exercises, and software instructions, are
                 available on the book's web page.",
  publisher   = "CRC Press",
  edition     = "3rd",
  month       =  nov,
  year        =  2013,
  url         = "https://market.android.com/details?id=book-ZXL6AQAAQBAJ",
  address     = "Boca Raton, FL",
  keywords    = "Alligator\_Morphology;CF Growth/Bayes general;Bayes Readings",
  language    = "en",
  isbn        = "9781439840955, 9781439840962",
  original_id = "a7d129c3-7eeb-0516-9935-38fff52b1fd2"
}

@ARTICLE{Gelman1992-af,
  title       = "{A single series from the Gibbs sampler provides a false sense
                 of security}",
  author      = "Gelman, Andrew and Rubin, Donald B",
  journal     = "Bayesian statistics",
  volume      =  4,
  pages       = "625--631",
  year        =  1992,
  url         = "http://nma.berkeley.edu/ark:/28722/bk000472578",
  keywords    = "Alligator\_Morphology;Bayes Readings",
  original_id = "85b0131a-04a8-0a57-b073-a4b14fa5c90e"
}

@ARTICLE{Brooks1998-gz,
  title       = "General methods for monitoring convergence of iterative
                 simulations",
  author      = "Brooks, Stephen P and Gelman, Andrew",
  abstract    = "Abstract We generalize the method proposed by Gelman and Rubin
                 (1992a) for monitoring the convergence of iterative
                 simulations by comparing between and within variances of
                 multiple chains, in order to obtain a family of tests for
                 convergence. We review methods of inference from simulations
                 in order to develop convergence-monitoring summaries that are
                 relevant for the purposes for which the simulations are used.
                 We recommend applying a battery of tests for mixing based on
                 the comparison of inferences from individual sequences and
                 from the mixture of sequences. Finally, we discuss
                 multivariate analogues, for assessing convergence of several
                 parameters simultaneously.",
  journal     = "J. Comput. Graph. Stat.",
  volume      =  7,
  number      =  4,
  pages       = "434--455",
  year        =  1998,
  url         = "http://www.tandfonline.com/doi/abs/10.1080/10618600.1998.10474787",
  keywords    = "Alligator\_Morphology;Bayes Readings",
  eprint      = "http://www.tandfonline.com/doi/pdf/10.1080/10618600.1998.10474787",
  issn        = "1061-8600",
  doi         = "10.1080/10618600.1998.10474787",
  original_id = "f8040fff-25b8-053b-82f5-aefab8a1796f"
}

@ARTICLE{Watanabe2010-hf,
  title       = "Asymptotic equivalence of {Bayes} cross validation and widely
                 applicable information criterion in singular learning theory",
  author      = "Watanabe, Sumio",
  journal     = "J. Mach. Learn. Res.",
  volume      =  11,
  number      = "Dec",
  pages       = "3571--3594",
  year        =  2010,
  url         = "http://www.jmlr.org/papers/volume11/watanabe10a/watanabe10a.pdf",
  keywords    = "Alligator\_Morphology;Bayes Readings",
  issn        = "1532-4435",
  original_id = "097cc221-8bb7-01c1-9bdd-2ef170cd0583"
}

@ARTICLE{Gelman2014-nd,
  title       = "Beyond power calculations assessing type {S} (sign) and type
                 {M} (magnitude) errors",
  author      = "Gelman, A and Carlin, J",
  abstract    = "Abstract Statistical power analysis provides the conventional
                 approach to assess error rates when designing a research
                 study. However, power analysis is flawed in that a narrow
                 emphasis on statistical significance is placed as the primary
                 focus of study design. In noisy, small-sample settings,
                 statistically significant results can often be misleading. To
                 help researchers address this problem in the context of their
                 own studies, we recommend ...",
  journal     = "Perspect. Psychol. Sci.",
  publisher   = "pps.sagepub.com",
  volume      =  9,
  number      =  6,
  pages       = "641â651",
  year        =  2014,
  url         = "http://pps.sagepub.com/content/9/6/641.abstract",
  keywords    = "Bayes Readings",
  issn        = "1745-6916",
  original_id = "71c5f0db-97fd-0f34-bae9-557d50dfd4e3"
}

@TECHREPORT{Kruschke2011-cn,
  title       = "Solutions Manual (Complete) For Doing Bayesian Data Analysis:
                 a Tutorial with {R} and {BUGS}",
  author      = "Kruschke, John K",
  pages       = "1--187",
  year        =  2011,
  url         = "http://doingbayesiandataanalysis.blogspot.com/search?updated-max=2012-06-25T17:27:00-04:00&max-results=7&start=7&by-date=false",
  keywords    = "Bayes Readings",
  original_id = "75953c91-74e3-0002-9a74-9c0de88f1207"
}

@ARTICLE{Arsnoe2015-hj,
  title       = "Different populations of blacklegged tick nymphs exhibit
                 differences in questing behavior that have implications for
                 human lyme disease risk",
  author      = "Arsnoe, Isis M and Hickling, Graham J and Ginsberg, Howard S
                 and McElreath, Richard and Tsao, Jean I",
  abstract    = "Animal behavior can have profound effects on pathogen
                 transmission and disease incidence. We studied the questing (=
                 host-seeking) behavior of blacklegged tick (Ixodes scapularis)
                 nymphs, which are the primary vectors of Lyme disease in the
                 eastern United States. Lyme disease is common in northern but
                 not in southern regions, and prior ecological studies have
                 found that standard methods used to collect host-seeking
                 nymphs in northern regions are unsuccessful in the south. This
                 led us to hypothesize that there are behavior differences
                 between northern and southern nymphs that alter how readily
                 they are collected, and how likely they are to transmit the
                 etiological agent of Lyme disease to humans. To examine this
                 question, we compared the questing behavior of I. scapularis
                 nymphs originating from one northern (Lyme disease endemic)
                 and two southern (non-endemic) US regions at field sites in
                 Wisconsin, Rhode Island, Tennessee, and Florida.
                 Laboratory-raised uninfected nymphs were monitored in circular
                 0.2 m2 arenas containing wooden dowels (mimicking stems of
                 understory vegetation) for 10 (2011) and 19 (2012) weeks. The
                 probability of observing nymphs questing on these stems
                 (2011), and on stems, on top of leaf litter, and on arena
                 walls (2012) was much greater for northern than for southern
                 origin ticks in both years and at all field sites (19.5 times
                 greater in 2011; 3.6-11.6 times greater in 2012). Our findings
                 suggest that southern origin I. scapularis nymphs rarely
                 emerge from the leaf litter, and consequently are unlikely to
                 contact passing humans. We propose that this difference in
                 questing behavior accounts for observed geographic differences
                 in the efficacy of the standard sampling techniques used to
                 collect questing nymphs. These findings also support our
                 hypothesis that very low Lyme disease incidence in southern
                 states is, in part, a consequence of the type of host-seeking
                 behavior exhibited by southern populations of the key Lyme
                 disease vector.",
  journal     = "PLoS One",
  volume      =  10,
  number      =  5,
  pages       = "e0127450",
  month       =  may,
  year        =  2015,
  url         = "http://dx.doi.org/10.1371/journal.pone.0127450",
  keywords    = "Bayes Readings",
  language    = "en",
  issn        = "1932-6203",
  pmid        = "25996603",
  doi         = "10.1371/journal.pone.0127450",
  pmc         = "PMC4440738",
  original_id = "ce38d735-684f-07c5-a5c5-d0ada896959a"
}

@ARTICLE{Chater2010-fi,
  title    = "Bayesian models of cognition",
  author   = "Chater, Nick and Oaksford, Mike and Hahn, Ulrike and Heit, Evan",
  abstract = "There has been a recent explosion in research applying Bayesian
              models to cognitive phenomena. This development has resulted from
              the realization that across a wide variety of tasks the
              fundamental problem the cognitive system confronts is coping with
              uncertainty. From visual scene recognition to on-line language
              comprehension, from categorizing stimuli to determining to what
              degree an argument is convincing, people must deal with the
              incompleteness of the information they possess to perform these
              tasks, many of which have important survival-related
              consequences. This paper provides a review of Bayesian models of
              cognition, dividing them up by the different aspects of cognition
              to which they have been applied. The paper begins with a brief
              review of Bayesian inference. This falls short of a full
              technical introduction but the reader is referred to the relevant
              literature for further details. There follows reviews of Bayesian
              models in Perception, Categorization, Learning and Causality,
              Language Processing, Inductive Reasoning, Deductive Reasoning,
              and Argumentation. In all these areas, it is argued that
              sophisticated Bayesian models are enhancing our understanding of
              the underlying cognitive computations involved. It is concluded
              that a major challenge is to extend the evidential basis for
              these models, especially to accounts of higher level cognition.
              WIREs Cogn Sci 2010 1 811-823 For further resources related to
              this article, please visit the WIREs website.",
  journal  = "Wiley Interdiscip. Rev. Cogn. Sci.",
  volume   =  1,
  number   =  6,
  pages    = "811--823",
  month    =  nov,
  year     =  2010,
  url      = "http://dx.doi.org/10.1002/wcs.79",
  keywords = "xx Archived/SMS;Bayes Readings",
  language = "en",
  issn     = "1939-5078",
  pmid     = "26271779",
  doi      = "10.1002/wcs.79"
}

@ARTICLE{Cumming2014-rp,
  title    = "The new statistics: why and how",
  author   = "Cumming, Geoff",
  abstract = "We need to make substantial changes to how we conduct research.
              First, in response to heightened concern that our published
              research literature is incomplete and untrustworthy, we need new
              requirements to ensure research integrity. These include
              prespecification of studies whenever possible, avoidance of
              selection and other inappropriate data-analytic practices,
              complete reporting, and encouragement of replication. Second, in
              response to renewed recognition of the severe flaws of
              null-hypothesis significance testing (NHST), we need to shift
              from reliance on NHST to estimation and other preferred
              techniques. The new statistics refers to recommended practices,
              including estimation based on effect sizes, confidence intervals,
              and meta-analysis. The techniques are not new, but adopting them
              widely would be new for many researchers, as well as highly
              beneficial. This article explains why the new statistics are
              important and offers guidance for their use. It describes an
              eight-step new-statistics strategy for research with integrity,
              which starts with formulation of research questions in estimation
              terms, has no place for NHST, and is aimed at building a
              cumulative quantitative discipline.",
  journal  = "Psychol. Sci.",
  volume   =  25,
  number   =  1,
  pages    = "7--29",
  month    =  jan,
  year     =  2014,
  url      = "http://dx.doi.org/10.1177/0956797613504966",
  keywords = "estimation; meta-analysis; replication; research integrity;
              research methods; statistical analysis; the new statistics;xx
              Archived/SMS;Bayes Readings",
  language = "en",
  issn     = "0956-7976, 1467-9280",
  pmid     = "24220629",
  doi      = "10.1177/0956797613504966"
}

@ARTICLE{Bayes1763-tm,
  title    = "{LII}. An essay towards solving a problem in the doctrine of
              chances. By the late Rev. Mr. Bayes, F. R. S. communicated by Mr.
              Price, in a letter to John Canton, A. M. F. R. {S}",
  author   = "Bayes, Thomas",
  journal  = "Philosophical Transactions",
  volume   =  53,
  pages    = "370--418",
  month    =  jan,
  year     =  1763,
  url      = "http://rstl.royalsocietypublishing.org/content/53/370.short",
  keywords = "xx Archived/SMS;Bayes Readings",
  doi      = "10.1098/rstl.1763.0053"
}

@BOOK{De_Moivre1756-wa,
  title     = "The doctrine of chances: or, A method of calculating the
               probabilities of events in play",
  author    = "de Moivre, Abraham",
  publisher = "A. Millar",
  year      =  1756,
  address   = "London",
  keywords  = "xx Archived/SMS;Bayes Readings"
}

@ARTICLE{Yao2018-zi,
  title     = "Using Stacking to Average Bayesian Predictive Distributions",
  author    = "Yao, Yuling and Vehtari, Aki and Simpson, Daniel and Gelman,
               Andrew",
  abstract  = "Bayesian model averaging is flawed in the M-open setting in
               which the true data-generating process is not one of the
               candidate models being fit. We take the idea of stacking from
               the point estimation literature and generalize to the
               combination of predictive distributions. We extend the utility
               function to any proper scoring rule and use Pareto smoothed
               importance sampling to efficiently compute the required
               leave-one-out posterior distributions. We compare stacking of
               predictive distributions to several alternatives: stacking of
               means, Bayesian model averaging (BMA), Pseudo-BMA, and a variant
               of Pseudo-BMA that is stabilized using the Bayesian bootstrap.
               Based on simulations and real-data applications, we recommend
               stacking of predictive distributions, with
               bootstrapped-Pseudo-BMA as an approximate alternative when
               computation cost is an issue.",
  journal   = "Bayesian Analysis",
  publisher = "International Society for Bayesian Analysis",
  volume    =  13,
  number    =  3,
  pages     = "917--1007",
  year      =  2018,
  url       = "http://dx.doi.org/10.1214/17-BA1091",
  keywords  = "Bayesian model averaging; model combination; proper scoring
               rule; predictive distribution; stacking; Stan; ;xx Archived/AJPA
               Reply;CF Growth/Bayes general;Bayes Readings",
  language  = "en",
  issn      = "1936-0975, 1931-6690",
  doi       = "10.1214/17-BA1091"
}

@ARTICLE{Kruschke2018-mg,
  title     = "Rejecting or Accepting Parameter Values in Bayesian Estimation",
  author    = "Kruschke, John K",
  abstract  = "This article explains a decision rule that uses Bayesian
               posterior distributions as the basis for accepting or rejecting
               null values of parameters. This decision rule focuses on the
               range of plausible values indicated by the highest density
               interval of the posterior distribution and the relation between
               this range and a region of practical equivalence (ROPE) around
               the null value. The article also discusses considerations for
               setting the limits of a ROPE and emphasizes that analogous
               considerations apply to setting the decision thresholds for p
               values and Bayes factors.",
  journal   = "Advances in Methods and Practices in Psychological Science",
  publisher = "SAGE Publications Inc",
  volume    =  1,
  number    =  2,
  pages     = "270--280",
  month     =  may,
  year      =  2018,
  url       = "https://doi.org/10.1177/2515245918771304",
  keywords  = "Bayes Readings;Quantitative Methods",
  issn      = "2515-2459",
  doi       = "10.1177/2515245918771304"
}

@ARTICLE{Turner2012-rw,
  title    = "A tutorial on approximate Bayesian computation",
  author   = "Turner, Brandon M and Van Zandt, Trisha",
  abstract = "This tutorial explains the foundation of approximate Bayesian
              computation (ABC), an approach to Bayesian inference that does
              not require the specification of a likelihood function, and hence
              that can be used to estimate posterior distributions of
              parameters for simulation-based models. We discuss briefly the
              philosophy of Bayesian inference and then present several
              algorithms for ABC. We then apply these algorithms in a number of
              examples. For most of these examples, the posterior distributions
              are known, and so we can compare the estimated posteriors derived
              from ABC to the true posteriors and verify that the algorithms
              recover the true posteriors accurately. We also consider a
              popular simulation-based model of recognition memory (REM) for
              which the true posteriors are unknown. We conclude with a number
              of recommendations for applying ABC methods to solve real-world
              problems.",
  journal  = "J. Math. Psychol.",
  volume   =  56,
  number   =  2,
  pages    = "69--85",
  month    =  apr,
  year     =  2012,
  url      = "http://www.sciencedirect.com/science/article/pii/S0022249612000272",
  keywords = "Approximate Bayesian computation; Tutorial; Bayesian estimation;
              Population Monte Carlo;ABC;Bayes Readings",
  issn     = "0022-2496",
  doi      = "10.1016/j.jmp.2012.02.005"
}

@ARTICLE{Sisson2018-mv,
  title         = "Overview of Approximate Bayesian Computation",
  author        = "Sisson, S A and Fan, Y and Beaumont, M A",
  abstract      = "This Chapter, ``Overview of Approximate Bayesian
                   Computation'', is to appear as the first chapter in the
                   forthcoming Handbook of Approximate Bayesian Computation
                   (2018). It details the main ideas and concepts behind ABC
                   methods with many examples and illustrations.",
  month         =  feb,
  year          =  2018,
  url           = "http://arxiv.org/abs/1802.09720",
  keywords      = "ABC;Bayes Readings",
  archivePrefix = "arXiv",
  eprint        = "1802.09720",
  primaryClass  = "stat.CO",
  arxivid       = "1802.09720"
}

@ARTICLE{Marin2011-cf,
  title         = "Approximate Bayesian Computational methods",
  author        = "Marin, Jean-Michel and Pudlo, Pierre and Robert, Christian P
                   and Ryder, Robin",
  abstract      = "Also known as likelihood-free methods, approximate Bayesian
                   computational (ABC) methods have appeared in the past ten
                   years as the most satisfactory approach to untractable
                   likelihood problems, first in genetics then in a broader
                   spectrum of applications. However, these methods suffer to
                   some degree from calibration difficulties that make them
                   rather volatile in their implementation and thus render them
                   suspicious to the users of more traditional Monte Carlo
                   methods. In this survey, we study the various improvements
                   and extensions made to the original ABC algorithm over the
                   recent years.",
  month         =  jan,
  year          =  2011,
  url           = "http://arxiv.org/abs/1101.0955",
  keywords      = "ABC;Bayes Readings",
  archivePrefix = "arXiv",
  eprint        = "1101.0955",
  primaryClass  = "stat.CO",
  arxivid       = "1101.0955"
}

@BOOK{Stan_Development_Team2018-kt,
  title    = "Bayesian Statistics Using Stan",
  author   = "{Stan Development Team}",
  year     =  2018,
  keywords = "Bayes Readings"
}

@MISC{Zapico_undated-fi,
  title    = "Applied Gaussian Processes in Stan",
  author   = "Zapico, Andre",
  keywords = "Bayes Readings"
}

@ARTICLE{Pericchi1981-wm,
  title       = "A Bayesian approach to transformation to normality",
  author      = "Pericchi, L R",
  journal     = "Biometrika",
  volume      =  68,
  number      =  1,
  pages       = "35--43",
  year        =  1981,
  keywords    = "Bayes Readings;Quantitative Methods",
  issn        = "0006-3444",
  original_id = "c228b15d-a31b-037f-a8e4-2b7514d90e33"
}

@BOOK{Gelman2020-pw,
  title     = "Regression and Other Stories",
  author    = "Gelman, Andrew and Hill, Jennifer and Vehtari, Aki",
  publisher = "Cambridge University Press",
  year      =  2020,
  keywords  = "Bayes Readings;Quantitative Methods"
}

@ARTICLE{Vehtari2019-oj,
  title         = "Rank-normalization, folding, and localization: An improved
                   {$\widehat{R}$} for assessing convergence of {MCMC}",
  author        = "Vehtari, Aki and Gelman, Andrew and Simpson, Daniel and
                   Carpenter, Bob and BÃ¼rkner, Paul-Christian",
  abstract      = "Markov chain Monte Carlo is a key computational tool in
                   Bayesian statistics, but it can be challenging to monitor
                   the convergence of an iterative stochastic algorithm. In
                   this paper we show that the convergence diagnostic
                   $\widehat\{R\}$ of Gelman and Rubin (1992) has serious
                   flaws. Traditional $\widehat\{R\}$ will fail to correctly
                   diagnose convergence failures when the chain has a heavy
                   tail or when the variance varies across the chains. In this
                   paper we propose an alternative rank-based diagnostic that
                   fixes these problems. We also introduce a collection of
                   quantile-based local efficiency measures, along with a
                   practical approach for computing Monte Carlo error estimates
                   for quantiles. We suggest that common trace plots should be
                   replaced with rank plots from multiple chains. Finally, we
                   give recommendations for how these methods should be used in
                   practice.",
  month         =  mar,
  year          =  2019,
  url           = "http://arxiv.org/abs/1903.08008",
  keywords      = "Bayes Readings",
  archivePrefix = "arXiv",
  eprint        = "1903.08008",
  primaryClass  = "stat.CO",
  arxivid       = "1903.08008"
}

@BOOK{Downey2013-sq,
  title       = "Think Bayes",
  author      = "Downey, Allen B",
  publisher   = "O'Reilly Media, Inc.",
  year        =  2013,
  address     = "Sebastopol, CA",
  keywords    = "Bayes Readings",
  original_id = "403dffb4-ef40-0c3c-ac63-774a56886105"
}

@INPROCEEDINGS{Van_der_Zander_undated-no,
  title     = "Constructing separators and adjustment sets in ancestral graphs",
  booktitle = "Proceedings of the Thirtieth Conference on Uncertainty in
               Artificial Intelligence",
  author    = "van der Zander, Benito and LiÅkiewicz, Maciej and Textor,
               Johannes",
  url       = "https://dl.acm.org/doi/10.5555/3020751.3020845",
  keywords  = "Bayes Readings",
  language  = "en"
}

@BOOK{McElreath2020-tk,
  title     = "{Statistical Rethinking: A Bayesian Course with Examples in {R}
               and Stan}",
  author    = "McElreath, Richard",
  publisher = "CRC Press",
  edition   = "2nd",
  year      =  2020,
  keywords  = "Alligator\_Morphology;Bayes Teaching;Bayes Readings;Quantitative
               Methods"
}

@ARTICLE{Gelman2020-fv,
  title         = "Bayesian Workflow",
  author        = "Gelman, Andrew and Vehtari, Aki and Simpson, Daniel and
                   Margossian, Charles C and Carpenter, Bob and Yao, Yuling and
                   Kennedy, Lauren and Gabry, Jonah and BÃ¼rkner, Paul-Christian
                   and ModrÃ¡k, Martin",
  abstract      = "The Bayesian approach to data analysis provides a powerful
                   way to handle uncertainty in all observations, model
                   parameters, and model structure using probability theory.
                   Probabilistic programming languages make it easier to
                   specify and fit Bayesian models, but this still leaves us
                   with many options regarding constructing, evaluating, and
                   using these models, along with many remaining challenges in
                   computation. Using Bayesian inference to solve real-world
                   problems requires not only statistical skills, subject
                   matter knowledge, and programming, but also awareness of the
                   decisions made in the process of data analysis. All of these
                   aspects can be understood as part of a tangled workflow of
                   applied Bayesian statistics. Beyond inference, the workflow
                   also includes iterative model building, model checking,
                   validation and troubleshooting of computational problems,
                   model understanding, and model comparison. We review all
                   these aspects of workflow in the context of several
                   examples, keeping in mind that in practice we will be
                   fitting many models for any given problem, even if only a
                   subset of them will ultimately be relevant for our
                   conclusions.",
  month         =  nov,
  year          =  2020,
  url           = "http://arxiv.org/abs/2011.01808",
  keywords      = "Bayes Readings",
  copyright     = "http://creativecommons.org/licenses/by/4.0/",
  archivePrefix = "arXiv",
  eprint        = "2011.01808",
  primaryClass  = "stat.ME",
  arxivid       = "2011.01808"
}

@ARTICLE{Burkner2021-mp,
  title    = "Efficient leave-one-out cross-validation for Bayesian
              non-factorized normal and Student-t models",
  author   = "BÃ¼rkner, Paul-Christian and Gabry, Jonah and Vehtari, Aki",
  abstract = "Cross-validation can be used to measure a model's predictive
              accuracy for the purpose of model comparison, averaging, or
              selection. Standard leave-one-out cross-validation (LOO-CV)
              requires that the observation model can be factorized into simple
              terms, but a lot of important models in temporal and spatial
              statistics do not have this property or are inefficient or
              unstable when forced into a factorized form. We derive how to
              efficiently compute and validate both exact and approximate
              LOO-CV for any Bayesian non-factorized model with a multivariate
              normal or Student-$$t$$distribution on the outcome values. We
              demonstrate the method using lagged simultaneously autoregressive
              (SAR) models as a case study.",
  journal  = "Comput. Stat.",
  volume   =  36,
  number   =  2,
  pages    = "1243--1261",
  month    =  jun,
  year     =  2021,
  url      = "https://doi.org/10.1007/s00180-020-01045-4",
  keywords = "Bayes Readings",
  issn     = "0943-4062",
  doi      = "10.1007/s00180-020-01045-4"
}

@ARTICLE{Gabry2019-tw,
  title     = "Visualization in Bayesian workflow",
  author    = "Gabry, Jonah and Simpson, Daniel and Vehtari, Aki and
               Betancourt, Michael and Gelman, Andrew",
  abstract  = "Bayesian data analysis is about more than just computing a
               posterior distribution, and Bayesian visualization is about more
               than trace plots of Markov chains. Practical Bayesian data
               analysis, like all data analysis, is an iterative process of
               model building, inference, model checking and evaluation, and
               model expansion. Visualization is helpful in each of these
               stages of the Bayesian workflow and it is indispensable when
               drawing inferences from the types of modern, high dimensional
               models that are used by applied researchers.",
  journal   = "J. R. Stat. Soc. Ser. A Stat. Soc.",
  publisher = "Wiley",
  volume    =  182,
  number    =  2,
  pages     = "389--402",
  month     =  feb,
  year      =  2019,
  url       = "https://onlinelibrary.wiley.com/doi/10.1111/rssa.12378",
  keywords  = "Bayes Readings",
  copyright = "http://onlinelibrary.wiley.com/termsAndConditions\#vor",
  language  = "en",
  issn      = "0964-1998, 1467-985X",
  doi       = "10.1111/rssa.12378"
}

@ARTICLE{El-Gamal1995-vy,
  title     = "Are People Bayesian? Uncovering Behavioral Strategies",
  author    = "El-Gamal, Mahmoud A and Grether, David M",
  abstract  = "[Economists and psychologists have recently been developing new
               theories of decision making under uncertainty that can
               accommodate the observed violations of standard statistical
               decision theoretic axioms by experimental subjects. We propose a
               procedure that finds a collection of decision rules that best
               explain the behavior of experimental subjects. The procedure is
               a combination of maximum likelihood estimation of the rules
               together with an implicit classification of subjects to the
               various rules and a penalty for having too many rules. We apply
               our procedure to data on probabilistic updating by subjects in
               four different universities. We get remarkably robust results
               showing that the most important rules used by the subjects (in
               order of importance) are Bayes's rule, a representativeness rule
               (ignoring the prior), and, to a lesser extent, conservatism
               (overweighting the prior).]",
  journal   = "J. Am. Stat. Assoc.",
  publisher = "[American Statistical Association, Taylor \& Francis, Ltd.]",
  volume    =  90,
  number    =  432,
  pages     = "1137--1145",
  year      =  1995,
  url       = "http://www.jstor.org/stable/2291506",
  keywords  = "Bayes Readings",
  issn      = "0162-1459",
  doi       = "10.2307/2291506"
}

@ARTICLE{Vehtari2021-yi,
  title     = "{Rank-Normalization}, Folding, and Localization: An Improved
               {$\hat{R}$} for Assessing Convergence of {MCMC}",
  author    = "Vehtari, Aki and Gelman, Andrew and Simpson, Daniel and
               Carpenter, Bob and BÃ¼rkner, Paul-Christian",
  abstract  = "Markov chain Monte Carlo is a key computational tool in Bayesian
               statistics, but it can be challenging to monitor the convergence
               of an iterative stochastic algorithm. In this paper we show that
               the convergence diagnostic RË of Gelman and Rubin (1992) has
               serious flaws. Traditional RË will fail to correctly diagnose
               convergence failures when the chain has a heavy tail or when the
               variance varies across the chains. In this paper we propose an
               alternative rank-based diagnostic that fixes these problems. We
               also introduce a collection of quantile-based local efficiency
               measures, along with a practical approach for computing Monte
               Carlo error estimates for quantiles. We suggest that common
               trace plots should be replaced with rank plots from multiple
               chains. Finally, we give recommendations for how these methods
               should be used in practice.",
  journal   = "Bayesian Analysis",
  publisher = "International Society for Bayesian Analysis",
  volume    = "-1",
  number    = "-1",
  pages     = "1--38",
  month     =  jan,
  year      =  2021,
  url       = "http://dx.doi.org/10.1214/20-BA1221",
  keywords  = "Alligator\_Morphology;Bayes Readings",
  language  = "en",
  issn      = "1936-0975, 1931-6690",
  doi       = "10.1214/20-BA1221"
}

@ARTICLE{Metropolis1953-lm,
  title       = "Equation of State Calculations by Fast Computing Machines",
  author      = "Metropolis, Nicholas and Rosenbluth, Arianna W and Rosenbluth,
                 Marshall N and Teller, Augusta H and Teller, Edward",
  abstract    = "A general method, suitable for fast computing machines, for
                 investigating such properties as equations of state for
                 substances consisting of interacting individual molecules is
                 described. The method consists of a modified Monte Carlo
                 integration over configuration space. Results for the
                 two?dimensional rigid?sphere system have been obtained on the
                 Los Alamos MANIAC and are presented here. These results are
                 compared to the free volume equation of state and to a
                 four?term virial coefficient expansion.",
  journal     = "J. Chem. Phys.",
  publisher   = "American Institute of Physics",
  volume      =  21,
  number      =  6,
  pages       = "1087--1092",
  month       =  jun,
  year        =  1953,
  url         = "https://doi.org/10.1063/1.1699114",
  keywords    = "Bayes Readings",
  issn        = "0021-9606",
  doi         = "10.1063/1.1699114",
  original_id = "d302fa8f-94c1-0dde-9189-c91923529ca0"
}

@BOOK{Johnson2022-db,
  title     = "Bayes Rules! An Introduction to Applied Bayesian Modeling",
  author    = "Johnson, Alicia A and Ott, Miles Q and Dogucu, Mine",
  publisher = "Chapman \& Hall",
  year      =  2022,
  keywords  = "Bayes Readings"
}

@ARTICLE{Piironen2017-sn,
  title     = "Sparsity information and regularization in the horseshoe and
               other shrinkage priors",
  author    = "Piironen, Juho and Vehtari, Aki",
  abstract  = "The horseshoe prior has proven to be a noteworthy alternative
               for sparse Bayesian estimation, but has previously suffered from
               two problems. First, there has been no systematic way of
               specifying a prior for the global shrinkage hyperparameter based
               on the prior information about the degree of sparsity in the
               parameter vector. Second, the horseshoe prior has the undesired
               property that there is no possibility of specifying separately
               information about sparsity and the amount of regularization for
               the largest coefficients, which can be problematic with weakly
               identified parameters, such as the logistic regression
               coefficients in the case of data separation. This paper proposes
               solutions to both of these problems. We introduce a concept of
               effective number of nonzero parameters, show an intuitive way of
               formulating the prior for the global hyperparameter based on the
               sparsity assumptions, and argue that the previous default
               choices are dubious based on their tendency to favor solutions
               with more unshrunk parameters than we typically expect a priori.
               Moreover, we introduce a generalization to the horseshoe prior,
               called the regularized horseshoe, that allows us to specify a
               minimum level of regularization to the largest values. We show
               that the new prior can be considered as the continuous
               counterpart of the spike-and-slab prior with a finite slab
               width, whereas the original horseshoe resembles the
               spike-and-slab with an infinitely wide slab. Numerical
               experiments on synthetic and real world data illustrate the
               benefit of both of these theoretical advances.",
  journal   = "Electronic Journal of Statistics",
  publisher = "Institute of Mathematical Statistics and Bernoulli Society",
  volume    =  11,
  number    =  2,
  pages     = "5018--5051",
  month     =  jan,
  year      =  2017,
  url       = "https://projecteuclid.org/journals/electronic-journal-of-statistics/volume-11/issue-2/Sparsity-information-and-regularization-in-the-horseshoe-and-other-shrinkage/10.1214/17-EJS1337SI.full",
  keywords  = "62F15; Bayesian inference; horseshoe prior; shrinkage priors;
               Sparse estimation; ;Bayes Readings",
  issn      = "1746-1391",
  doi       = "10.1214/17-EJS1337SI"
}

@ARTICLE{Piironen2020-xz,
  title     = "Projective inference in high-dimensional problems: Prediction
               and feature selection",
  author    = "Piironen, Juho and Paasiniemi, Markus and Vehtari, Aki",
  abstract  = "This paper reviews predictive inference and feature selection
               for generalized linear models with scarce but high-dimensional
               data. We demonstrate that in many cases one can benefit from a
               decision theoretically justified two-stage approach: first,
               construct a possibly non-sparse model that predicts well, and
               then find a minimal subset of features that characterize the
               predictions. The model built in the first step is referred to as
               the reference model and the operation during the latter step as
               predictive projection. The key characteristic of this approach
               is that it finds an excellent tradeoff between sparsity and
               predictive accuracy, and the gain comes from utilizing all
               available information including prior and that coming from the
               left out features. We review several methods that follow this
               principle and provide novel methodological contributions. We
               present a new projection technique that unifies two existing
               techniques and is both accurate and fast to compute. We also
               propose a way of evaluating the feature selection process using
               fast leave-one-out cross-validation that allows for easy and
               intuitive model size selection. Furthermore, we prove a theorem
               that helps to understand the conditions under which the
               projective approach could be beneficial. The key ideas are
               illustrated via several experiments using simulated and real
               world data.",
  journal   = "Electronic Journal of Statistics",
  publisher = "Institute of Mathematical Statistics and Bernoulli Society",
  volume    =  14,
  number    =  1,
  pages     = "2155--2197",
  month     =  jan,
  year      =  2020,
  url       = "https://projecteuclid.org/journals/electronic-journal-of-statistics/volume-14/issue-1/Projective-inference-in-high-dimensional-problems--Prediction-and-feature/10.1214/20-EJS1711.full",
  keywords  = "62F07; 62F15; 62J12; Feature selection; Post-selection
               inference; prediction; projection; Sparsity; ;Bayes Readings",
  language  = "en",
  issn      = "1746-1391",
  doi       = "10.1214/20-EJS1711"
}

@ARTICLE{Etz2018-bg,
  title    = "How to become a Bayesian in eight easy steps: An annotated
              reading list",
  author   = "Etz, Alexander and Gronau, Quentin F and Dablander, Fabian and
              Edelsbrunner, Peter A and Baribault, Beth",
  abstract = "In this guide, we present a reading list to serve as a concise
              introduction to Bayesian data analysis. The introduction is
              geared toward reviewers, editors, and interested researchers who
              are new to Bayesian statistics. We provide commentary for eight
              recommended sources, which together cover the theoretical and
              practical cornerstones of Bayesian statistics in psychology and
              related sciences. The resources are presented in an incremental
              order, starting with theoretical foundations and moving on to
              applied issues. In addition, we outline an additional 32 articles
              and books that can be consulted to gain background knowledge
              about various theoretical specifics and Bayesian approaches to
              frequently used models. Our goal is to offer researchers a
              starting point for understanding the core tenets of Bayesian
              analysis, while requiring a low level of time commitment. After
              consulting our guide, the reader should understand how and why
              Bayesian methods work, and feel able to evaluate their use in the
              behavioral and social sciences.",
  journal  = "Psychon. Bull. Rev.",
  volume   =  25,
  number   =  1,
  pages    = "219--234",
  month    =  feb,
  year     =  2018,
  url      = "http://dx.doi.org/10.3758/s13423-017-1317-5",
  keywords = "Bayesian statistics; Hypothesis testing;Bayes Readings",
  language = "en",
  issn     = "1069-9384, 1531-5320",
  pmid     = "28660424",
  doi      = "10.3758/s13423-017-1317-5"
}
