---
title: "Problem Set 2"
author:
  - Your Name Here
format: 
  html:
    embed-resources: true
    toc: true
    toc-depth: 2
    toc-title: Contents
---

```{r}
#| echo: false
#| message: false


# Required files for this problem set:
#   - Silver_fir.xlsx

```


## Adaptation to drought stress in Silver Fir

The Silver Fir (*Abies alba*) is a widespread evergreen conifer tree native to the mountainous regions of Europe. Csilléry and colleagues^[Csilléry, K., N. Buchmann, and B. Fady. 2020. Adaptation to drought is coupled with slow growth, but independent from phenology in marginal silver fir (*Abies alba* Mill.) populations. *Evol. Appl.* 13:2357–2376.] carried out a very large (*n* > 8000 measurements) greenhouse experiment to study the genetic adaptations to drought tolerance in *Abies*.

We used this dataset in *Quantitative Methods 1* unit 5 to explore the mean and median. Here we will use it to estimate the means through Bayesian inference and explore the effect of priors on posteriors.

![](https://i.imgur.com/0KwbeIe.jpg)

- Load the data in the file `Silver_fir.xlsx`.
    - NA values are coded with "NA"
- Subset only populations "CHE" and "LAC"
- Select only the columns `population` and `ht99mm`
- Drop any rows with NA

```{r}

```

You should have 512 rows remaining in two columns. Each row represents one tree measured in 1999.

Split the data into two `data.frame`s (or `tibble`s): one for CHE and one for LAC (for simplicity, call these objects `CHE` and `LAC`).

We will model these separately.

```{r}

```

Starting with the CHE population:

- Calculate the mean and median of `ht99mm`
- Plot a histogram of `ht99mm`
- Add the mean and median as vertical lines on the density plot.

```{r}

```

Based on the values of the mean and median, do these data seem acceptable to model the mean height as normally distributed?

> Yes, seems fine. The mean and median are very close to one another (202 vs. 200).

Note that there are formal tests of normality, but they are all pretty flawed, not working very well for either large or small datasets. We can use prior predictive simulation to determine if a normal distribution seems good enough.


### Setting up an `ulam()` model

The basic template of an `ulam()`^[So named after [Stanisław Ulam](https://en.wikipedia.org/wiki/Stanislaw_Ulam), inventor of Monte Carlo sampling and namesake of the Stan language.] model is:

```
ulam(
  alist(
    Distribution for outcome variable,
    Model statement (if necessary),
    Prior 1,
    Prior 2,
    etc.
  ),
  data = Data,
  chains = 4,
  iter = 5e3
)
```

`ulam()` is the call to the `rethinking` function of the same name. This function converts the code contained within `alist()` into stan code. An `alist()` is a special kind of un-evaluated list in R. You don't need to know anything more about it than it's the way that models are specified in `ulam()`^[Neither of us has ever used an `alist` in any other context.]. It's elements are separated by commas.

The first line in the `alist()` the is distribution for the outcome variable. The second is the model statement. And the remaining lines are the priors. It's is not required that things go in this order or that everything fits on one line. It does help to keep the lines in this order for clarity, and if you always set them up the same way, you will be less likely to make a mistake.

The other parameter options control data and sampling. We will cover these later. If you have a multicore computer (most are these days), you can set `cores = 4` to sample all the chains in parallel. Here we are sampling 4 chains for 5,000 total iterations. By default, half of the iterations are used for warmup. So we end up with 10,000 post-warmup samples.

Here is a basic model to estimate a mean for height. When it compiles, you will see the message `Removing one or more character or factor variables`. Stan doesn't want any non-numeric variables, so `ulam()` drops those when preparing data for stan. This does not impact sampling.

You will need to set `eval` to `true` (lower case) to have the chunk below run during rendering.

```{r}
#| eval: false

ulam(
  alist(
    ht99mm ~ dnorm(mu, sigma),
    mu ~ dnorm(100, 10),
    sigma ~ dhalfnorm(0, 5)
  ),
  data = CHE
)
```

Inside `alist()`, the lines are:

1. Distributional definition for `ht99mm`: a normal distribution with a mean `mu` and standard deviation `sigma`. Using `normal()` would also work here.
    - `mu` and `sigma` are estimated from the data.
2. Prior for `mu`: normal distribution with a mean of 100 and standard deviation of 10
3. Prior for `sigma`: half-normal distribution with a mean of 0 and standard deviation of 5

Note that there is no model statement in this case. Because we are only estimating a single mean, we can just put in a prior for `mu` directly. Alternately, we could make an explicit intercept (which would accomplish the same goal) like this:

```{r}
#| eval: false

ulam(
  alist(
    ht99mm ~ dnorm(mu, sigma),
    mu <- a,
    a ~ dnorm(100, 10),
    sigma ~ dhalfnorm(0, 5)
  ),
  data = CHE
)
```

Here the second line says that the `mu` from the line above `ht99mm ~ dnorm(mu, sigma)` is a function of `a`. You use `<-` to define a functional relationship rather than `~` which defines distributions. In line 3, a prior for `a` is specified instead of `mu`. 

If there were other predictors, we could add them: `mu <- a + bx` for example if there were a continuous predictor `x`.


### Prior prediction with `ulam()`

In the lectures we showed you how to carry out prior prediction manually. This is a good approach for learning and works very well for uncomplicated models. Although modeling a mean is straightforward, we can use it to learn another way to do prior predictive simulation directly using `ulam()`.

There is an `ulam()` option `sample_prior`, which, when set to `TRUE`, will ignore the data and sample from the prior only. The model will still compile and sample, but the samples will be drawn from the prior only.

This option makes prior prediction very easy: you can set up the model and prospective priors, and then sample directly. When you are doing more complicated linear models with many predictors or multilevel models with priors of priors (QMLS 2: Bayesian Inference 2), this approach is especially helpful.

Let's add `sample_prior` to sample the prior and save the output to `PP`. This model object is not especially interesting, but the 1,000 samples it contains for `mu` and `sigma` will let us plot the prior predictive distributions. You will again need to set `eval` to `true`.

```{r}
#| eval: false

PP <- ulam(
  alist(
    ht99mm ~ dnorm(mu, sigma),
    mu ~ dnorm(100, 10),
    sigma ~ dhalfnorm(0, 5)
  ),
  data = CHE,
  sample_prior = TRUE
)
```

You again might get the warning `Declaration of arrays by placing brackets...`. You can ignore this warning.

Next we extract the samples from the posterior. This returns a list with `mu` and `sigma` that we convert into a `data.frame()`.

```{r}
#| eval: false

post <- extract.samples(PP) |> as.data.frame()
str(post)
```

We can use these samples to simulate datasets from the specified distributions. The code below generates 20 sets of normal density plots (`dnorm()`) in the range of 0 to 500 drawn from the first 20 prior prediction:

```{r}
#| eval: false
set.seed(4468623)
PP_sim <- purrr::map(
  .x = 1:20,
  .f = function(ii) {
    tibble(ID = ii,
           ht99mm_sim = seq(0, 500, length.out = 200),
           density = dnorm(ht99mm_sim,
                           mean = post$mu[ii],
                           sd = post$sigma[ii]))
  }) |> 
  list_rbind()
head(PP_sim)
```

We can plot these along with our observed data to see if we get reasonable values for `ht99mm`. What we are looking for is that the centers and spreads of the simulated values are in the right range. We don't need them to be exact (actually you don't want this at all). You want ballpark closeness, not off by orders of magnitude.

We fix the x-axis for the observed data in the range 0 to 500 to match the simulated data. This will result in warnings from `ggplot()` that points are dropped. This is okay, because we want to have matching x-axes between the two plots.

```{r}
#| eval: false
p_obs <- ggplot(CHE, aes(ht99mm)) +
  geom_histogram(aes(y = after_stat(density)),
                 fill = "#2D5DA1", bins = 30) +
  xlim(c(0, 500))
p_pp <- ggplot(PP_sim, aes(x = ht99mm_sim, y = density, group = ID)) +
  geom_line()
cowplot::plot_grid(p_obs, p_pp, nrow = 2, align = "hv")
```

Looking at the plot, it looks like the means are probably way too low and the spreads are too narrow. The height of the `density` on the y-axis is not very useful as a metric. Occasionally you will get a distribution that has high density on one region and flattens out all the others. You could pick a different set of prior distributions or a different seed if you want.

In the chunk below, first change the value for the priors to something completely unrealistic (mean of 10) to see the effect on prior prediction. Then try some other combinations of values until you get a set of priors that you think will work.

There is no one correct answer. Generally with Bayesian inference, the priors are helpful in guiding the sampler to parameter estimates. You don't want the prior to determine the posterior.

```{r}
#| eval: false
PP <- ulam(
  alist(
    ht99mm ~ dnorm(mu, sigma),
    mu ~ dnorm(0, 5),
    sigma ~ dhalfnorm(0, 1)
  ),
  data = CHE,
  sample_prior = TRUE
)

post <- extract.samples(PP) |> as.data.frame()

set.seed(313123)
PP_sim <- purrr::map(
  .x = 1:20,
  .f = function(ii) {
    tibble(ID = ii,
           ht99mm_sim = seq(0, 500, length.out = 200),
           density = dnorm(ht99mm_sim,
                           mean = post$mu[ii],
                           sd = post$sigma[ii]))
  }) |> 
  list_rbind()

p_obs <- ggplot(CHE, aes(ht99mm)) +
  geom_histogram(aes(y = after_stat(density)),
                 fill = "#2D5DA1", bins = 30) +
  xlim(c(0, 500))
p_pp <- ggplot(PP_sim, aes(x = ht99mm_sim, y = density, group = ID)) +
  geom_line()
cowplot::plot_grid(p_obs, p_pp, nrow = 2, align = "hv")
```


### Sampling and posterior prediction

Once you have the priors that you think will work for this dataset, it's time to turn on sampling with the data. Copy the whole chunk above into the one below.

Either delete the `sample_prior` option or set it to `FALSE`. Then add the following options:

- `iter = 5000` to sample 5000 iterations
- `refresh = 2500` to reduce the verbosity of the stan sampling output
    - You can set `refresh = 0` to not show the sampling iterations at all, and only the final sampling output.
- `chains = 4` to sample 4 independent chains

If you have a multicore machine, you can also use `cores = 4`. But this model should sample really fast anyway, so it's not necessary.

```{r}

```

Looking at the plot of the samples from the posterior compared to the distribution of `ht99mm`, does it look like the posterior is a good representation of the data ("posterior predictive check")? It should -- and if it does not, you should rethink your priors.

> 

Use `summary()` to summarize the fit model.

```{r}
```

Does everything look okay from a diagnostics standpoint (`n_eff` and `Rhat`)?

> 

What are the means and 89% HDIs for `mu` and `sigma`? You can use the `HPDI()` function from rethinking.

```{r}

```

> 

How do the means and HDIs of these posterior estimates compare to the values you used for the prior?

> 


## Repeat for `LAC`

Repeat the steps above with the `LAC` dataset for only the "LAC" population. You can/should copy the code from above and modify rather than starting from scratch.

We will do some things the same and some differently.

- Calculate mean and median. Make the histogram with vertical lines.
- Instead of modeling `ht99mm` directly, use the natural log transformation.
    - Another way to do this would be to use a log-normal distribution. However, priors for log-normal distributions can be really difficult to figure out, so we will not attempt that here. 
- The code will look very similar, but the scale of `mu` and `sigma` will be *very* different from above. You will have to figure out the range in which to simulate data.

Make the initial histogram + mean and median plot here (pre-transformation): 

```{r}

```

Describe the shape of the histogram and the relationship between mean and median.

> 

Do the log-transformation here:

```{r}

```

From here on, use the log-transformed height. Carry out the prior predictive check below. You will need to figure out what priors to use.

```{r}

```

What priors did you try, what did you decide to use, and explain why.

> 

Carry out the sampling and posterior predictive check here:

```{r}

```

Looking at the plot of the samples from the posterior compared to the distribution of `ht99mm`, does it look like the posterior is a good representation of the data ("posterior predictive check")? It should -- and if it does not, you should rethink your priors.

> 

Use `summary()` to summarize the fit model.

```{r}

```

Does everything look okay from a diagnostics standpoint (`n_eff` and `Rhat`)?

> 

What are the means and 89% HDIs for `mu` and `sigma`? You can use the `HPDI()` function from rethinking.

```{r}

```

> 

How do the means and HDIs of these posterior estimates compare to the values you used for the prior?

> 


## Sensitivity of the posterior to the prior

We want to explore how sensitive the posterior is to the choice of prior for these data and this model. 

In the chunk below, copy the sampling code from above (the "LAC" model). Try changing different aspects of the priors (means and/or standard deviations). Sample the model and compare the posterior summaries (`summary()` or `precis()`). You don't need to save all the combinations, just compare as you go, observing any patterns.

```{r}

```

What do you conclude about the relationship between prior and posterior for these data and this model?

> 


## Looking at the samples

Lastly, we want to just examine the samples some to gain a little insight into the MCMC/HMC process.

First, go back to your final "LAC" model and add a line to save the posterior to an object and convert to a data.frame (so it is not over-written by the code in the previous section). Below we'll work with these samples.

Make a plot of the first 25 samples from the posterior:

- Slice the first 25 rows of the posterior from above
- Create a column `Iteration` that goes from 1-25
- Pivot the data to long format by Iteration. Make the new "names" column `Parameter` and the new "values" column to `Estimate`.
- Plot `Iteration` on the x-axis and `Estimate` on the y-axis. Facet by `Parameter` so that the two parameters are in 2 rows.
- Free the y-axis scale.

```{r}

```

What do you observe about the samples? How does this compare to the samples from our "homebrew" MCMC sampler in lecture 1-3?

> 

Make a scatter plot of sigma vs. mu from all the posterior samples. This is a "pairs" plot, which a common diagnostic tool for MCMC.

```{r}

```

What do you observe in the scatter plot?

> 

Finally load the `coda` package. Convert the posterior which is currently a data.frame to an `mcmc` object with `as.mcmc()`. Then run the autocorrelation diagnostics on this object with `autocorr.diag()`. This is similar to what we did in the lecture.

```{r}


```

You should observe that all the autocorrelations (except for lag 1, which is 1 by definition -- any value has a correlation of 1 with itself) are very close to zero. You might also see that some are less than zero. Such negative autocorrelations are common in stan because of the no u-turn sampler (NUTS). This peculiarity can lead to effective sample sizes (`n_eff`) that are actually greater than the number of post-warmup samples. [Here is a little more discussion](https://mc-stan.org/docs/2_21/reference-manual/effective-sample-size-section.html).
