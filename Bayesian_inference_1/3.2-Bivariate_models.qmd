---
title: "Bivariate models and regularizing priors"
author:
  - Elizabeth King
  - Kevin Middleton
format:
  revealjs:
    theme: [default, custom.scss]
    standalone: true
    embed-resources: true
    logo: QMLS_Logo.png
    slide-number: true
    show-slide-number: all
    link-external-newwindow: true
bibliography: Bayes.bib
---

## Bivariate models


```{r}
#| label: setup
#| echo: false
#| warning: false
#| message: false

library(tidyverse)
library(cowplot)
ggplot2::theme_set(theme_cowplot(font_size = 18))

library(truncnorm)

library(rethinking)
```

$$y ~ \beta_0 + \beta_1 x$$

- Need to explicitly include the intercept
- $x$ is continuous: "linear regression"
- $x$ is categorical: $t$-test or ANOVA
  - $\beta_0 == 0$


## Simulate data

- $\beta_0 = 2$
- $\beta_1 = 6.7$

```{r}
#| echo: true
#| output-location: slide

set.seed(4534759)
D <- tibble(x = runif(20, 0, 10),
            y = 2 + 6.7 * x + rnorm(20, 0, 3))

ggplot(D, aes(x, y)) +
  geom_point(size = 4)
```


## Model statements

$y$ follows a normal distribution with a mean and standard deviation:

$$y \sim  \mathcal{N}(\mu, \sigma)$$

The mean is a linear function of an intercept ($a$) and a slope ($b$):

$$\mu =  a + b \cdot x$$

. . .

$a$ and $b$ are arbitrary names.

- With more predictors, more meaningful names are useful


## Prior prediction: Reasonable values for $a$, $b$, and $\sigma$

- Think about the slope first
- Intercept depends on the slope, so let it vary more
  - Intercept is often not "interesting"
  - Byproduct of the other parameters
- Don't need to think about $\sigma$ for prior prediction (usually)


## Normal distributions

$\sim 95\%$ of points fall within $\pm 2$ standard deviations of the mean

- Center parameter estimates on 0 (unless you have a reason otherwise *a priori*)
  - "Slope not different from zero"
  - "No difference in means"
  - "No difference between groups"


## Regularizing priors

- Bayesian priors are conservative
  - All values *not* equally likely (true for maximum likelihood)
- Less likely to find an effect by chance
- Lessens the influence of extreme or unusual values automatically


## Prior prediction

- Intercept probably $\pm 40$
- Slope probably $\pm 10$

```{r}
#| echo: true
#| output-location: slide

set.seed(3457522)

sim_model <- function(ii) {
  a <- rnorm(1, 0, 20)
  b <- rnorm(1, 0, 5)
  tibble(rep = ii,
         x = seq(0:10),
         y = a + b * x)
}

sims <- map_dfr(.x = 1:50,
                .f = sim_model)
print(sims, n = 200)
```


## Plotting the prior predictions

Are the predicted outcomes on the right scale?

```{r}
#| echo: true
#| output-location: slide

ggplot(sims, aes(x, y, group = rep)) +
  geom_line(alpha = 0.5)
```


## Fitting the model

```{r}
#| echo: true
#| output-location: slide

fm <- ulam(
  alist(
    y ~ normal(mu, sigma),
    mu <- a + b * x,
    a ~ dnorm(0, 20),
    b ~ dnorm(0, 5),
    sigma ~ dhalfnorm(0, 5)
  ),
  data = D,
  chains = 4,
  iter = 5e3
)
```


## Inspecting the chains: `traceplot`

```{r}
#| echo: true

traceplot(fm)
```


## Inspecting the chains: Rank histogram

```{r}
#| echo: true

trankplot(fm)
```


## Inspecting the output

```{r}
#| echo: true

precis(fm)
```

Compare to `lm()`

```{r}
#| echo: true

fm_lm <- lm(y ~ x, data = D)
coef(fm_lm)
confint(fm_lm, level = 0.89)
```


## Posteriors

```{r}
#| echo: true

post <- extract.samples(fm) |> 
  as.data.frame()
post
```


## Distributions of posterior parameter estimates

```{r}
#| echo: true
#| output-location: slide

post |> 
  pivot_longer(cols = everything(),
               names_to = "Parameter",
               values_to = "Estimate") |> 
  ggplot(aes(Estimate)) +
  geom_density() +
  facet_wrap("Parameter", scales = "free")
```

