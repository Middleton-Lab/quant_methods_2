---
title: "Cross-Validation and PSIS"
author:
  - Elizabeth King
  - Kevin Middleton
format:
  revealjs:
    theme: [default, custom.scss]
    standalone: true
    embed-resources: true
    logo: QMLS_Logo.png
    slide-number: true
    show-slide-number: all
    link-external-newwindow: true
bibliography: Bayes.bib
csl: evolution.csl
---

## Information criteria vs. Cross-validation

```{r}
#| label: setup
#| echo: false
#| warning: false
#| message: false

library(tidyverse)
library(readxl)
library(wesanderson)
library(cowplot)
ggplot2::theme_set(theme_cowplot(font_size = 18))

library(cmdstanr)
library(brms)
library(bayesplot)
color_scheme_set(scheme = "red")

options(brms.backend = "cmdstanr",
        mc.cores = 4)
```

- Information equals "Surprise"
- Cross validation
    - Divide the data (*k* folds), refit models, out-of-sample prediction
    - Leave-one-out is the logical extreme
    - Review *Quantitative Methods* lecture 10-4


## Why not leave-one-out for Bayesian models?

- Resample the model for every data point?
    - Hundreds or thousands of distributions
    - Time-intensive
- We need a LOO-CV approximation [@Burkner2021-mp]
    - Which is also an approximation of expected log-predictive density (elpd)
    - Like AIC and WAIC are approximations


## PSIS-LOO-CV

Pareto-smoothed importance sampling [@Vehtari2017-we; @Vehtari2015-kl]

- Determine the "importance" of each observation to the posterior
- "Unlikely" observations are more important than expected ones (more weight)
- Pareto-smoothing makes these estimates more reliable
- Math not critical to understand


## Naked Mole Rats

```{r}
#| echo: false

M <- abdData::MoleRats |> 
  rename(Caste = caste,
         Mass = ln.mass,
         Energy= ln.energy) |> 
  mutate(Caste = if_else(Caste == "worker", "Worker", "NonWorker"),
         Caste = factor(Caste))

ggplot(M, aes(x = Mass, y = Energy, color = Caste)) +
  geom_point(size = 4) +
  scale_color_manual(values = wes_palette("Cavalcanti1")) +
  theme(legend.justification = c(0, 1),
        legend.position = c(0.05, 1)) +
  labs(x = "ln Body Mass (g)",
       y = "ln Daily Energy Expenditure (kJ)")
```


## Five models

1. Mean: grand mean (no body mass)
2. ANOVA: group means only (no body mass)
3. OLS regression: body mass only, no grouping
4. ANCOVA: intercepts varying
5. ANCOVA: slopes varying and intercepts varying


## 4: ANCOVA, intercepts varying

```{r}
#| echo: false

fm4 <- lm(Energy ~ Mass + Caste, data = M)
M <- M |> mutate(pred4 = predict(fm4))
ggplot(M, aes(x = Mass, y = Energy, color = Caste)) +
  geom_point(size = 4) +
  geom_line(aes(x = Mass, y = pred4, color = Caste), lwd = 2) +
  scale_color_manual(values = wes_palette("Cavalcanti1")) +
  theme(legend.justification = c(0, 1),
        legend.position = c(0.05, 1)) +
  labs(x = "ln Body Mass (g)",
       y = "ln Daily Energy Expenditure (kJ)")
```


## 5: ANCOVA, intercepts and slopes vary

```{r}
#| echo: false

fm5 <- lm(Energy ~ Mass * Caste, data = M)
M <- M |> mutate(pred5 = predict(fm5))
ggplot(M, aes(x = Mass, y = Energy, color = Caste)) +
  geom_point(size = 4) +
  geom_line(aes(x = Mass, y = pred5, color = Caste), lwd = 2) +
  scale_color_manual(values = wes_palette("Cavalcanti1")) +
  theme(legend.justification = c(0, 1),
        legend.position = c(0.05, 1)) +
  labs(x = "ln Body Mass (g)",
       y = "ln Daily Energy Expenditure (kJ)")
```

```{r}
#| echo: false
#| output: false

fm1 <- brm(Energy ~ 1, data = M,
           prior = prior(normal(0, 3), class = Intercept), iter = 2e4, refresh = 0)
fm2 <- brm(Energy ~ Caste, data = M,
           prior = prior(normal(0, 3), class = b), iter = 2e4, refresh = 0)
fm3 <- brm(Energy ~ Mass, data = M,
           prior = prior(normal(0, 3), class = b), iter = 2e4, refresh = 0)
fm4 <- brm(Energy ~ Mass + Caste, data = M,
           prior = prior(normal(0, 3), class = b), iter = 2e4, refresh = 0)
fm5 <- brm(Energy ~ Mass * Caste, data = M,
           prior = prior(normal(0, 3), class = b), iter = 2e4, refresh = 0)
```


## PSIS-LOO-CV with `loo()`

Built-in diagnostics

  - Very high weights can make PSIS-LOO-CV unreliable (Pareto $k$ estimates)
  - These influential values are flagged by `loo()`
  - Inspect with `loo::pareto_k_ids()`
  - `reloo()` to refit problem obervations


## PSIS-LOO-CV with `loo()`

- `looic` functions like a WAIC or AIC

```{r}
#| echo: true

loo(fm5)
```


## Adding PSIS-LOO-CV to a model

`add_criterion()` adds different criteria to models (WAIC, PSIS-LOO-CV)

```{r}
#| echo: true

fm1 <- add_criterion(fm1, criterion = "loo")
fm2 <- add_criterion(fm2, criterion = "loo")
fm3 <- add_criterion(fm3, criterion = "loo")
fm4 <- add_criterion(fm4, criterion = "loo")
fm5 <- add_criterion(fm5, criterion = "loo")
```

Also do this if you want to save the model to an object for later use

## LOO compare

```{r}
#| echo: true

loo_compare(fm1, fm2, fm3, fm4, fm5, criterion = "loo")
```


## LOO model weights

```{r}
#| echo: true
#| output-location: slide

model_weights(fm1, fm2, fm3, fm4, fm5, weights = "loo") |> 
  as.data.frame() |> 
  knitr::kable(col.names = "Weight", digits = 4)
```


## References

::: {#refs}
:::
